A Brief Introduction to



Neural Networks
David Kriesel
dkriesel.com




                                               Download location:
               http://www.dkriesel.com/en/science/neural_networks

                                         NEW – for the programmers:
                 Scalable and efficient NN framework, written in JAVA
                             http://www.dkriesel.com/en/tech/snipe
dkriesel.com




                     In remembrance of
                     Dr. Peter Kemp, Notary (ret.), Bonn, Germany.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)      iii
A small preface
                  "Originally, this work has been prepared in the framework of a seminar of the
                    University of Bonn in Germany, but it has been and will be extended (after
                            being presented and published online under www.dkriesel.com on
                   5/27/2005). First and foremost, to provide a comprehensive overview of the
                         subject of neural networks and, second, just to acquire more and more
                  knowledge about LATEX . And who knows – maybe one day this summary will
                                                                        become a real preface!"
                                                            Abstract of this work, end of 2005



The above abstract has not yet become a        stand the definitions without reading the
preface but at least a little preface, ever    running text, while the opposite holds for
since the extended text (then 40 pages         readers only interested in the subject mat-
long) has turned out to be a download          ter; everything is explained in both collo-
hit.                                           quial and formal language. Please let me
                                               know if you find out that I have violated
                                               this principle.
Ambition and intention of this
manuscript
                                               The sections of this text are mostly
                                               independent from each other
The entire text is written and laid out
more effectively and with more illustra-
                                             The document itself is divided into differ-
tions than before. I did all the illustra-
                                             ent parts, which are again divided into
tions myself, most of them directly in
                                             chapters. Although the chapters contain
LATEX by using XYpic. They reflect what
                                             cross-references, they are also individually
I would have liked to see when becoming
                                             accessible to readers with little previous
acquainted with the subject: Text and il-
                                             knowledge. There are larger and smaller
lustrations should be memorable and easy
                                             chapters: While the larger chapters should
to understand to offer as many people as
                                             provide profound insight into a paradigm
possible access to the field of neural net-
                                             of neural networks (e.g. the classic neural
works.
                                             network structure: the perceptron and its
Nevertheless, the mathematically and for- learning procedures), the smaller chapters
mally skilled readers will be able to under- give a short overview – but this is also ex-



                                                                                             v
                                                                                      dkriesel.com


plained in the introduction of each chapter.         the original high-performance simulation
In addition to all the definitions and expla-        design goal. Those of you who are up for
nations I have included some excursuses              learning by doing and/or have to use a
to provide interesting information not di-           fast and stable neural networks implemen-
rectly related to the subject.                       tation for some reasons, should definetely
                                                     have a look at Snipe.
Unfortunately, I was not able to find free
German sources that are multi-faceted
in respect of content (concerning the        However, the aspects covered by Snipe are
paradigms of neural networks) and, nev-      not entirely congruent with those covered
ertheless, written in coherent style. The    by this manuscript. Some of the kinds
aim of this work is (even if it could not    of neural networks are not supported by
be fulfilled at first go) to close this gap bit
                                             Snipe, while when it comes to other kinds
by bit and to provide easy access to the     of neural networks, Snipe may have lots
subject.                                     and lots more capabilities than may ever
                                             be covered in the manuscript in the form
                                             of practical hints. Anyway, in my experi-
Want to learn not only by                    ence almost all of the implementation re-
reading, but also by coding?                 quirements of my readers are covered well.
                                             On the Snipe download page, look for the
Use SNIPE!                                   section "Getting started with Snipe" – you
                                             will find an easy step-by-step guide con-
SNIPE 1 is a well-documented JAVA li- cerning Snipe and its documentation, as
brary that implements a framework for well as some examples.
neural networks in a speedy, feature-rich
and usable way. It is available at no           SNIPE: This manuscript frequently incor-
cost for non-commercial purposes. It was        porates Snipe. Shaded Snipe-paragraphs
originally designed for high performance        like this one are scattered among large
simulations with lots and lots of neural
                                                parts of the manuscript, providing infor-
                                                mation on how to implement their con-
networks (even large ones) being trained        text in Snipe. This also implies that
simultaneously. Recently, I decided to          those who do not want to use Snipe,
give it away as a professional reference im-    just have to skip the shaded Snipe-
plementation that covers network aspects        paragraphs! The Snipe-paragraphs as-
                                                sume the reader has had a close look at
handled within this work, while at the          the "Getting started with Snipe" section.
same time being faster and more efficient       Often, class names are used. As Snipe con-
than lots of other implementations due to       sists of only a few different packages, I omit-
                                                       ted the package names within the qualified
1 Scalable and Generalized Neural Information Pro-     class names for the sake of readability.
  cessing Engine, downloadable at http://www.
  dkriesel.com/tech/snipe, online JavaDoc at
  http://snipe.dkriesel.com




vi                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com


It’s easy to print this                      Speaking headlines throughout the
manuscript                                   text, short ones in the table of
                                             contents

This text is completely illustrated in       The whole manuscript is now pervaded by
color, but it can also be printed as is in   such headlines. Speaking headlines are
monochrome: The colors of figures, tables    not just title-like ("Reinforcement Learn-
and text are well-chosen so that in addi-    ing"), but centralize the information given
tion to an appealing design the colors are   in the associated section to a single sen-
still easy to distinguish when printed in    tence. In the named instance, an appro-
monochrome.                                  priate headline would be "Reinforcement
                                             learning methods provide feedback to the
                                             network, whether it behaves good or bad".
                                             However, such long headlines would bloat
                                             the table of contents in an unacceptable
There are many tools directly                way. So I used short titles like the first one
integrated into the text                     in the table of contents, and speaking ones,
                                             like the latter, throughout the text.

Different aids are directly integrated in the
document to make reading more flexible: Marginal notes are a navigational
However, anyone (like me) who prefers aid
reading words on paper rather than on
screen can also enjoy some features.          The entire document contains marginal
                                              notes in colloquial language (see the exam-
                                                                                              Hypertext
                                              ple in the margin), allowing you to "scan"      on paper
                                              the document quickly to find a certain pas-     :-)
In the table of contents, different           sage in the text (including the titles).
types of chapters are marked
                                           New mathematical symbols are marked by
                                           specific marginal notes for easy finding
Different types of chapters are directly (see the example for x in the margin).
                                                                                              Jx
marked within the table of contents. Chap-
ters, that are marked as "fundamental"
are definitely ones to read because almost There are several kinds of indexing
all subsequent chapters heavily depend on
them. Other chapters additionally depend This document contains different types of
on information given in other (preceding) indexing: If you have found a word in
chapters, which then is marked in the ta- the index and opened the corresponding
ble of contents, too.                      page, you can easily find it by searching



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         vii
                                                                             dkriesel.com


for highlighted text – all indexed words        3. You must maintain the author’s attri-
are highlighted like this.                         bution of the document at all times.

Mathematical symbols appearing in sev-          4. You may not use the attribution to
eral chapters of this document (e.g. Ω for         imply that the author endorses you
an output neuron; I tried to maintain a            or your document use.
consistent nomenclature for regularly re- For I’m no lawyer, the above bullet-point
curring elements) are separately indexed  summary is just informational: if there is
under "Mathematical Symbols", so they     any conflict in interpretation between the
can easily be assigned to the correspond- summary and the actual license, the actual
ing term.                                 license always takes precedence. Note that
                                          this license does not extend to the source
Names of persons written in small caps
                                          files used to produce the document. Those
are indexed in the category "Persons" and
                                          are still mine.
ordered by the last names.

                                               How to cite this manuscript
Terms of use and license
                                               There’s no official publisher, so you need
                                               to be careful with your citation. Please
Beginning with the epsilon edition, the        find more information in English and
text is licensed under the Creative Com-       German language on my homepage, re-
mons Attribution-No Derivative Works           spectively the subpage concerning the
3.0 Unported License 2 , except for some       manuscript3 .
little portions of the work licensed under
more liberal licenses as mentioned (mainly
some figures from Wikimedia Commons).          Acknowledgement
A quick license summary:

  1. You are free to redistribute this docu- Now I would like to express my grati-
                                        tude to all the people who contributed, in
       ment (even though it is a much better
                                        whatever manner, to the success of this
       idea to just distribute the URL of my
                                        work, since a work like this needs many
       homepage, for it always contains the
       most recent version of the text).helpers. First of all, I want to thank
                                        the proofreaders of this text, who helped
  2. You may not modify, transform, or me and my readers very much. In al-
     build upon the document except for phabetical order: Wolfgang Apolinarski,
     personal use.                      Kathrin Gräve, Paul Imhoff, Thomas
2 http://creativecommons.org/licenses/         3 http://www.dkriesel.com/en/science/
  by-nd/3.0/                                     neural_networks




viii                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com


Kühn, Christoph Kunze, Malte Lohmeyer,        of the University of Bonn – they all made
Joachim Nock, Daniel Plohmann, Daniel         sure that I always learned (and also had
Rosenthal, Christian Schulz and Tobias        to learn) something new about neural net-
Wilken.                                       works and related subjects. Especially Dr.
                                              Goerke has always been willing to respond
Additionally, I want to thank the readers
                                              to any questions I was not able to answer
Dietmar Berger, Igor Buchmüller, Marie
                                              myself during the writing process. Conver-
Christ, Julia Damaschek, Jochen Döll,
                                              sations with Prof. Eckmiller made me step
Maximilian Ernestus, Hardy Falk, Anne
                                              back from the whiteboard to get a better
Feldmeier, Sascha Fink, Andreas Fried-
                                              overall view on what I was doing and what
mann, Jan Gassen, Markus Gerhards, Se-
                                              I should do next.
bastian Hirsch, Andreas Hochrath, Nico
Höft, Thomas Ihme, Boris Jentsch, Tim
                                              Globally, and not only in the context of
Hussein, Thilo Keller, Mario Krenn, Mirko
                                              this work, I want to thank my parents who
Kunze, Maikel Linke, Adam Maciak,
                                              never get tired to buy me specialized and
Benjamin Meier, David Möller, Andreas
                                              therefore expensive books and who have
Müller, Rainer Penninger, Lena Reichel,
                                              always supported me in my studies.
Alexander Schier, Matthias Siegmund,
Mathias Tirtasana, Oliver Tischler, Max-
                                              For many "remarks" and the very special
imilian Voit, Igor Wall, Achim Weber,
                                              and cordial atmosphere ;-) I want to thank
Frank Weinreis, Gideon Maillette de Buij
                                              Andreas Huber and Tobias Treutler. Since
Wenniger, Philipp Woock and many oth-
                                              our first semester it has rarely been boring
ers for their feedback, suggestions and re-
                                              with you!
marks.
Additionally, I’d like to thank Sebastian  Now I would like to think back to my
Merzbach, who examined this work in a      school days and cordially thank some
very conscientious way finding inconsisten-teachers who (in my opinion) had im-
cies and errors. In particular, he cleared parted some scientific knowledge to me –
lots and lots of language clumsiness from  although my class participation had not
the English version.                       always been wholehearted: Mr. Wilfried
                                           Hartmann, Mr. Hubert Peters and Mr.
Especially, I would like to thank Beate
                                           Frank Nökel.
Kuhl for translating the entire text from
German to English, and for her questions
                                           Furthermore I would like to thank the
which made me think of changing the
                                           whole team at the notary’s office of Dr.
phrasing of some paragraphs.
                                           Kemp and Dr. Kolb in Bonn, where I have
I would particularly like to thank Prof. always felt to be in good hands and who
Rolf Eckmiller and Dr. Nils Goerke as have helped me to keep my printing costs
well as the entire Division of Neuroinfor- low - in particular Christiane Flamme and
matics, Department of Computer Science Dr. Kemp!



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         ix
                                                                         dkriesel.com


Thanks go also to the Wikimedia Com-
mons, where I took some (few) images and
altered them to suit this text.
Last but not least I want to thank two
people who made outstanding contribu-
tions to this work who occupy, so to speak,
a place of honor: My girlfriend Verena
Thomas, who found many mathematical
and logical errors in my text and dis-
cussed them with me, although she has
lots of other things to do, and Chris-
tiane Schultze, who carefully reviewed the
text for spelling mistakes and inconsisten-
cies.




                  David Kriesel




x                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
Contents

A small preface                                                                                                            v


I   From biology to formalization – motivation, philosophy, history and
    realization of neural models                                        1

1 Introduction, motivation and history                                                                                     3
  1.1 Why neural networks? . . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    3
       1.1.1 The 100-step rule . . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    5
       1.1.2 Simple application examples . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    6
  1.2 History of neural networks . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    8
       1.2.1 The beginning . . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    8
       1.2.2 Golden age . . . . . . . . . . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    9
       1.2.3 Long silence and slow reconstruction         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   11
       1.2.4 Renaissance . . . . . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   12
  Exercises . . . . . . . . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   12

2 Biological neural networks                                                                                              13
  2.1 The vertebrate nervous system . . . . . . . . . .               . . . . .           .   .   .   .   .   .   .   .   13
       2.1.1 Peripheral and central nervous system . .                . . . . .           .   .   .   .   .   .   .   .   13
       2.1.2 Cerebrum . . . . . . . . . . . . . . . . . .             . . . . .           .   .   .   .   .   .   .   .   14
       2.1.3 Cerebellum . . . . . . . . . . . . . . . . .             . . . . .           .   .   .   .   .   .   .   .   15
       2.1.4 Diencephalon . . . . . . . . . . . . . . . .             . . . . .           .   .   .   .   .   .   .   .   15
       2.1.5 Brainstem . . . . . . . . . . . . . . . . . .            . . . . .           .   .   .   .   .   .   .   .   16
  2.2 The neuron . . . . . . . . . . . . . . . . . . . . .            . . . . .           .   .   .   .   .   .   .   .   16
       2.2.1 Components . . . . . . . . . . . . . . . .               . . . . .           .   .   .   .   .   .   .   .   16
       2.2.2 Electrochemical processes in the neuron .                . . . . .           .   .   .   .   .   .   .   .   19
  2.3 Receptor cells . . . . . . . . . . . . . . . . . . . .          . . . . .           .   .   .   .   .   .   .   .   24
       2.3.1 Various types . . . . . . . . . . . . . . . .            . . . . .           .   .   .   .   .   .   .   .   24
       2.3.2 Information processing within the nervous                system              .   .   .   .   .   .   .   .   25
       2.3.3 Light sensing organs . . . . . . . . . . . .             . . . . .           .   .   .   .   .   .   .   .   26
  2.4 The amount of neurons in living organisms . . .                 . . . . .           .   .   .   .   .   .   .   .   28



                                                                                                                          xi
Contents                                                                                                dkriesel.com


      2.5 Technical neurons as caricature of biology . . . . . . . . . . . . . . . . . 30
      Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

3 Components of artificial neural networks (fundamental)                                                                    33
  3.1 The concept of time in neural networks . . . . . .                    .   .   .   .   .   .   .   .   .   .   .   .   33
  3.2 Components of neural networks . . . . . . . . . . .                   .   .   .   .   .   .   .   .   .   .   .   .   33
       3.2.1 Connections . . . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   34
       3.2.2 Propagation function and network input . .                     .   .   .   .   .   .   .   .   .   .   .   .   34
       3.2.3 Activation . . . . . . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   .   .   .   35
       3.2.4 Threshold value . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   36
       3.2.5 Activation function . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   36
       3.2.6 Common activation functions . . . . . . . .                    .   .   .   .   .   .   .   .   .   .   .   .   37
       3.2.7 Output function . . . . . . . . . . . . . . .                  .   .   .   .   .   .   .   .   .   .   .   .   38
       3.2.8 Learning strategy . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   38
  3.3 Network topologies . . . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   39
       3.3.1 Feedforward . . . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   39
       3.3.2 Recurrent networks . . . . . . . . . . . . . .                 .   .   .   .   .   .   .   .   .   .   .   .   40
       3.3.3 Completely linked networks . . . . . . . . .                   .   .   .   .   .   .   .   .   .   .   .   .   42
  3.4 The bias neuron . . . . . . . . . . . . . . . . . . .                 .   .   .   .   .   .   .   .   .   .   .   .   43
  3.5 Representing neurons . . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   45
  3.6 Orders of activation . . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   45
       3.6.1 Synchronous activation . . . . . . . . . . .                   .   .   .   .   .   .   .   .   .   .   .   .   45
       3.6.2 Asynchronous activation . . . . . . . . . . .                  .   .   .   .   .   .   .   .   .   .   .   .   46
  3.7 Input and output of data . . . . . . . . . . . . . .                  .   .   .   .   .   .   .   .   .   .   .   .   48
  Exercises . . . . . . . . . . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   .   48

4 Fundamentals on learning and training samples            (fundamental)                                                    51
  4.1 Paradigms of learning . . . . . . . . . . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   51
      4.1.1 Unsupervised learning . . . . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   52
      4.1.2 Reinforcement learning . . . . . . .            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   53
      4.1.3 Supervised learning . . . . . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   53
      4.1.4 Offline or online learning? . . . . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   54
      4.1.5 Questions in advance . . . . . . . . .          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   54
  4.2 Training patterns and teaching input . . . .          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   54
  4.3 Using training samples . . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   56
      4.3.1 Division of the training set . . . . .          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   57
      4.3.2 Order of pattern representation . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   57
  4.4 Learning curve and error measurement . . .            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   58
      4.4.1 When do we stop learning? . . . . .             .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   59



xii                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                         Contents


     4.5  Gradient optimization procedures . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   61
          4.5.1 Problems of gradient procedures          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   62
     4.6 Exemplary problems . . . . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   64
          4.6.1 Boolean functions . . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   64
          4.6.2 The parity function . . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   64
          4.6.3 The 2-spiral problem . . . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   64
          4.6.4 The checkerboard problem . . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   65
          4.6.5 The identity function . . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   65
          4.6.6 Other exemplary problems . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   66
     4.7 Hebbian rule . . . . . . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   66
          4.7.1 Original rule . . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   66
          4.7.2 Generalized form . . . . . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   67
     Exercises . . . . . . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   67


II    Supervised learning network paradigms                                                                                      69

5 The perceptron, backpropagation and its variants                                                                                71
  5.1 The singlelayer perceptron . . . . . . . . . . . . . . . . . . . . .                                   .   .   .   .   .    74
      5.1.1 Perceptron learning algorithm and convergence theorem                                            .   .   .   .   .    75
      5.1.2 Delta rule . . . . . . . . . . . . . . . . . . . . . . . . . .                                   .   .   .   .   .    75
  5.2 Linear separability . . . . . . . . . . . . . . . . . . . . . . . . .                                  .   .   .   .   .    81
  5.3 The multilayer perceptron . . . . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .    84
  5.4 Backpropagation of error . . . . . . . . . . . . . . . . . . . . . .                                   .   .   .   .   .    86
      5.4.1 Derivation . . . . . . . . . . . . . . . . . . . . . . . . . .                                   .   .   .   .   .    87
      5.4.2 Boiling backpropagation down to the delta rule . . . . .                                         .   .   .   .   .    91
      5.4.3 Selecting a learning rate . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .    92
  5.5 Resilient backpropagation . . . . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .    93
      5.5.1 Adaption of weights . . . . . . . . . . . . . . . . . . . .                                      .   .   .   .   .    94
      5.5.2 Dynamic learning rate adjustment . . . . . . . . . . . .                                         .   .   .   .   .    94
      5.5.3 Rprop in practice . . . . . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .    95
  5.6 Further variations and extensions to backpropagation . . . . .                                         .   .   .   .   .    96
      5.6.1 Momentum term . . . . . . . . . . . . . . . . . . . . . .                                        .   .   .   .   .    96
      5.6.2 Flat spot elimination . . . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .    97
      5.6.3 Second order backpropagation . . . . . . . . . . . . . .                                         .   .   .   .   .    98
      5.6.4 Weight decay . . . . . . . . . . . . . . . . . . . . . . . .                                     .   .   .   .   .    98
      5.6.5 Pruning and Optimal Brain Damage . . . . . . . . . . .                                           .   .   .   .   .    98
  5.7 Initial configuration of a multilayer perceptron . . . . . . . . .                                     .   .   .   .   .    99
      5.7.1 Number of layers . . . . . . . . . . . . . . . . . . . . . .                                     .   .   .   .   .    99
      5.7.2 The number of neurons . . . . . . . . . . . . . . . . . .                                        .   .   .   .   .   100



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                  xiii
Contents                                                                                          dkriesel.com


           5.7.3 Selecting an activation function . . . . . . .       .   .   .   .   .   .   .   .   .   .   .   .   100
           5.7.4 Initializing weights . . . . . . . . . . . . . .     .   .   .   .   .   .   .   .   .   .   .   .   101
      5.8 The 8-3-8 encoding problem and related problems             .   .   .   .   .   .   .   .   .   .   .   .   101
      Exercises . . . . . . . . . . . . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   102

6 Radial basis functions                                                                                            105
  6.1 Components and structure . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   . 105
  6.2 Information processing of an RBF network . . . . . .                    .   .   .   .   .   .   .   .   .   . 106
       6.2.1 Information processing in RBF neurons . . . .                    .   .   .   .   .   .   .   .   .   . 108
       6.2.2 Analytical thoughts prior to the training . . . .                .   .   .   .   .   .   .   .   .   . 111
  6.3 Training of RBF networks . . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   . 114
       6.3.1 Centers and widths of RBF neurons . . . . . .                    .   .   .   .   .   .   .   .   .   . 115
  6.4 Growing RBF networks . . . . . . . . . . . . . . . . .                  .   .   .   .   .   .   .   .   .   . 118
       6.4.1 Adding neurons . . . . . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   . 118
       6.4.2 Limiting the number of neurons . . . . . . . . .                 .   .   .   .   .   .   .   .   .   . 119
       6.4.3 Deleting neurons . . . . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   . 119
  6.5 Comparing RBF networks and multilayer perceptrons                       .   .   .   .   .   .   .   .   .   . 119
  Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .           .   .   .   .   .   .   .   .   .   . 120

7 Recurrent perceptron-like networks (depends on chapter 5)                                                           121
  7.1 Jordan networks . . . . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   122
  7.2 Elman networks . . . . . . . . . . . . . . . . . . . . .            .   .   .   .   .   .   .   .   .   .   .   123
  7.3 Training recurrent networks . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   124
      7.3.1 Unfolding in time . . . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   125
      7.3.2 Teacher forcing . . . . . . . . . . . . . . . . .             .   .   .   .   .   .   .   .   .   .   .   127
      7.3.3 Recurrent backpropagation . . . . . . . . . .                 .   .   .   .   .   .   .   .   .   .   .   127
      7.3.4 Training with evolution . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   .   .   127

8 Hopfield networks                                                                                                 129
  8.1 Inspired by magnetism . . . . . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   . 129
  8.2 Structure and functionality . . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   . 129
      8.2.1 Input and output of a Hopfield network . . . .                    .   .   .   .   .   .   .   .   .   . 130
      8.2.2 Significance of weights . . . . . . . . . . . . . .               .   .   .   .   .   .   .   .   .   . 131
      8.2.3 Change in the state of neurons . . . . . . . . .                  .   .   .   .   .   .   .   .   .   . 131
  8.3 Generating the weight matrix . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   . 132
  8.4 Autoassociation and traditional application . . . . . .                 .   .   .   .   .   .   .   .   .   . 133
  8.5 Heteroassociation and analogies to neural data storage                  .   .   .   .   .   .   .   .   .   . 134
      8.5.1 Generating the heteroassociative matrix . . . .                   .   .   .   .   .   .   .   .   .   . 135
      8.5.2 Stabilizing the heteroassociations . . . . . . . .                .   .   .   .   .   .   .   .   .   . 135
      8.5.3 Biological motivation of heterassociation . . . .                 .   .   .   .   .   .   .   .   .   . 136



xiv                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                              Contents


   8.6 Continuous Hopfield networks . . . . . . . . . . . . . . . . . . . . . . . . 136
   Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

9 Learning vector quantization                                                                                                      139
  9.1 About quantization . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 139
  9.2 Purpose of LVQ . . . . . . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 140
  9.3 Using codebook vectors . . . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 140
  9.4 Adjusting codebook vectors . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 141
       9.4.1 The procedure of learning        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 141
  9.5 Connection to neural networks .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 143
  Exercises . . . . . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 143


III Unsupervised learning network paradigms                                                                                           145

10 Self-organizing feature maps                                                                                                       147
   10.1 Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                       .   .   .   .   147
   10.2 Functionality and output interpretation . . . . . . . . . . . . . .                                           .   .   .   .   149
   10.3 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                      .   .   .   .   149
        10.3.1 The topology function . . . . . . . . . . . . . . . . . . . .                                          .   .   .   .   150
        10.3.2 Monotonically decreasing learning rate and neighborhood                                                .   .   .   .   152
   10.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                        .   .   .   .   155
        10.4.1 Topological defects . . . . . . . . . . . . . . . . . . . . . .                                        .   .   .   .   156
   10.5 Adjustment of resolution and position-dependent learning rate .                                               .   .   .   .   156
   10.6 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                       .   .   .   .   159
        10.6.1 Interaction with RBF networks . . . . . . . . . . . . . . .                                            .   .   .   .   161
   10.7 Variations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                      .   .   .   .   161
        10.7.1 Neural gas . . . . . . . . . . . . . . . . . . . . . . . . . . .                                       .   .   .   .   161
        10.7.2 Multi-SOMs . . . . . . . . . . . . . . . . . . . . . . . . . .                                         .   .   .   .   163
        10.7.3 Multi-neural gas . . . . . . . . . . . . . . . . . . . . . . .                                         .   .   .   .   163
        10.7.4 Growing neural gas . . . . . . . . . . . . . . . . . . . . . .                                         .   .   .   .   164
   Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                      .   .   .   .   164

11 Adaptive resonance theory                                                                                                        165
   11.1 Task and structure of an ART network . . .                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 165
        11.1.1 Resonance . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 166
   11.2 Learning process . . . . . . . . . . . . . . .                .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 167
        11.2.1 Pattern input and top-down learning                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 167
        11.2.2 Resonance and bottom-up learning .                     .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 167
        11.2.3 Adding an output neuron . . . . . .                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   . 167



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                        xv
Contents                                                                                                         dkriesel.com


      11.3 Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167


IV Excursi, appendices and registers                                                                                                 169

A Excursus: Cluster analysis and regional and online learnable fields                                                                171
  A.1 k-means clustering . . . . . . . . . . . . . . . . . . . . . . . . .                                       .   .   .   .   .   172
  A.2 k-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . .                                        .   .   .   .   .   172
  A.3 ε-nearest neighboring . . . . . . . . . . . . . . . . . . . . . . .                                        .   .   .   .   .   173
  A.4 The silhouette coefficient . . . . . . . . . . . . . . . . . . . . . .                                     .   .   .   .   .   173
  A.5 Regional and online learnable fields . . . . . . . . . . . . . . . .                                       .   .   .   .   .   175
       A.5.1 Structure of a ROLF . . . . . . . . . . . . . . . . . . . .                                         .   .   .   .   .   176
       A.5.2 Training a ROLF . . . . . . . . . . . . . . . . . . . . . .                                         .   .   .   .   .   177
       A.5.3 Evaluating a ROLF . . . . . . . . . . . . . . . . . . . .                                           .   .   .   .   .   178
       A.5.4 Comparison with popular clustering methods . . . . . .                                              .   .   .   .   .   179
       A.5.5 Initializing radii, learning rates and multiplier . . . . . .                                       .   .   .   .   .   180
       A.5.6 Application examples . . . . . . . . . . . . . . . . . . .                                          .   .   .   .   .   180
  Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                    .   .   .   .   .   180

B Excursus: neural networks used for prediction                                                                                      181
  B.1 About time series . . . . . . . . . . . . . . . . . . .                        .   .   .   .   .   .   .   .   .   .   .   .   181
  B.2 One-step-ahead prediction . . . . . . . . . . . . . .                          .   .   .   .   .   .   .   .   .   .   .   .   183
  B.3 Two-step-ahead prediction . . . . . . . . . . . . . .                          .   .   .   .   .   .   .   .   .   .   .   .   185
      B.3.1 Recursive two-step-ahead prediction . . . .                              .   .   .   .   .   .   .   .   .   .   .   .   185
      B.3.2 Direct two-step-ahead prediction . . . . . .                             .   .   .   .   .   .   .   .   .   .   .   .   185
  B.4 Additional optimization approaches for prediction .                            .   .   .   .   .   .   .   .   .   .   .   .   185
      B.4.1 Changing temporal parameters . . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   185
      B.4.2 Heterogeneous prediction . . . . . . . . . .                             .   .   .   .   .   .   .   .   .   .   .   .   187
  B.5 Remarks on the prediction of share prices . . . . .                            .   .   .   .   .   .   .   .   .   .   .   .   187

C Excursus: reinforcement learning                                                                                                   191
  C.1 System structure . . . . . . . . . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   192
      C.1.1 The gridworld . . . . . . . . .          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   192
      C.1.2 Agent und environment . . .              .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   193
      C.1.3 States, situations and actions           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   194
      C.1.4 Reward and return . . . . . .            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   195
      C.1.5 The policy . . . . . . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   196
  C.2 Learning process . . . . . . . . . . .         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   198
      C.2.1 Rewarding strategies . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   198
      C.2.2 The state-value function . . .           .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   199



xvi                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                       Contents


        C.2.3 Monte Carlo method . . . . .         . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   201
        C.2.4 Temporal difference learning         . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   202
        C.2.5 The action-value function . .        . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   203
        C.2.6 Q learning . . . . . . . . . . .     . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   204
   C.3 Example applications . . . . . . . . .      . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   205
        C.3.1 TD gammon . . . . . . . . .          . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   205
        C.3.2 The car in the pit . . . . . .       . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   205
        C.3.3 The pole balancer . . . . . .        . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   206
   C.4 Reinforcement learning in connection        with    neural    networks      .   .   .   .   .   .   .   207
   Exercises . . . . . . . . . . . . . . . . . .   . . .   . . . .   . . . . . .   .   .   .   .   .   .   .   207

Bibliography                                                                                                   209

List of Figures                                                                                                215

Index                                                                                                          219




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                xvii
               Part I

 From biology to formalization –
motivation, philosophy, history and
   realization of neural models




                                      1
             Chapter 1
             Introduction, motivation and history
                              How to teach a computer? You can either write a fixed program – or you can
                                    enable the computer to learn on its own. Living beings do not have any
                              programmer writing a program for developing their skills, which then only has
                                to be executed. They learn by themselves – without the previous knowledge
                                   from external impressions – and thus can solve problems better than any
                                  computer today. What qualities are needed to achieve such a behavior for
                              devices like computers? Can such cognition be adapted from biology? History,
                                 development, decline and resurgence of a wide approach to solve problems.




             1.1 Why neural networks?                      If we compare computer and brain1 , we
                                                           will note that, theoretically, the computer
                                                           should be more powerful than our brain:
             There are problem categories that cannot      It comprises 109 transistors with a switch-
             be formulated as an algorithm. Problems       ing time of 10−9 seconds. The brain con-
             that depend on many subtle factors, for ex-   tains 1011 neurons, but these only have a
             ample the purchase price of a real estate     switching time of about 10−3 seconds.
             which our brain can (approximately) cal-      The largest part of the brain is work-
             culate. Without an algorithm a computer       ing continuously, while the largest part of
             cannot do the same. Therefore the ques-       the computer is only passive data storage.
             tion to be asked is: How do we learn to       Thus, the brain is parallel and therefore
                                                                                                                    parallelism
             explore such problems?                        performing close to its theoretical maxi-
                                                            1 Of course, this comparison is - for obvious rea-
             Exactly – we learn; a capability comput-         sons - controversially discussed by biologists and
             ers obviously do not have. Humans have           computer scientists, since response time and quan-
Computers
   cannot    a brain that can learn. Computers have           tity do not tell anything about quality and perfor-
     learn   some processing units and memory. They           mance of the processing units as well as neurons
                                                              and transistors cannot be compared directly. Nev-
             allow the computer to perform the most           ertheless, the comparison serves its purpose and
             complex numerical calculations in a very         indicates the advantage of parallelism by means
             short time, but they are not adaptive.           of processing time.




                                                                                                               3
              Chapter 1 Introduction, motivation and history                                    dkriesel.com


                                                      Brain                       Computer
                  No. of processing units             ≈ 1011                      ≈ 109
                  Type of processing units            Neurons                     Transistors
                  Type of calculation                 massively parallel          usually serial
                  Data storage                        associative                 address-based
                  Switching time                      ≈ 10−3 s                    ≈ 10−9 s
                  Possible switching operations       ≈ 1013 1s                   ≈ 1018 1s
                  Actual switching operations         ≈ 1012 1s                   ≈ 1010 1s

              Table 1.1: The (flawed) comparison between brain and computer at a glance. Inspired by: [Zel94]




              mum, from which the computer is orders          eralize and associate data: After suc-
              of magnitude away (Table 1.1). Addition-        cessful training a neural network can find
              ally, a computer is static - the brain as       reasonable solutions for similar problems
              a biological neural network can reorganize      of the same class that were not explicitly
              itself during its "lifespan" and therefore is   trained. This in turn results in a high de-
              able to learn, to compensate errors and so      gree of fault tolerance against noisy in-
              forth.                                          put data.

              Within this text I want to outline how Fault tolerance is closely related to biolog-
              we can use the said characteristics of our ical neural networks, in which this charac-
              brain for a computer system.                 teristic is very distinct: As previously men-
                                                           tioned, a human has about 1011 neurons
              So the study of artificial neural networks that continuously reorganize themselves
              is motivated by their similarity to success- or are reorganized by external influences
              fully working biological systems, which - in (about 105 neurons can be destroyed while
              comparison to the overall system - consist in a drunken stupor, some types of food
              of very simple but numerous nerve cells or environmental influences can also de-
    simple
 but many     that work massively in parallel and (which stroy brain cells). Nevertheless, our cogni-
processing    is probably one of the most significant tive abilities are not significantly affected.
              aspects) have the capability to learn. Thus, the brain is tolerant against internal
      units                                                                                                     n. network
                                                                                                                fault
              There is no need to explicitly program a errors – and also against external errors,               tolerant
              neural network. For instance, it can learn for we can often read a really "dreadful
              from training samples or by means of en- scrawl" although the individual letters are
n. network
   capable    couragement - with a carrot and a stick, nearly impossible to read.
   to learn   so to speak (reinforcement learning).
                                                           Our modern technology, however, is not
              One result from this learning procedure is automatically fault-tolerant. I have never
              the capability of neural networks to gen- heard that someone forgot to install the



              4                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                    1.1 Why neural networks?


hard disk controller into a computer and        What types of neural networks particu-
therefore the graphics card automatically       larly develop what kinds of abilities and
took over its tasks, i.e. removed con-          can be used for what problem classes will
ductors and developed communication, so         be discussed in the course of this work.
that the system as a whole was affected
by the missing component, but not com-          In the introductory chapter I want to
pletely destroyed.                              clarify the following: "The neural net-
                                                work" does not exist. There are differ-
                                                                                            Important!
A disadvantage of this distributed fault-       ent paradigms for neural networks, how
tolerant storage is certainly the fact that     they are trained and where they are used.
we cannot realize at first sight what a neu-    My goal is to introduce some of these
ral neutwork knows and performs or where        paradigms and supplement some remarks
its faults lie. Usually, it is easier to per-   for practical application.
form such analyses for conventional algo-
rithms. Most often we can only trans-       We have already mentioned that our brain
fer knowledge into our neural network by    works massively in parallel, in contrast to
means of a learning procedure, which can    the functioning of a computer, i.e. every
cause several errors and is not always easy component is active at any time. If we
to manage.                                  want to state an argument for massive par-
                                            allel processing, then the 100-step rule
Fault tolerance of data, on the other hand, can be cited.
is already more sophisticated in state-of-
the-art technology: Let us compare a
record and a CD. If there is a scratch on a
record, the audio information on this spot
                                            1.1.1 The 100-step rule
will be completely lost (you will hear a
pop) and then the music goes on. On a CD Experiments showed that a human can
the audio data are distributedly stored: A recognize the picture of a familiar object
scratch causes a blurry sound in its vicin- or person in ≈ 0.1 seconds, which cor-
ity, but the data stream remains largely responds to a neuron switching time of
unaffected. The listener won’t notice any- ≈ 10−3 seconds in ≈ 100 discrete time
thing.                                      steps of parallel processing.
                                                                                            parallel
                                                                                            processing
So let us summarize the main characteris- A computer following the von Neumann
tics we try to adapt from biology:        architecture, however, can do practically
                                          nothing in 100 time steps of sequential pro-
   . Self-organization and learning capa-
                                          cessing, which are 100 assembler steps or
     bility,
                                          cycle steps.
   . Generalization capability and
                                          Now we want to look at a simple applica-
   . Fault tolerance.                     tion example for a neural network.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        5
Chapter 1 Introduction, motivation and history                                  dkriesel.com


                                                put is called H for "halt signal"). There-
                                                fore we need a mapping

                                                               f : R8 → B1 ,

                                                that applies the input signals to a robot
                                                activity.


                                                1.1.2.1 The classical way

                                                There are two ways of realizing this map-
                                                ping. On the one hand, there is the clas-
                                                sical way: We sit down and think for a
                                                while, and finally the result is a circuit or
                                                a small computer program which realizes
                                                the mapping (this is easily possible, since
Figure 1.1: A small robot with eight sensors    the example is very simple). After that
and two motors. The arrow indicates the driv-
                                                we refer to the technical reference of the
ing direction.
                                                sensors, study their characteristic curve in
                                                order to learn the values for the different
                                                obstacle distances, and embed these values
                                                into the aforementioned set of rules. Such
1.1.2 Simple application examples               procedures are applied in the classic artifi-
                                                cial intelligence, and if you know the exact
Let us assume that we have a small robot        rules of a mapping algorithm, you are al-
as shown in fig. 1.1. This robot has eight      ways well advised to follow this scheme.
distance sensors from which it extracts in-
put data: Three sensors are placed on the       1.1.2.2 The way of learning
front right, three on the front left, and two
on the back. Each sensor provides a real        On the other hand, more interesting and
numeric value at any time, that means we        more successful for many mappings and
are always receiving an input I ∈ R8 .          problems that are hard to comprehend
                                                straightaway is the way of learning: We
Despite its two motors (which will be
                                                show different possible situations to the
needed later) the robot in our simple ex-
                                                robot (fig. 1.2 on page 8), – and the robot
ample is not capable to do much: It shall
                                                shall learn on its own what to do in the
only drive on but stop when it might col-
                                                course of its robot life.
lide with an obstacle. Thus, our output
is binary: H = 0 for "Everything is okay, In this example the robot shall simply
drive on" and H = 1 for "Stop" (The out- learn when to stop. We first treat the



6                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                         1.1 Why neural networks?


                                                  Our example can be optionally expanded.
                                                  For the purpose of direction control it
                                                  would be possible to control the motors
                                                  of our robot separately2 , with the sensor
                                                  layout being the same. In this case we are
                                                  looking for a mapping

                                                                   f : R8 → R2 ,
Figure 1.3: Initially, we regard the robot control
as a black box whose inner life is unknown. The
black box receives eight real sensor values and which gradually controls the two motors
maps these values to a binary output value.        by means of the sensor inputs and thus
                                            cannot only, for example, stop the robot
                                            but also lets it avoid obstacles. Here it
                                            is more difficult to analytically derive the
                                            rules, and de facto a neural network would
neural network as a kind of black box be more appropriate.
(fig. 1.3). This means we do not know its
structure but just regard its behavior in Our goal is not to learn the samples by
practice.                                   heart, but to realize the principle behind
                                            them: Ideally, the robot should apply the
The situations in form of simply mea- neural network in any situation and be
sured sensor values (e.g. placing the robot able to avoid obstacles. In particular, the
in front of an obstacle, see illustration), robot should query the network continu-
which we show to the robot and for which ously and repeatedly while driving in order
we specify whether to drive on or to stop, to continously avoid obstacles. The result
are called training samples. Thus, a train- is a constant cycle: The robot queries the
ing sample consists of an exemplary input network. As a consequence, it will drive
and a corresponding desired output. Now in one direction, which changes the sen-
the question is how to transfer this knowl- sors values. Again the robot queries the
edge, the information, into the neural net- network and changes its position, the sen-
work.                                       sor values are changed once again, and so
                                            on. It is obvious that this system can also
The samples can be taught to a neural be adapted to dynamic, i.e changing, en-
network by using a simple learning pro- vironments (e.g. the moving obstacles in
cedure (a learning procedure is a simple our example).
algorithm or a mathematical formula. If
we have done everything right and chosen 2 There is a robot called Khepera with more or less
                                               similar characteristics. It is round-shaped, approx.
good samples, the neural network will gen-     7 cm in diameter, has two motors with wheels
eralize from these samples and find a uni-     and various sensors. For more information I rec-
versal rule when it has to stop.               ommend to refer to the internet.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                  7
Chapter 1 Introduction, motivation and history                                       dkriesel.com




Figure 1.2: The robot is positioned in a landscape that provides sensor values for different situa-
tions. We add the desired output values H and so receive our learning samples. The directions in
which the sensors are oriented are exemplarily applied to two robots.




1.2 A brief history of neural                neously with the history of programmable
    networks                                 electronic computers. The youth of this
                                             field of research, as with the field of com-
                                             puter science itself, can be easily recog-
The field of neural networks has, like any nized due to the fact that many of the
other field of science, a long history of cited persons are still with us.
development with many ups and downs,
as we will see soon. To continue the style
of my work I will not represent this history 1.2.1 The beginning
in text form but more compact in form of a
timeline. Citations and bibliographical ref-
erences are added mainly for those topics As soon as 1943 Warren McCulloch
that will not be further discussed in this        and Walter Pitts introduced mod-
text. Citations for keywords that will be         els of neurological networks, recre-
explained later are mentioned in the corre-       ated threshold switches based on neu-
sponding chapters.                                rons and showed that even simple
                                                  networks of this kind are able to
The history of neural networks begins in          calculate nearly any logic or arith-
the early 1940’s and thus nearly simulta-         metic function [MP43].         Further-



8                        D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                   1.2 History of neural networks




Figure 1.4: Some institutions of the field of neural networks. From left to right: John von Neu-
mann, Donald O. Hebb, Marvin Minsky, Bernard Widrow, Seymour Papert, Teuvo Kohonen, John
Hopfield, "in the order of appearance" as far as possible.




     more, the first computer precur-                brain information storage is realized
     sors ("electronic brains")were de-              as a distributed system. His thesis
     veloped, among others supported by              was based on experiments on rats,
     Konrad Zuse, who was tired of cal-              where only the extent but not the
     culating ballistic trajectories by hand.        location of the destroyed nerve tissue
                                                     influences the rats’ performance to
1947: Walter Pitts and Warren Mc-                    find their way out of a labyrinth.
    Culloch indicated a practical field
    of application (which was not men-
    tioned in their work from 1943), 1.2.2 Golden age
    namely the recognition of spacial pat-
    terns by neural networks [PM47].        1951: For his dissertation Marvin Min-
                                                sky developed the neurocomputer
1949: Donald O. Hebb formulated the
                                                Snark, which has already been capa-
    classical Hebbian rule [Heb49] which
                                                ble to adjust its weights3 automati-
    represents in its more generalized
                                                cally. But it has never been practi-
    form the basis of nearly all neural
                                                cally implemented, since it is capable
    learning procedures. The rule im-
                                                to busily calculate, but nobody really
    plies that the connection between two
                                                knows what it calculates.
    neurons is strengthened when both
    neurons are active at the same time. 1956: Well-known scientists and ambi-
    This change in strength is propor-          tious students met at the Dart-
    tional to the product of the two activ-     mouth Summer Research Project
    ities. Hebb could postulate this rule,      and discussed, to put it crudely, how
    but due to the absence of neurological      to simulate a brain. Differences be-
    research he was not able to verify it.      tween top-down and bottom-up re-
                                                search developed. While the early
1950: The      neuropsychologist    Karl
    Lashley defended the thesis that 3 We will learn soon what weights are.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                               9
               Chapter 1 Introduction, motivation and history                             dkriesel.com


                    supporters of artificial intelligence       modern microprocessors. One advan-
                    wanted to simulate capabilities by          tage the delta rule had over the origi-
                    means of software, supporters of neu-       nal perceptron learning algorithm was
                    ral networks wanted to achieve sys-         its adaptivity: If the difference be-
                    tem behavior by imitating the small-        tween the actual output and the cor-
                    est parts of the system – the neurons.      rect solution was large, the connect-
                                                                ing weights also changed in larger
               1957-1958: At the MIT, Frank Rosen-              steps – the smaller the steps, the
                   blatt, Charles Wightman and                  closer the target was. Disadvantage:
                   their coworkers developed the first          missapplication led to infinitesimal
                   successful neurocomputer, the Mark           small steps close to the target. In the
                   I perceptron, which was capable to           following stagnation and out of fear
development
 accelerates       recognize simple numerics by means           of scientific unpopularity of the neu-
                   of a 20 × 20 pixel image sensor and          ral networks ADALINE was renamed
                   electromechanically worked with 512          in adaptive linear element – which
                   motor driven potentiometers - each           was undone again later on.
                   potentiometer representing one vari-
                   able weight.                            1961: Karl Steinbuch introduced tech-
                                                               nical realizations of associative mem-
               1959: Frank Rosenblatt described dif-           ory, which can be seen as predecessors
                   ferent versions of the perceptron, for-     of today’s neural associative mem-
                   mulated and verified his perceptron         ories [Ste61]. Additionally, he de-
                   convergence theorem. He described           scribed concepts for neural techniques
                   neuron layers mimicking the retina,         and analyzed their possibilities and
                   threshold switches, and a learning          limits.
                   rule adjusting the connecting weights.
                                                           1965: In his book Learning Machines,
               1960: Bernard Widrow and Mar-                   Nils Nilsson gave an overview of
                   cian E. Hoff introduced the ADA-            the progress and works of this period
                   LINE (ADAptive LInear NEu-                  of neural network research. It was
                   ron) [WH60], a fast and precise             assumed that the basic principles of
                   adaptive learning system being the          self-learning and therefore, generally
                   first widely commercially used neu-         speaking, "intelligent" systems had al-
                   ral network: It could be found in           ready been discovered. Today this as-
                   nearly every analog telephone for real-     sumption seems to be an exorbitant
                   time adaptive echo filtering and was        overestimation, but at that time it
                   trained by menas of the Widrow-Hoff         provided for high popularity and suf-
       first
     spread        rule or delta rule. At that time Hoff,      ficient research funds.
        use        later co-founder of Intel Corporation,
                   was a PhD student of Widrow, who 1969: Marvin Minsky and Seymour
                   himself is known as the inventor of         Papert published a precise mathe-



               10                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                                           1.2 History of neural networks


                   matical analysis of the perceptron         of view by James A. Anderson
                   [MP69] to show that the perceptron         [And72].
                   model was not capable of representing
                   many important problems (keywords: 1973: Christoph von der Malsburg
                   XOR problem and linear separability),      used a neuron model that was non-
                   and so put an end to overestimation,       linear and biologically more moti-
                   popularity and research funds. The         vated   [vdM73].
   research
                   implication that more powerful mod-
funds were
                                                          1974: For his dissertation in Harvard
   stopped         els would show exactly the same prob-
                                                              Paul Werbos developed a learning
                   lems and the forecast that the entire
                                                              procedure called backpropagation of
                   field would be a research dead end re-
                                                              error [Wer74], but it was not until
                   sulted in a nearly complete decline in
                                                              one decade later that this procedure
                   research funds for the next 15 years
                                                              reached today’s importance.
                   – no matter how incorrect these fore-                                              backprop
                   casts were from today’s point of view. 1976-1980 and thereafter: Stephen           developed

                                                              Grossberg presented many papers
                                                              (for instance [Gro76]) in which
              1.2.3 Long silence and slow
                                                              numerous neural models are analyzed
                      reconstruction
                                                              mathematically.      Furthermore, he
                                                              dedicated himself to the problem of
              The research funds were, as previously-         keeping a neural network capable
              mentioned, extremely short. Everywhere          of learning without destroying
              research went on, but there were neither        already learned associations. Under
              conferences nor other events and therefore      cooperation of Gail Carpenter
              only few publications. This isolation of        this led to models of adaptive
              individual researchers provided for many        resonance theory (ART).
              independently developed neural network
              paradigms: They researched, but there 1982: Teuvo Kohonen described the
              was no discourse among them.                    self-organizing feature maps
                                                              (SOM) [Koh82, Koh98] – also
              In spite of the poor appreciation the field
                                                              known as Kohonen maps. He was
              received, the basic theories for the still
                                                              looking for the mechanisms involving
              continuing renaissance were laid at that
                                                              self-organization in the brain (He
              time:
                                                              knew that the information about the
              1972: Teuvo Kohonen introduced a                creation of a being is stored in the
                   model of the linear associator,            genome, which has, however, not
                   a model of an associative memory           enough memory for a structure like
                   [Koh72]. In the same year, such a          the brain. As a consequence, the
                   model was presented independently          brain has to organize and create
                   and from a neurophysiologist’s point       itself for the most part).



              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                   11
              Chapter 1 Introduction, motivation and history                               dkriesel.com


                  John Hopfield also invented the              time a certain kind of fatigue spread
                  so-called Hopfield networks [Hop82]          in the field of artificial intelligence,
                  which are inspired by the laws of mag-       caused by a series of failures and un-
                  netism in physics. They were not             fulfilled hopes.
                  widely used in technical applications,
                                                          From this time on, the development of
                  but the field of neural networks slowly
                                                               the field of research has almost been
                  regained importance.
                                                               explosive. It can no longer be item-
              1983: Fukushima, Miyake and Ito in-              ized, but some of its results will be
                  troduced the neural model of the             seen in the following.
                  Neocognitron which could recognize
                  handwritten characters [FMI83] and
                  was an extension of the Cognitron net- Exercises
                  work already developed in 1975.
                                                          Exercise 1. Give one example for each
                                                          of the following topics:
              1.2.4 Renaissance
                                                               . A book on neural networks or neuroin-
              Through the influence of John Hopfield,            formatics,
              who had personally convinced many re-            . A collaborative group of a university
              searchers of the importance of the field,          working with neural networks,
              and the wide publication of backpro-
              pagation by Rumelhart, Hinton and                . A software tool realizing neural net-
              Williams, the field of neural networks             works ("simulator"),
              slowly showed signs of upswing.                  . A company using neural networks,
                                                                 and
              1985: John Hopfield published an arti-
                  cle describing a way of finding accept-      . A product or service being realized by
                  able solutions for the Travelling Sales-       means of neural networks.
                  man problem by using Hopfield nets.
Renaissance                                                  Exercise 2. Show at least four applica-
              1986: The backpropagation of error learn-      tions of technical neural networks: two
                  ing procedure as a generalization of       from the field of pattern recognition and
                  the delta rule was separately devel-       two from the field of function approxima-
                  oped and widely published by the Par-      tion.
                  allel Distributed Processing Group
                  [RHW86a]:      Non-linearly-separable      Exercise 3. Briefly characterize the four
                  problems could be solved by multi-         development phases of neural networks
                  layer perceptrons, and Marvin Min-         and give expressive examples for each
                  sky’s negative evaluations were dis-       phase.
                  proven at a single blow. At the same



              12                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
Chapter 2
Biological neural networks
                    How do biological systems solve problems? How does a system of neurons
                 work? How can we understand its functionality? What are different quantities
                        of neurons able to do? Where in the nervous system does information
                     processing occur? A short biological overview of the complexity of simple
                   elements of neural information processing followed by some thoughts about
                                         their simplification in order to technically adapt them.




Before we begin to describe the technical       2.1 The vertebrate nervous
side of neural networks, it would be use-           system
ful to briefly discuss the biology of neu-
ral networks and the cognition of living
organisms – the reader may skip the fol-       The entire information processing system,
lowing chapter without missing any tech-       i.e. the vertebrate nervous system, con-
nical information. On the other hand I         sists of the central nervous system and the
recommend to read the said excursus if         peripheral nervous system, which is only
you want to learn something about the          a first and simple subdivision. In real-
underlying neurophysiology and see that        ity, such a rigid subdivision does not make
our small approaches, the technical neural     sense, but here it is helpful to outline the
networks, are only caricatures of nature       information processing in a body.
– and how powerful their natural counter-
parts must be when our small approaches
are already that effective. Now we want         2.1.1 Peripheral and central
to take a brief look at the nervous system            nervous system
of vertebrates: We will start with a very
rough granularity and then proceed with        The peripheral nervous system (PNS)
the brain and up to the neural level. For      comprises the nerves that are situated out-
further reading I want to recommend the        side of the brain or the spinal cord. These
books [CR00, KSJ00], which helped me a         nerves form a branched and very dense net-
lot during this chapter.                       work throughout the whole body. The pe-



                                                                                              13
Chapter 2 Biological neural networks                                                dkriesel.com


ripheral nervous system includes, for ex-
ample, the spinal nerves which pass out
of the spinal cord (two within the level of
each vertebra of the spine) and supply ex-
tremities, neck and trunk, but also the cra-
nial nerves directly leading to the brain.

The central nervous system (CNS),
however, is the "main-frame" within the
vertebrate. It is the place where infor-
mation received by the sense organs are
stored and managed. Furthermore, it con-
trols the inner processes in the body and,
last but not least, coordinates the mo-
tor functions of the organism. The ver-
tebrate central nervous system consists of
the brain and the spinal cord (Fig. 2.1).
However, we want to focus on the brain,
which can - for the purpose of simplifica-
tion - be divided into four areas (Fig. 2.2
on the next page) to be discussed here.


2.1.2 The cerebrum is responsible
      for abstract thinking
      processes.

The cerebrum (telencephalon) is one of
the areas of the brain that changed most
during evolution. Along an axis, running
from the lateral face to the back of the
head, this area is divided into two hemi-
spheres, which are organized in a folded
structure. These cerebral hemispheres
are connected by one strong nerve cord
("bar") and several small ones. A large
number of neurons are located in the cere-
bral cortex (cortex) which is approx. 2-
4 cm thick and divided into different cor-
tical fields, each having a specific task to Figure 2.1: Illustration of the central nervous
                                               system with spinal cord and brain.


14                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                  2.1 The vertebrate nervous system


                                                     and errors are continually corrected. For
                                                     this purpose, the cerebellum has direct
                                                     sensory information about muscle lengths
                                                     as well as acoustic and visual informa-
                                                     tion. Furthermore, it also receives mes-
                                                     sages about more abstract motor signals
                                                     coming from the cerebrum.

                                                     In the human brain the cerebellum is con-
                                                     siderably smaller than the cerebrum, but
                                                     this is rather an exception. In many ver-
                                                     tebrates this ratio is less pronounced. If
Figure 2.2: Illustration of the brain. The col-      we take a look at vertebrate evolution, we
ored areas of the brain are discussed in the text.   will notice that the cerebellum is not "too
The more we turn from abstract information pro-      small" but the cerebum is "too large" (at
cessing to direct reflexive processing, the darker   least, it is the most highly developed struc-
the areas of the brain are colored.
                                                     ture in the vertebrate brain). The two re-
                                                     maining brain areas should also be briefly
                                                     discussed: the diencephalon and the brain-
                                                     stem.
fulfill. Primary cortical fields are re-
sponsible for processing qualitative infor-
mation, such as the management of differ-
                                            2.1.4 The diencephalon controls
ent perceptions (e.g. the visual cortex
                                                    fundamental physiological
is responsible for the management of vi-
                                                    processes
sion). Association cortical fields, how-
ever, perform more abstract association
and thinking processes; they also contain The interbrain (diencephalon) includes
our memory.                                 parts of which only the thalamus will
                                                                                                     thalamus
                                            be briefly discussed: This part of the di-               filters
                                            encephalon mediates between sensory and                  incoming
2.1.3 The cerebellum controls and           motor signals and the cerebrum. Particu-
                                                                                                     data

         coordinates motor functions        larly, the thalamus decides which part of
                                            the information is transferred to the cere-
The cerebellum is located below the cere- brum, so that especially less important
brum, therefore it is closer to the spinal sensory perceptions can be suppressed at
cord. Accordingly, it serves less abstract short notice to avoid overloads. Another
functions with higher priority: Here, large part of the diencephalon is the hypotha-
parts of motor coordination are performed, lamus, which controls a number of pro-
i.e., balance and movements are controlled cesses within the body. The diencephalon



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                15
Chapter 2 Biological neural networks                                          dkriesel.com


is also heavily involved in the human cir- All parts of the nervous system have one
cadian rhythm ("internal clock") and the thing in common: information processing.
sensation of pain.                            This is accomplished by huge accumula-
                                              tions of billions of very similar cells, whose
                                              structure is very simple but which com-
2.1.5 The brainstem connects the              municate continuously. Large groups of
        brain with the spinal cord and these cells send coordinated signals and
        controls reflexes.                    thus reach the enormous information pro-
                                              cessing capacity we are familiar with from
                                              our brain. We will now leave the level of
In comparison with the diencephalon the
                                              brain areas and continue with the cellular
brainstem or the (truncus cerebri) re-
                                              level of the body - the level of neurons.
spectively is phylogenetically much older.
Roughly speaking, it is the "extended
spinal cord" and thus the connection be-
tween brain and spinal cord. The brain- 2.2 Neurons are information
stem can also be divided into different ar-          processing cells
eas, some of which will be exemplarily in-
troduced in this chapter. The functions
will be discussed from abstract functions Before specifying the functions and pro-
towards more fundamental ones. One im- cesses within a neuron, we will give a
portant component is the pons (=bridge), rough description of neuron functions: A
a kind of transit station for many nerve sig- neuron is nothing more than a switch with
nals from brain to body and vice versa.       information input and output. The switch
                                              will be activated if there are enough stim-
If the pons is damaged (e.g. by a cere- uli of other neurons hitting the informa-
bral infarct), then the result could be the tion input. Then, at the information out-
locked-in syndrome – a condition in put, a pulse is sent to, for example, other
which a patient is "walled-in" within his neurons.
own body. He is conscious and aware
with no loss of cognitive function, but can-
not move or communicate by any means.
                                              2.2.1 Components of a neuron
Only his senses of sight, hearing, smell and
taste are generally working perfectly nor-
mal. Locked-in patients may often be able Now we want to take a look at the com-
to communicate with others by blinking or ponents of a neuron (Fig. 2.3 on the fac-
moving their eyes.                            ing page). In doing so, we will follow the
                                              way the electrical information takes within
Furthermore, the brainstem is responsible the neuron. The dendrites of a neuron
for many fundamental reflexes, such as the receive the information by special connec-
blinking reflex or coughing.                  tions, the synapses.



16                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                                                    2.2 The neuron




                Figure 2.3: Illustration of a biological neuron with the components discussed in this text.




             2.2.1.1 Synapses weight the individual           The chemical synapse is the more dis-
                     parts of information                     tinctive variant. Here, the electrical cou-
                                                              pling of source and target does not take
                                                              place, the coupling is interrupted by the
             Incoming signals from other neurons or
                                                              synaptic cleft. This cleft electrically sep-
             cells are transferred to a neuron by special
                                                              arates the presynaptic side from the post-
             connections, the synapses. Such connec-
                                                              synaptic one. You might think that, never-
             tions can usually be found at the dendrites
                                                              theless, the information has to flow, so we
             of a neuron, sometimes also directly at the
                                                              will discuss how this happens: It is not an
             soma. We distinguish between electrical
                                                              electrical, but a chemical process. On the
             and chemical synapses.
                                                              presynaptic side of the synaptic cleft the
                                                              electrical signal is converted into a chemi-
             The electrical synapse is the simpler            cal signal, a process induced by chemical
electrical
synapse:     variant. An electrical signal received by        cues released there (the so-called neuro-
   simple    the synapse, i.e. coming from the presy-         transmitters). These neurotransmitters
             naptic side, is directly transferred to the      cross the synaptic cleft and transfer the
             postsynaptic nucleus of the cell. Thus,          information into the nucleus of the cell
             there is a direct, strong, unadjustable          (this is a very simple explanation, but later
             connection between the signal transmitter        on we will see how this exactly works),
             and the signal receiver, which is, for exam-     where it is reconverted into electrical in-
             ple, relevant to shortening reactions that       formation. The neurotransmitters are de-
             must be "hard coded" within a living or-         graded very fast, so that it is possible to re-
             ganism.



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                  17
           Chapter 2 Biological neural networks                                       dkriesel.com


           lease very precise information pulses here, many different sources, which are then
           too.                                         transferred into the nucleus of the cell.
                                                        The amount of branching dendrites is also
           In spite of the more complex function- called dendrite tree.
 cemical
 synapse   ing, the chemical synapse has - compared
 is more   with the electrical synapse - utmost advan-
           tages:
complex
but also                                                2.2.1.3 In the soma the weighted
    more
powerful   One-way connection: A              chemical            information is accumulated
                synapse is a one-way connection.
                Due to the fact that there is no direct After the cell nucleus (soma) has re-
                electrical connection between the ceived a plenty of activating (=stimulat-
                pre- and postsynaptic area, electrical ing) and inhibiting (=diminishing) signals
                pulses in the postsynaptic area by synapses or dendrites, the soma accu-
                cannot flash over to the presynaptic mulates these signals. As soon as the ac-
                area.                                   cumulated signal exceeds a certain value
                                                        (called threshold value), the cell nucleus
           Adjustability: There is a large number of
                                                        of the neuron activates an electrical pulse
                different neurotransmitters that can
                                                        which then is transmitted to the neurons
                also be released in various quantities
                                                        connected to the current one.
                in a synaptic cleft. There are neuro-
                transmitters that stimulate the post-
                synaptic cell nucleus, and others that
                slow down such stimulation. Some 2.2.1.4 The axon transfers outgoing
                synapses transfer a strongly stimulat-            pulses
                ing signal, some only weakly stimu-
                lating ones. The adjustability varies The pulse is transferred to other neurons
                a lot, and one of the central points by means of the axon. The axon is a
                in the examination of the learning long, slender extension of the soma. In
                ability of the brain is, that here the an extreme case, an axon can stretch up
                synapses are variable, too. That is, to one meter (e.g. within the spinal cord).
                over time they can form a stronger or The axon is electrically isolated in order
                weaker connection.                      to achieve a better conduction of the elec-
                                                        trical signal (we will return to this point
                                                        later on) and it leads to dendrites, which
           2.2.1.2 Dendrites collect all parts of       transfer the information to, for example,
                     information                        other neurons. So now we are back at the
                                                        beginning of our description of the neuron
           Dendrites branch like trees from the cell elements. An axon can, however, transfer
           nucleus of the neuron (which is called information to other kinds of cells in order
           soma) and receive electrical signals from to control them.



           18                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                               2.2 The neuron


2.2.2 Electrochemical processes in              ron, i.e., we assume that no electrical sig-
      the neuron and its                        nals are received from the outside. In this
      components                                case, the membrane potential is −70 mV.
                                                Since we have learned that this potential
After having pursued the path of an elec-       depends on the concentration gradients of
trical signal from the dendrites via the        various ions, there is of course the central
synapses to the nucleus of the cell and         question of how to maintain these concen-
from there via the axon into other den-         tration gradients: Normally, diffusion pre-
drites, we now want to take a small step        dominates and therefore each ion is eager
from biology towards technology. In doing       to decrease concentration gradients and
so, a simplified introduction of the electro-   to spread out evenly. If this happens,
chemical information processing should be       the membrane potential will move towards
provided.                                       0 mV, so finally there would be no mem-
                                                brane potential anymore. Thus, the neu-
                                                ron actively maintains its membrane po-
2.2.2.1 Neurons maintain electrical             tential to be able to process information.
        membrane potential                      How does this work?
                                                The secret is the membrane itself, which is
One fundamental aspect is the fact that         permeable to some ions, but not for others.
compared to their environment the neu-          To maintain the potential, various mecha-
rons show a difference in electrical charge,    nisms are in progress at the same time:
a potential. In the membrane (=enve-
lope) of the neuron the charge is differentConcentration gradient: As     described
from the charge on the outside. This dif-     above the ions try to be as uniformly
ference in charge is a central concept that   distributed as possible.       If the
is important to understand the processes      concentration of an ion is higher on
within the neuron. The difference is called   the inside of the neuron than on
membrane potential. The membrane              the outside, it will try to diffuse
potential, i.e., the difference in charge, is to the outside and vice versa.
created by several kinds of charged atoms     The positively charged ion K+
(ions), whose concentration varies within     (potassium) occurs very frequently
and outside of the neuron. If we penetrate    within the neuron but less frequently
the membrane from the inside outwards,        outside of the neuron, and therefore
we will find certain kinds of ions more of-   it slowly diffuses out through the
ten or less often than on the inside. This    neuron’s membrane. But another
descent or ascent of concentration is called  group of negative ions, collectively
a concentration gradient.                     called A− , remains within the neuron
                                              since the membrane is not permeable
Let us first take a look at the membrane      to them. Thus, the inside of the
potential in the resting state of the neu-    neuron becomes negatively charged.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          19
Chapter 2 Biological neural networks                                        dkriesel.com


     Negative A ions remain, positive K      with its environment. But even with these
     ions disappear, and so the inside of    two ions a standstill with all gradients be-
     the cell becomes more negative. The     ing balanced out could still be achieved.
     result is another gradient.             Now the last piece of the puzzle gets into
                                             the game: a "pump" (or rather, the protein
Electrical Gradient: The electrical gradi- ATP) actively transports ions against the
      ent acts contrary to the concentration direction they actually want to take!
      gradient. The intracellular charge is
      now very strong, therefore it attracts Sodium is actively pumped out of the cell,
      positive ions: K+ wants to get back         although it tries to get into the cell
      into the cell.                              along the concentration gradient and
                                                  the electrical gradient.
If these two gradients were now left alone,
they would eventually balance out, reach Potassium, however, diffuses strongly out
a steady state, and a membrane poten-             of the cell, but is actively pumped
tial of −85 mV would develop. But we              back into it.
want to achieve a resting membrane po-
tential of −70 mV, thus there seem to ex- For this reason the pump is also called
ist some disturbances which prevent this. sodium-potassium pump. The pump
Furthermore, there is another important maintains the concentration gradient for
ion, Na+ (sodium), for which the mem- the sodium as well as for the potassium,
brane is not very permeable but which, so that some sort of steady state equilib-
however, slowly pours through the mem- rium is created and finally the resting po-
brane into the cell. As a result, the sodium tential is −70 mV as observed. All in all
is driven into the cell all the more: On the the membrane potential is maintained by
one hand, there is less sodium within the the fact that the membrane is imperme-
neuron than outside the neuron. On the able to some ions and other ions are ac-
other hand, sodium is positively charged tively pumped against the concentration
but the interior of the cell has negative and electrical gradients. Now that we
charge, which is a second reason for the know that each neuron has a membrane
sodium wanting to get into the cell.         potential we want to observe how a neu-
                                             ron receives and transmits signals.
Due to the low diffusion of sodium into the
cell the intracellular sodium concentration
increases. But at the same time the inside 2.2.2.2 The neuron is activated by
of the cell becomes less negative, so that            changes in the membrane
  +
K pours in more slowly (we can see that               potential
this is a complex mechanism where every-
thing is influenced by everything). The Above we have learned that sodium and
sodium shifts the intracellular equilibrium potassium can diffuse through the mem-
from negative to less negative, compared brane - sodium slowly, potassium faster.



20                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                              2.2 The neuron


They move through channels within the Stimulus up to the threshold: A stimu-
membrane, the sodium and potassium               lus opens channels so that sodium
channels.       In addition to these per-        can pour in. The intracellular charge
manently open channels responsible for           becomes more positive. As soon as
diffusion and balanced by the sodium-            the membrane potential exceeds the
potassium pump, there also exist channels        threshold of −55 mV, the action po-
that are not always open but which only          tential is initiated by the opening of
response "if required". Since the opening        many sodium channels.
of these channels changes the concentra-
tion of ions within and outside of the mem- Depolarization: Sodium is pouring in. Re-
brane, it also changes the membrane po-          member: Sodium wants to pour into
tential.                                         the cell because there is a lower in-
                                                 tracellular than extracellular concen-
These controllable channels are opened as        tration of sodium. Additionally, the
soon as the accumulated received stimulus        cell is dominated by a negative en-
exceeds a certain threshold. For example,        vironment which attracts the posi-
stimuli can be received from other neurons       tive sodium ions. This massive in-
or have other causes. There exist, for ex-       flux of sodium drastically increases
ample, specialized forms of neurons, the         the membrane potential - up to ap-
sensory cells, for which a light incidence       prox. +30 mV - which is the electrical
could be such a stimulus. If the incom-          pulse, i.e., the action potential.
ing amount of light exceeds the threshold,
controllable channels are opened.             Repolarization: Now the sodium channels
                                                 are closed and the potassium channels
The said threshold (the threshold poten-         are opened. The positively charged
tial) lies at about −55 mV. As soon as the       ions want to leave the positive inte-
received stimuli reach this value, the neu-      rior of the cell. Additionally, the intra-
ron is activated and an electrical signal,       cellular concentration is much higher
an action potential, is initiated. Then          than the extracellular one, which in-
this signal is transmitted to the cells con-     creases the efflux of ions even more.
nected to the observed neuron, i.e. the          The interior of the cell is once again
cells "listen" to the neuron. Now we want        more negatively charged than the ex-
to take a closer look at the different stages    terior.
of the action potential (Fig. 2.4 on the next
page):                                        Hyperpolarization: Sodium as well as
                                                 potassium channels are closed again.
Resting state: Only the permanently              At first the membrane potential is
     open sodium and potassium channels          slightly more negative than the rest-
     are permeable.        The membrane          ing potential. This is due to the
     potential is at −70 mV and actively         fact that the potassium channels close
     kept there by the neuron.                   more slowly. As a result, (positively



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         21
Chapter 2 Biological neural networks                                        dkriesel.com




                    Figure 2.4: Initiation of action potential over time.




22                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                    2.2 The neuron


    charged) potassium effuses because of          Now you may assume that these less in-
    its lower extracellular concentration.         sulated nodes are a disadvantage of the
    After a refractory period of 1 − 2             axon - however, they are not. At the
    ms the resting state is re-established         nodes, mass can be transferred between
    so that the neuron can react to newly          the intracellular and extracellular area, a
    applied stimuli with an action poten-          transfer that is impossible at those parts
    tial. In simple terms, the refractory          of the axon which are situated between
    period is a mandatory break a neu-             two nodes (internodes) and therefore in-
    ron has to take in order to regenerate.        sulated by the myelin sheath. This mass
    The shorter this break is, the more            transfer permits the generation of signals
    often a neuron can fire per time.              similar to the generation of the action po-
                                                   tential within the soma. The action po-
Then the resulting pulse is transmitted by
                                                   tential is transferred as follows: It does
the axon.
                                                   not continuously travel along the axon but
                                                   jumps from node to node. Thus, a series
2.2.2.3 In the axon a pulse is                     of depolarization travels along the nodes of
        conducted in a saltatory way               Ranvier. One action potential initiates the
                                                   next one, and mostly even several nodes
                                                   are active at the same time here. The
We have already learned that the axon
                                                   pulse "jumping" from node to node is re-
is used to transmit the action potential
                                                   sponsible for the name of this pulse con-
across long distances (remember: You will
                                                   ductor: saltatory conductor.
find an illustration of a neuron including
an axon in Fig. 2.3 on page 17). The axon             Obviously, the pulse will move faster if its
is a long, slender extension of the soma.             jumps are larger. Axons with large in-
In vertebrates it is normally coated by a             ternodes (2 mm) achieve a signal disper-
myelin sheath that consists of Schwann                sion of approx. 180 meters per second.
cells (in the PNS) or oligodendrocytes                However, the internodes cannot grow in-
(in the CNS) 1 , which insulate the axon              definitely, since the action potential to be
very well from electrical activity. At a dis-         transferred would fade too much until it
tance of 0.1 − 2mm there are gaps between             reaches the next node. So the nodes have
these cells, the so-called nodes of Ran-              a task, too: to constantly amplify the sig-
vier. The said gaps appear where one in-              nal. The cells receiving the action poten-
sulate cell ends and the next one begins.             tial are attached to the end of the axon –
It is obvious that at such a node the axon            often connected by dendrites and synapses.
is less insulated.                                    As already indicated above, the action po-
1 Schwann cells as well as oligodendrocytes are vari- tentials are not only generated by informa-
  eties of the glial cells. There are about 50 times tion received by the dendrites from other
  more glial cells than neurons: They surround the neurons.
  neurons (glia = glue), insulate them from each
  other, provide energy, etc.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                               23
Chapter 2 Biological neural networks                                          dkriesel.com


2.3 Receptor cells are                          2.3.1 There are different receptor
    modified neurons                                  cells for various types of
                                                      perceptions


                                                Primary receptors transmit their pulses
                                                directly to the nervous system. A good
Action potentials can also be generated by
                                                example for this is the sense of pain.
sensory information an organism receives
                                                Here, the stimulus intensity is propor-
from its environment through its sensory
                                                tional to the amplitude of the action po-
cells. Specialized receptor cells are able
                                                tential. Technically, this is an amplitude
to perceive specific stimulus energies such
                                                modulation.
as light, temperature and sound or the ex-
istence of certain molecules (like, for exam-
ple, the sense of smell). This is working       Secondary receptors, however, continu-
because of the fact that these sensory cells    ously transmit pulses. These pulses con-
are actually modified neurons. They do          trol the amount of the related neurotrans-
not receive electrical signals via dendrites    mitter, which is responsible for transfer-
but the existence of the stimulus being         ring the stimulus. The stimulus in turn
specific for the receptor cell ensures that     controls the frequency of the action poten-
the ion channels open and an action po-         tial of the receiving neuron. This process
tential is developed. This process of trans-    is a frequency modulation, an encoding of
forming stimulus energy into changes in         the stimulus, which allows to better per-
the membrane potential is called sensory        ceive the increase and decrease of a stimu-
transduction. Usually, the stimulus en-         lus.
ergy itself is too weak to directly cause
nerve signals. Therefore, the signals are
amplified either during transduction or by      There can be individual receptor cells or
means of the stimulus-conducting ap-            cells forming complex sensory organs (e.g.
paratus. The resulting action potential         eyes or ears). They can receive stimuli
can be processed by other neurons and is        within the body (by means of the intero-
then transmitted into the thalamus, which       ceptors) as well as stimuli outside of the
is, as we have already learned, a gateway       body (by means of the exteroceptors).
to the cerebral cortex and therefore can re-
ject sensory impressions according to cur-      After having outlined how information is
rent relevance and thus prevent an abun-        received from the environment, it will be
dance of information to be managed.             interesting to look at how the information
                                                is processed.




24                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                            2.3 Receptor cells


2.3.2 Information is processed on                 this subject is to prevent the transmis-
      every level of the nervous                  sion of "continuous stimuli" to the cen-
      system                                      tral nervous system because of sen-
                                                  sory adaptation: Due to continu-
There is no reason to believe that all re-        ous stimulation many receptor cells
ceived information is transmitted to the          automatically become insensitive to
brain and processed there, and that the           stimuli. Thus, receptor cells are not
brain ensures that it is "output" in the          a direct mapping of specific stimu-
form of motor pulses (the only thing an           lus energy onto action potentials but
organism can actually do within its envi-         depend on the past. Other sensors
ronment is to move). The information pro-         change their sensitivity according to
cessing is entirely decentralized. In order       the situation: There are taste recep-
to illustrate this principle, we want to take     tors which respond more or less to the
a look at some examples, which leads us           same stimulus according to the nutri-
again from the abstract to the fundamen-          tional condition of the organism.
tal in our hierarchy of information process-
ing.
                                                . Even before a stimulus reaches the
  . It is certain that information is pro-        receptor cells, information processing
    cessed in the cerebrum, which is the          can already be executed by a preced-
    most developed natural information            ing signal carrying apparatus, for ex-
    processing structure.                         ample in the form of amplification:
  . The midbrain and the thalamus,                The external and the internal ear
    which serves – as we have already             have a specific shape to amplify the
    learned – as a gateway to the cere-           sound, which also allows – in asso-
    bral cortex, are situated much lower          ciation with the sensory cells of the
    in the hierarchy. The filtering of in-        sense of hearing – the sensory stim-
    formation with respect to the current         ulus only to increase logarithmically
    relevance executed by the midbrain            with the intensity of the heard sig-
    is a very important method of infor-          nal. On closer examination, this is
    mation processing, too. But even the          necessary, since the sound pressure of
    thalamus does not receive any prepro-         the signals for which the ear is con-
    cessed stimuli from the outside. Now          structed can vary over a wide expo-
    let us continue with the lowest level,        nential range. Here, a logarithmic
    the sensory cells.                            measurement is an advantage. Firstly,
                                                  an overload is prevented and secondly,
  . On the lowest level, i.e. at the recep-       the fact that the intensity measure-
    tor cells, the information is not only        ment of intensive signals will be less
    received and transferred but directly         precise, doesn’t matter as well. If a jet
    processed. One of the main aspects of         fighter is starting next to you, small



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         25
Chapter 2 Biological neural networks                                         dkriesel.com


     changes in the noise level can be ig- 2.3.3.1 Compound eyes and pinhole
     nored.                                         eyes only provide high temporal
                                                    or spatial resolution
Just to get a feeling for sensory organs
and information processing in the organ-
ism, we will briefly describe "usual" light Let us first take a look at the so-called
sensing organs, i.e. organs often found in compound eye (Fig. 2.5 on the next
nature. For the third light sensing organ page), which is, for example, common in
described below, the single lens eye, we insects and crustaceans. The compound
will discuss the information processing in eye consists of a great number of small,          Compound eye:
                                                                                             high temp.,
the eye.                                    individual eyes. If we look at the com-          low

                                              pound eye from the outside, the individ-
                                                                                             spatial
                                                                                             resolution
                                              ual eyes are clearly visible and arranged
2.3.3 An outline of common light              in a hexagonal pattern. Each individual
      sensing organs                          eye has its own nerve fiber which is con-
                                              nected to the insect brain. Since the indi-
                                              vidual eyes can be distinguished, it is ob-
For many organisms it turned out to be ex-    vious that the number of pixels, i.e. the
tremely useful to be able to perceive elec-   spatial resolution, of compound eyes must
tromagnetic radiation in certain regions of   be very low and the image is blurred. But
the spectrum. Consequently, sensory or-       compound eyes have advantages, too, espe-
gans have been developed which can de-        cially for fast-flying insects. Certain com-
tect such electromagnetic radiation and       pound eyes process more than 300 images
the wavelength range of the radiation per-    per second (to the human eye, however,
ceivable by the human eye is called visible   movies with 25 images per second appear
range or simply light. The different wave-    as a fluent motion).
lengths of this electromagnetic radiation
are perceived by the human eye as differ-
ent colors. The visible range of the elec-    Pinhole eyes are, for example, found in
tromagnetic radiation is different for each   octopus species and work – as you can
organism. Some organisms cannot see the       guess – similar to a pinhole camera. A
                                                                                             pinhole
colors (=wavelength ranges) we can see,       pinhole eye has a very small opening for       camera:
others can even perceive additional wave-     light entry, which projects a sharp image      high spat.,

                                              onto the sensory cells behind. Thus, the
                                                                                             low
length ranges (e.g. in the UV range). Be-                                                    temporal
fore we begin with the human being – in       spatial resolution is much higher than in      resolution
order to get a broader knowledge of the       the compound eye. But due to the very
sense of sight– we briefly want to look at    small opening for light entry the resulting
two organs of sight which, from an evolu-     image is less bright.
tionary point of view, exist much longer
than the human.



26                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                                                                2.3 Receptor cells


                                                           2.3.3.3 The retina does not only
                                                                   receive information but is also
                                                                   responsible for information
                                                                   processing

                                                           The light signals falling on the eye are
                                                           received by the retina and directly pre-
                                                           processed by several layers of information-
                                                           processing cells. We want to briefly dis-
                                                           cuss the different steps of this informa-
                                                           tion processing and in doing so, we follow
                                                           the way of the information carried by the
                Figure 2.5: Compound eye of a robber fly   light:
                                                            Photoreceptors receive the light signal
                                                               und cause action potentials (there
                                                               are different receptors for different
              2.3.3.2 Single lens eyes combine the             color components and light intensi-
                        advantages of the other two            ties). These receptors are the real
                        eye types, but they are more           light-receiving part of the retina and
                        complex                                they are sensitive to such an extent
                                                               that only one single photon falling
                                                               on the retina can cause an action po-
              The light sensing organ common in verte-         tential. Then several photoreceptors
              brates is the single lense eye. The result-      transmit their signals to one single
              ing image is a sharp, high-resolution image
              of the environment at high or variable light bipolar cell. This means that here the in-
              intensity. On the other hand it is more          formation has already been summa-
              complex. Similar to the pinhole eye the          rized. Finally, the now transformed
              light enters through an opening (pupil)          light signal travels from several bipo-
              and is projected onto a layer of sensory         lar cells 2 into
              cells in the eye. (retina). But in contrast ganglion cells. Various bipolar cells can
     Single
 lense eye:   to the pinhole eye, the size of the pupil can    transmit their information to one gan-
high temp.    be adapted to the lighting conditions (by        glion cell. The higher the number
              means of the iris muscle, which expands
  and spat.
 resolution                                                    of photoreceptors that affect the gan-
              or contracts the pupil). These differences       glion cell, the larger the field of per-
              in pupil dilation require to actively focus      ception, the receptive field, which
              the image. Therefore, the single lens eye        covers the ganglions – and the less
              contains an additional adjustable lens.
                                                           2 There are different kinds of bipolar cells, as well,
                                                             but to discuss all of them would go too far.




              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                27
Chapter 2 Biological neural networks                                        dkriesel.com


     sharp is the image in the area of this 2.4 The amount of neurons in
     ganglion cell. So the information is          living organisms at
     already reduced directly in the retina
     and the overall image is, for exam-           different stages of
     ple, blurred in the peripheral field          development
     of vision. So far, we have learned
     about the information processing in An overview of different organisms and
     the retina only as a top-down struc- their neural capacity (in large part from
     ture. Now we want to take a look at [RD05]):
     the
                                             302 neurons are required by the nervous
horizontal and amacrine cells. These             system of a nematode worm, which
     cells are not connected from the            serves as a popular model organism
     front backwards but laterally. They         in biology. Nematodes live in the soil
     allow the light signals to influence        and feed on bacteria.
     themselves laterally directly during
     the information processing in the 104 neurons make an ant (To simplify
     retina – a much more powerful               matters we neglect the fact that some
     method of information processing            ant species also can have more or less
     than compressing and blurring.              efficient nervous systems). Due to the
     When the horizontal cells are excited       use of different attractants and odors,
     by a photoreceptor, they are able to        ants are able to engage in complex
     excite other nearby photoreceptors          social behavior and form huge states
     and at the same time inhibit more           with millions of individuals. If you re-
     distant bipolar cells and receptors.        gard such an ant state as an individ-
     This ensures the clear perception of        ual, it has a cognitive capacity similar
     outlines and bright points. Amacrine        to a chimpanzee or even a human.
     cells can further intensify certain
                                             With 105 neurons the nervous system of
     stimuli by distributing information
                                                 a fly can be constructed. A fly can
     from bipolar cells to several ganglion
                                                 evade an object in real-time in three-
     cells or by inhibiting ganglions.
                                                 dimensional space, it can land upon
These first steps of transmitting visual in-     the ceiling upside down, has a consid-
formation to the brain show that informa-        erable sensory system because of com-
tion is processed from the first moment the      pound eyes, vibrissae, nerves at the
information is received and, on the other        end of its legs and much more. Thus,
hand, is processed in parallel within mil-       a fly has considerable differential and
lions of information-processing cells. The       integral calculus in high dimensions
system’s power and resistance to errors          implemented "in hardware". We all
is based upon this massive division of           know that a fly is not easy to catch.
work.                                            Of course, the bodily functions are



28                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                              2.4 The amount of neurons in living organisms


    also controlled by neurons, but these 1.6 · 108 neurons are required by the
    should be ignored here.                    brain of a dog, companion of man for
                                               ages. Now take a look at another pop-
With 0.8 · 106 neurons we have enough
                                               ular companion of man:
    cerebral matter to create a honeybee.
    Honeybees build colonies and have 3 · 108 neurons can be found in a cat,
    amazing capabilities in the field of       which is about twice as much as in
    aerial reconnaissance and navigation.      a dog. We know that cats are very
4 · 106 neurons result in a mouse, and            elegant, patient carnivores that can
     here the world of vertebrates already        show a variety of behaviors. By the
     begins.                                      way, an octopus can be positioned
                                                  within the same magnitude. Only
1.5 · 107 neurons are sufficient for a rat,       very few people know that, for exam-
     an animal which is denounced as be-          ple, in labyrinth orientation the octo-
     ing extremely intelligent and are of-        pus is vastly superior to the rat.
     ten used to participate in a variety
     of intelligence tests representative for For 6 · 109 neurons you already get a
     the animal world. Rats have an ex-             chimpanzee, one of the animals being
     traordinary sense of smell and orien-         very similar to the human.
     tation, and they also show social be-
     havior. The brain of a frog can be 1011 neurons make a human. Usually,
     positioned within the same dimension.          the human has considerable cognitive
     The frog has a complex build with              capabilities, is able to speak, to ab-
     many functions, it can swim and has            stract, to remember and to use tools
     evolved complex behavior. A frog               as well as the knowledge of other hu-
     can continuously target the said fly           mans to develop advanced technolo-
     by means of his eyes while jumping             gies and manifold social structures.
     in three-dimensional space and and
     catch it with its tongue with consid- With 2 · 10 neurons there are nervous
                                                            11

     erable probability.                            systems having more neurons than
                                                    the human nervous system. Here we
5 · 107 neurons make a bat. The bat can             should mention elephants and certain
     navigate in total darkness through a          whale species.
     room, exact up to several centime-
     ters, by only using their sense of hear- Our state-of-the-art computers are not
     ing. It uses acoustic signals to localize able to keep up with the aforementioned
     self-camouflaging insects (e.g. some processing power of a fly. Recent research
     moths have a certain wing structure results suggest that the processes in ner-
     that reflects less sound waves and the vous systems might be vastly more pow-
     echo will be small) and also eats its erful than people thought until not long
     prey while flying.                        ago: Michaeva et al. describe a separate,



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        29
Chapter 2 Biological neural networks                                      dkriesel.com


synapse-integrated information way of in-       therefore it is a vector. In nature a
formation processing [MBW+ 10]. Poster-         neuron receives pulses of 103 to 104
ity will show if they are right.                other neurons on average.

                                             Scalar output: The output of a neuron is
                                                 a scalar, which means that the neu-
2.5 Transition to technical                      ron only consists of one component.
       neurons: neural networks                  Several scalar outputs in turn form
       are a caricature of biology               the vectorial input of another neuron.
                                                 This particularly means that some-
                                                 where in the neuron the various input
How do we change from biological neural          components have to be summarized in
networks to the technical ones? Through          such a way that only one component
radical simplification. I want to briefly        remains.
summarize the conclusions relevant for the
technical part:                              Synapses change input: In technical neu-
                                                 ral networks the inputs are prepro-
We have learned that the biological neu-
                                                 cessed, too. They are multiplied by
rons are linked to each other in a weighted
                                                 a number (the weight) – they are
way and when stimulated they electrically
                                                 weighted. The set of such weights rep-
transmit their signal via the axon. From
                                                 resents the information storage of a
the axon they are not directly transferred
                                                 neural network – in both biological
to the succeeding neurons, but they first
                                                 original and technical adaptation.
have to cross the synaptic cleft where the
signal is changed again by variable chem- Accumulating the inputs: In biology, the
ical processes. In the receiving neuron          inputs are summarized to a pulse ac-
the various inputs that have been post-          cording to the chemical change, i.e.,
processed in the synaptic cleft are summa-       they are accumulated – on the techni-
rized or accumulated to one single pulse.        cal side this is often realized by the
Depending on how the neuron is stimu-            weighted sum, which we will get to
lated by the cumulated input, the neuron         know later on. This means that after
itself emits a pulse or not – thus, the out-     accumulation we continue with only
put is non-linear and not proportional to        one value, a scalar, instead of a vec-
the cumulated input. Our brief summary           tor.
corresponds exactly with the few elements
of biological neural networks we want to Non-linear characteristic: The input of
take over into the technical approxima-          our technical neurons is also not pro-
tion:                                            portional to the output.

Vectorial input: The input of technical Adjustable weights: The weights weight-
    neurons consists of many components,    ing the inputs are variable, similar to



30                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                             2.5 Technical neurons as caricature of biology


    the chemical processes at the synap-     bits of information. Naïvely calculated:
    tic cleft. This adds a great dynamic     How much storage capacity does the brain
    to the network because a large part of   have? Note: The information which neu-
    the "knowledge" of a neural network is   ron is connected to which other neuron is
    saved in the weights and in the form     also important.
    and power of the chemical processes
    in a synaptic cleft.

So our current, only casually formulated
and very simple neuron model receives a
vectorial input
                   ~x,
with components xi . These are multiplied
by the appropriate weights wi and accumu-
lated:          X
                    wi x i .
                     i

The aforementioned term is called
weighted sum.      Then the nonlinear
mapping f defines the scalar output y:
                                 !
               y=f
                         X
                             wi x i .
                         i


After this transition we now want to spec-
ify more precisely our neuron model and
add some odds and ends. Afterwards we
will take a look at how the weights can be
adjusted.


Exercises

Exercise 4. It is estimated that a hu-
man brain consists of approx. 1011 nerve
cells, each of which has about 103 to 104
synapses. For this exercise we assume 103
synapses per neuron. Let us further as-
sume that a single synapse could save 4



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                     31
              Chapter 3
              Components of artificial neural networks
                                 Formal definitions and colloquial explanations of the components that realize
                                 the technical adaptations of biological neural networks. Initial descriptions of
                                                     how to combine these components into a neural network.




              This chapter contains the formal defini-         certain point in time, the notation will be,
              tions for most of the neural network com-        for example, netj (t − 1) or oi (t).
              ponents used later in the text. After this
              chapter you will be able to read the indi-
              vidual chapters of this work without hav-
              ing to know the preceding ones (although         From a biological point of view this is, of
              this would be useful).                           course, not very plausible (in the human
                                                               brain a neuron does not wait for another
                                                               one), but it significantly simplifies the im-
              3.1 The concept of time in                       plementation.
                  neural networks

              In some definitions of this text we use the
              term time or the number of cycles of the
              neural network, respectively. Time is di-
                                                               3.2 Components of neural
              vided into discrete time steps:                      networks
   discrete
time steps
              Definition 3.1 (The concept of time).
              The current time (present time) is referred
              to as (t), the next time step as (t + 1),        A technical neural network consists of sim-
   (t)I       the preceding one as (t − 1). All other          ple processing units, the neurons, and
              time steps are referred to analogously. If in    directed, weighted connections between
              the following chapters several mathemati-        those neurons. Here, the strength of a
              cal variables (e.g. netj or oi ) refer to a      connection (or the connecting weight) be-



                                                                                                              33
             Chapter 3 Components of artificial neural networks (fundamental)                              dkriesel.com


             tween two neurons i and j is referred to as ber of the matrix indicating where the con-
             wi,j 1 .                                     nection begins, and the column number of
                                                          the matrix indicating, which neuron is the
             Definition 3.2 (Neural network). A
                                                          target. Indeed, in this case the numeric
             neural network is a sorted triple
                                                          0 marks a non-existing connection. This
             (N, V, w) with two sets N , V and a func-
                                                          matrix representation is also called Hin-
             tion w, where N is the set of neurons and
                                                          ton diagram 2 .
             V a set {(i, j)|i, j ∈ N} whose elements are
             called connections between neuron i and The neurons and connections comprise the
             neuron j. The function w : V → R defines following components and variables (I’m
n. network
 = neurons   the weights, where w((i, j)), the weight of following the path of the data within a
+ weighted   the connection between neuron i and neu- neuron, which is according to fig. 3.1 on
             ron j, is shortened to wi,j . Depending on the facing page in top-down direction):
connection
  wi,j I
             the point of view it is either undefined or
             0 for connections that do not exist in the
             network.                                     3.2.1 Connections carry information
                                                                            that is processed by neurons
                  SNIPE: In Snipe, an instance of the class
                  NeuralNetworkDescriptor is created in
                  the first place.     The descriptor object        Data are transferred between neurons via
                  roughly outlines a class of neural networks,      connections with the connecting weight be-
                  e.g. it defines the number of neuron lay-
                  ers in a neural network. In a second step,
                                                                    ing either excitatory or inhibitory. The
                  the descriptor object is used to instantiate      definition of connections has already been
                  an arbitrary number of NeuralNetwork ob-          included in the definition of the neural net-
                  jects. To get started with Snipe program-         work.
                  ming, the documentations of exactly these
                  two classes are – in that order – the right
                                                                       SNIPE:        Connection               weights
                  thing to read. The presented layout involv-
                                                                       can   be   set   using   the           method
                  ing descriptor and dependent neural net-
                                                                       NeuralNetwork.setSynapse.
                  works is very reasonable from the imple-
                  mentation point of view, because it is en-
                  ables to create and maintain general param-
                                                                    3.2.2 The propagation function
                  eters of even very large sets of similar (but
                  not neccessarily equal) networks.
                                                                          converts vector inputs to
                                                                          scalar network inputs
             So the weights can be implemented in a
             square weight matrix W or, optionally,
                                                                    Looking at a neuron j, we will usually find
             in a weight vector W with the row num-
    WI                                                              a lot of neurons with a connection to j, i.e.
              1 Note: In some of the cited literature i and j could which transfer their output to j.
               be interchanged in wi,j . Here, a consistent stan-
               dard does not exist. But in this text I try to use   2 Note that, here again, in some of the cited liter-
               the notation I found more frequently and in the        ature axes and rows could be interchanged. The
               more significant citations.                            published literature is not consistent here, as well.




             34                          D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                              Propagierungsfunktion
                           (oft gewichtete Summe, verarbeitet
                                Eingaben zur Netzeingabe)
Eingaben anderer
    Neuronen               Netzeingabe

                               Aktivierungsfunktion
               dkriesel.com
                         (Erzeugt aus Netzeingabe und alter                     3.2 Components of neural networks
                            Aktivierung die neue Aktivierung)

                           Aktivierung
                                                                       Ausgabe zu
                                  Ausgabefunktion
                                                                            neuron j the propagation func-
                                                                      For aNeuronen
                                                                    anderen
                          (Erzeugt aus Aktivierung die Ausgabe,       tion receives the outputs oi1 , . . . , oin of
                                                                      other neurons i1 , i2 , . . . , in (which are con-
                                     ist oft Identität)

                                                                      nected to j), and transforms them in con-                      manages
                                                                      sideration of the connecting weights wi,j                      inputs
                                                                      into the network input netj that can be fur-
                                                                      ther processed by the activation function.
                                                                      Thus, the network input is the result of
                                                                      the propagation function.
                         Data Input of
                        other Neurons
                                                                      Definition 3.3 (Propagation func-
                                                                      tion and network input).                      Let
                                                                      I = {i1 , i2 , . . . , in } be the set of neurons,
                                                                      such that ∀z ∈ {1, . . . , n} : ∃wiz ,j . Then
                               Propagation function
                            (often weighted sum, transforms
                                                                      the network input of j, called netj , is
                          outputs of other neurons to net input)      calculated by the propagation function
                           Network Input
                                                                      fprop as follows:

                                                                       netj = fprop (oi1 , . . . , oin , wi1 ,j , . . . , win ,j )
                                Activation function
                          (Transforms net input and sometimes
                             old activation to new activation)                                                               (3.1)
                           Activation

                                  Output function                     Here the weighted sum is very popular:
                                                                      The multiplication of the output of each
                           (often identity function, transforms
                          activation to output for other neurons)
                                                                      neuron i by wi,j , and the summation of
                                                                      the results:
                                             Data Output to
                                                                                      netj =           (oi · wi,j )          (3.2)
                                                                                                X
                                             other Neurons
                                                                                                 i∈I

               Figure 3.1: Data processing of a neuron. The
                                                                         SNIPE: The propagation function in
               activation function of a neuron implies the               Snipe was implemented using the weighted
               threshold value.                                          sum.



                                                                      3.2.3 The activation is the
                                                                            "switching status" of a
                                                                            neuron

                                                                      Based on the model of nature every neuron
                                                                      is, to a certain extent, at all times active,
                                                                      excited or whatever you will call it. The



               D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                  35
             Chapter 3 Components of artificial neural networks (fundamental)                    dkriesel.com


             reactions of the neurons to the input val-          3.2.5 The activation function
How active
             ues depend on this activation state. The                  determines the activation of a
      is a   activation state indicates the extent of a                neuron dependent on network
  neuron?    neuron’s activation and is often shortly re-              input and treshold value
             ferred to as activation. Its formal defini-
             tion is included in the following definition        At a certain time – as we have already
             of the activation function. But generally,          learned – the activation aj of a neuron j
             it can be defined as follows:                       depends on the previous 3 activation state
                                                                 of the neuron and the external input.
             Definition 3.4 (Activation state / activa-
             tion in general). Let j be a neuron. The Definition 3.6 (Activation function and
             activation state aj , in short activation, is Activation). Let j be a neuron. The ac-
             explicitly assigned to j, indicates the ex- tivation function is defined as
                                                                                                                  calculates
                                                                                                                  activation
             tent of the neuron’s activity and results
             from the activation function.                  aj (t) = fact (netj (t), aj (t − 1), Θj ). (3.3)

                  SNIPE: It is possible to get and set activa-    It transforms the network input netj ,
                  tion states of neurons by using the meth-      as well as the previous activation state
                                                                                                                  Jfact
                  ods getActivation or setActivation in
                  the class NeuralNetwork.                       aj (t − 1) into a new activation state aj (t),
                                                                 with the threshold value Θ playing an im-
                                                                 portant role, as already mentioned.
             3.2.4 Neurons get activated if the
                   network input exceeds their                   Unlike the other variables within the neu-
                   treshold value                                ral network (particularly unlike the ones
                                                                 defined so far) the activation function is
             Near the threshold value, the activation            often defined globally for all neurons or
             function of a neuron reacts particularly            at least for a set of neurons and only the
             sensitive. From the biological point of             threshold values are different for each neu-
             view the threshold value represents the             ron. We should also keep in mind that
             threshold at which a neuron starts fir-             the threshold values can be changed, for
             ing. The threshold value is also mostly             example by a learning procedure. So it
   highest
             included in the definition of the activation        can in particular become necessary to re-
                                                                 late the threshold value to the time and to
  point of
 sensation   function, but generally the definition is the
             following:                                          write, for instance Θj as Θj (t) (but for rea-
                                                                 sons of clarity, I omitted this here). The
             Definition 3.5 (Threshold value in gen-             activation function is also called transfer
             eral). Let j be a neuron. The threshold             function.
             value Θj is uniquely assigned to j and 3 The previous activation is not always relevant for
    ΘI
             marks the position of the maximum gradi- the current – we will see examples for both vari-
             ent value of the activation function.    ants.




             36                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
     dkriesel.com                                                 3.2 Components of neural networks

                                                                               1
        SNIPE: In Snipe, activation functions are                                       .          (3.5)
                                                                            1+e
                                                                                   −x
        generalized to neuron behaviors. Such                                      T

        behaviors can represent just normal acti-
        vation functions, or even incorporate in-         The smaller this parameter, the more does
        ternal states and dynamics. Correspond-           it compress the function on the x axis.
        ing parts of Snipe can be found in the            Thus, one can arbitrarily approximate the
        package neuronbehavior, which also con-
        tains some of the activation functions in-        Heaviside function. Incidentally, there ex-
        troduced in the next section. The inter-          ist activation functions which are not ex-
        face NeuronBehavior allows for implemen-          plicitly defined but depend on the input ac-
        tation of custom behaviors. Objects that          cording to a random distribution (stochas-
        inherit from this interface can be passed to
        a NeuralNetworkDescriptor instance. It
                                                          tic activation function).
        is possible to define individual behaviors
        per neuron layer.                                 A alternative to the hypberbolic tangent
                                                          that is really worth mentioning was sug-
                                                          gested by Anguita et al. [APZ93], who
     3.2.6 Common activation functions                    have been tired of the slowness of the work-
                                                          stations back in 1993. Thinking about
                                                          how to make neural network propagations
     The simplest activation function is the bi-
                                                          faster, they quickly identified the approx-
     nary threshold function (fig. 3.2 on the
                                                          imation of the e-function used in the hy-
     next page), which can only take on two val-
                                                          perbolic tangent as one of the causes of
     ues (also referred to as Heaviside func-
                                                          slowness. Consequently, they "engineered"
     tion). If the input is above a certain
                                                          an approximation to the hyperbolic tan-
     threshold, the function changes from one
                                                          gent, just using two parabola pieces and
     value to another, but otherwise remains
                                                          two half-lines. At the price of delivering
     constant. This implies that the function
                                                          a slightly smaller range of values than the
     is not differentiable at the threshold and
                                                          hyperbolic tangent ([−0.96016; 0.96016] in-
     for the rest the derivative is 0. Due to
                                                          stead of [−1; 1]), dependent on what CPU
     this fact, backpropagation learning, for ex-
                                                          one uses, it can be calculated 200 times
     ample, is impossible (as we will see later).
                                                          faster because it just needs two multipli-
     Also very popular is the Fermi function
                                                          cations and one addition. What’s more,
     or logistic function (fig. 3.2)
                                                          it has some other advantages that will be
                           1                              mentioned later.
                                ,                 (3.4)
                        1 + e−x
                                                             SNIPE: The activation functions intro-
     which maps to the range of values of (0, 1)             duced here are implemented within the
     and the hyperbolic tangent (fig. 3.2)                   classes Fermi and TangensHyperbolicus,
                                                             both of which are located in the package
     which maps to (−1, 1). Both functions are               neuronbehavior. The fast hyperbolic tan-
     differentiable. The Fermi function can be               gent approximation is located within the
     expanded by a temperature parameter                     class TangensHyperbolicusAnguita.
     T into the form
TI


     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                    37
Chapter 3 Components of artificial neural networks (fundamental)                                     dkriesel.com


                             Heaviside Function               3.2.7 An output function may be
             1                                                      used to process the activation
                                                                    once again
            0.5


                                                              The output function of a neuron j cal-
 f(x)




             0
                                                              culates the values which are transferred to
           −0.5                                               the other neurons connected to j. More
                                                              formally:
            −1
                  −4       −2        0            2      4
                                                              Definition 3.7 (Output function). Let j
                                     x                                                                                  informs
                  Fermi Function with Temperature Parameter
                                                              be a neuron. The output function                          other
             1
                                                                                                                        neurons
                                                                                fout (aj ) = oj                (3.6)
            0.8

                                                              calculates the output value oj of the neu-
            0.6
                                                              ron j from its activation state aj .
                                                                                                                        Jfout
 f(x)




            0.4


            0.2                                               Generally, the output function is defined
             0
                                                              globally, too. Often this function is the
                  −4       −2         0           2      4    identity, i.e. the activation aj is directly
                                                              output4 :
                                     x

                             Hyperbolic Tangent
             1
            0.8                                                          fout (aj ) = aj , so oj = aj          (3.7)
            0.6
            0.4
                                                              Unless explicitly specified differently, we
            0.2
                                                              will use the identity as output function
 tanh(x)




             0
           −0.2                                               within this text.
           −0.4
           −0.6
           −0.8
            −1                                                3.2.8 Learning strategies adjust a
                                                                    network to fit our needs
                  −4       −2         0           2      4
                                     x



                                                              Since we will address this subject later in
Figure 3.2: Various popular activation func-                  detail and at first want to get to know the
tions, from top to bottom: Heaviside or binary
                                                              principles of neural network structures, I
threshold function, Fermi function, hyperbolic
tangent. The Fermi function was expanded by                   will only provide a brief and general defi-
a temperature parameter. The original Fermi                   nition here:
function is represented by dark colors, the tem-              4 Other definitions of output functions may be use-
perature parameters of the modified Fermi func-                 ful if the range of values of the activation function
tions are, ordered ascending by steepness, 12 , 51 ,            is not sufficient.
 1       1
10 und 25 .



38                                  D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                      3.3 Network topologies


Definition 3.8 (General learning rule). 3.3.1 Feedforward networks consist
The learning strategy is an algorithm                  of layers and connections
that can be used to change and thereby                 towards each following layer
train the neural network, so that the net-
work produces a desired output for a given
input.                                         Feedforward In this text feedforward net-
                                               works (fig. 3.3 on the following page) are
                                               the networks we will first explore (even if
                                               we will use different topologies later). The
3.3 Network topologies                         neurons are grouped in the following lay-
                                               ers: One input layer, n hidden pro-
                                                                                              network of
After we have become acquainted with the cessing layers (invisible from the out-              layers
composition of the elements of a neural side, that’s why the neurons are also re-
network, I want to give an overview of ferred to as hidden neurons) and one out-
the usual topologies (= designs) of neural put layer. In a feedforward network each
networks, i.e. to construct networks con- neuron in one layer has only directed con-
sisting of these elements. Every topology nections to the neurons of the next layer
described in this text is illustrated by a (towards the output layer). In fig. 3.3 on
map and its Hinton diagram so that the the next page the connections permitted
reader can immediately see the character- for a feedforward network are represented
istics and apply them to other networks.       by solid lines. We will often be confronted
                                               with feedforward networks in which every
In the Hinton diagram the dotted weights neuron i is connected to all neurons of the
are represented by light grey fields, the next layer (these layers are called com-
solid ones by dark grey fields. The input pletely linked). To prevent naming con-
and output arrows, which were added for flicts the output neurons are often referred
reasons of clarity, cannot be found in the to as Ω.
Hinton diagram. In order to clarify that
the connections are between the line neu- Definition 3.9 (Feedforward network).
rons and the column neurons, I have in- The neuron layers of a feedforward net-
serted the small arrow  in the upper-left work (fig. 3.3 on the following page) are
cell.                                          clearly separated: One input layer, one
                                               output layer and one or more processing
    SNIPE: Snipe is designed for realization   layers which are invisible from the outside
    of arbitrary network topologies. In this   (also called hidden layers). Connections
    respect, Snipe defines different kinds of
    synapses depending on their source and     are only permitted to neurons of the fol-
    their target. Any kind of synapse can sep- lowing layer.
   arately be allowed or forbidden for a set of
   networks using the setAllowed methods in
   a NeuralNetworkDescriptor instance.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         39
Chapter 3 Components of artificial neural networks (fundamental)                              dkriesel.com


                                                       3.3.1.1 Shortcut connections skip layers
                                                                                                                 Shortcuts
                                                       Some feedforward networks permit the so-
                                                                                                                 skip
                                                                                                                 layers
                                                       called shortcut connections (fig. 3.4 on
                                                       the next page): connections that skip one
                                                       or more levels. These connections may
                                                       only be directed towards the output layer,
                                                       too.

               @ABC
               GFED                              @ABC
                                                 GFED
                                                  
                 i1 UU                          i i2                Definition 3.10 (Feedforward network
              }} AAAUUUUUUUiiiiiii}} AAA                            with shortcut connections). Similar to the
            }}              AA iiii UUUU }}               AA
           }                i i
                              A             U
                                            } U             A
                                          }} UUUUUUU AAA feedforward network, but the connections
                           i                   U
         }}      iiii AAA
GFED
@ABC                             @ABC
                                 GFED                           @ABC
                                                              i GFED
    ~}ti}iiiiii                        ~}}                 UUU*     may not only be directed towards the next
 h1 AUUUU                          h2 A                      i   h3
             U
        AA UUUU                  }       AA            ii }} layer but also towards any other subse-
                                                          ii
          AA        UUUU }}}               AA iiiiii         }
            AA              }U
                             } UUU U  i  iiiiAA           }}}       quent layer.
              AA                   iUiU        A
               GFED
               @ABC                              @ABC
                                                 GFED
                           }                             }
                     }~ it}iiiiii UUUUUUA*           }~ }
                 Ω1                               Ω2

                                                       3.3.2 Recurrent networks have
                                        
                                                             influence on themselves
        i1 i2 h1 h2 h3 Ω1 Ω2
       i1                                              Recurrence is defined as the process of a
       i2                                              neuron influencing itself by any means or
       h1                                              by any connection. Recurrent networks do
       h2                                              not always have explicitly defined input or
       h3                                              output neurons. Therefore in the figures
       Ω1                                              I omitted all markings that concern this
       Ω2                                              matter and only numbered the neurons.

Figure 3.3: A feedforward network with three
layers: two input neurons, three hidden neurons        3.3.2.1 Direct recurrences start and
and two output neurons. Characteristic for the                 end at the same neuron
Hinton diagram of completely linked feedforward
networks is the formation of blocks above the
diagonal.
                                                       Some networks allow for neurons to be
                                                       connected to themselves, which is called
                                                       direct recurrence (or sometimes self-
                                                       recurrence (fig. 3.5 on the facing page).
                                                       As a result, neurons inhibit and therefore
                                                       strengthen themselves in order to reach
                                                       their activation limits.



40                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                            3.3 Network topologies



                                                                         89:;
                                                                         ?>=<                        89:;
                                                                                                     ?>=<
                                                                                v                            v
                                                                           1                           2


                                                        89:;
                                                        ?>=<                            ?>=<
                                                                                        89:;                           89:;
                                                                                                                     ) ?>=<
                                                             u   v                          v                             v
                                                          3                                4                           5


                                                                         ?>=<
                                                                         89:;                          89:;
                                                                                                     ) ?>=<
                                                                           uv                             v
                                                                           6                           7

            @ABC
            GFED                   @ABC
                                   GFED
                                    
             i1                     i2                                  1          2   3    4   5         6     7
                                                                     1
                                                                     2
GFED
@ABC                    @ABC
                        GFED                     @ABC
                                               * GFED
    ~t                      ~                                        3
 h1                      h2                     h3
                                                                     4
                                                                     5
            @ABC
            GFED                       @ABC
                                   + * GFED
               ~
                  t
                                          ~                         6
             Ω1 s                       Ω  2                         7

                                                     Figure 3.5: A network similar to a feedforward
                                      
                                                     network with directly recurrent neurons. The di-
                                                     rect recurrences are represented by solid lines and
          i1 i2 h1 h2 h3 Ω1 Ω2                      exactly correspond to the diagonal in the Hinton
         i1                                          diagram matrix.
         i2
         h1
         h2
         h3
         Ω1
         Ω2

Figure 3.4: A feedforward network with short-        Definition 3.11 (Direct recurrence).
cut connections, which are represented by solid      Now we expand the feedforward network                                      neurons
lines. On the right side of the feedforward blocks   by connecting a neuron j to itself, with the                               influence
new connections have been added to the Hinton        weights of these connections being referred                                themselves
diagram.
                                                     to as wj,j . In other words: the diagonal
                                                     of the weight matrix W may be different
                                                     from 0.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                            41
Chapter 3 Components of artificial neural networks (fundamental)                                dkriesel.com


3.3.2.2 Indirect recurrences can
        influence their starting neuron
        only by making detours

If connections are allowed towards the in-
put layer, they will be called indirect re-
currences. Then a neuron j can use in-
direct forwards connections to influence it-
                                                                89:;
                                                              8 ?>=<                          89:;
                                                                                           82 ?>=<
self, for example, by influencing the neu-
rons of the next layer and the neurons of                         1 g                           2
                                                                   X                              X
this next layer influencing j (fig. 3.6).
                                                 ?>=<
                                                 89:;                         89:;
                                                                            8 ?>=<
                                                                                                             ) 89:;
                                                                                                          82 ?>=<
Definition 3.12 (Indirect recurrence).                u                                                        
                                                   3 g                           4                                5
Again our network is based on a feedfor-            X                              X
ward network, now with additional connec-
tions between neurons and their preceding                     89:;
                                                              ?>=<                           89:;
                                                                                           ) ?>=<
                                                                u                            
                                                                6                            7
layer being allowed. Therefore, below the
diagonal of W is different from 0.
                                                             1         2   3      4   5         6    7
                                                          1
3.3.2.3 Lateral recurrences connect                       2
        neurons within one layer                          3
                                                          4
                                                          5
Connections between neurons within one
                                                          6
layer are called lateral recurrences
                                                          7
(fig. 3.7 on the facing page). Here, each
neuron often inhibits the other neurons of
                                         Figure 3.6: A network similar to a feedforward
the layer and strengthens itself. As a re-
                                         network with indirectly recurrent neurons. The
sult only the strongest neuron becomes ac-
                                         indirect recurrences are represented by solid lines.
tive (winner-takes-all scheme).          As we can see, connections to the preceding lay-
                                         ers can exist here, too. The fields that are sym-
Definition 3.13 (Lateral recurrence). A metric to the feedforward blocks in the Hinton
laterally recurrent network permits con- diagram are now occupied.
nections within one layer.


3.3.3 Completely linked networks
      allow any possible connection

Completely linked networks permit connec-
tions between all neurons, except for direct



42                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                   3.4 The bias neuron


                                                                      recurrences. Furthermore, the connections
                                                                      must be symmetric (fig. 3.8 on the next
                                                                      page). A popular example are the self-
                                                                      organizing maps, which will be introduced
                                                                      in chapter 10.
                                                                      Definition 3.14 (Complete interconnec-
                                                                      tion). In this case, every neuron is always
               89:;
               ?>=<                          89:;
                                           + ?>=<                     allowed to be connected to every other neu-
                 1 k                         2                        ron – but as a result every neuron can
                                                                      become an input neuron. Therefore, di-
  89:;
  ?>=<                       89:; k
                           + ?>=<                              89:;
                                                          +* ) ?>=<
       u                                                            rect recurrences normally cannot be ap-
    3 jk                       4                             5        plied here and clearly defined layers do not
                                                                      longer exist. Thus, the matrix W may be

               89:;
               ?>=<                            89:;
                                           + ) ?>=<
                 u                                                  unequal to 0 everywhere, except along its
                 6 k                         7                        diagonal.

              1       2   3       4   5         6    7
           1
           2                                                          3.4 The bias neuron is a
           3                                                              technical trick to consider
           4                                                              threshold values as
           5
           6
                                                                          connection weights
           7
                                                                      By now we know that in many network
Figure 3.7: A network similar to a feedforward                        paradigms neurons have a threshold value
network with laterally recurrent neurons. The                         that indicates when a neuron becomes ac-
direct recurrences are represented by solid lines.
                                                                      tive. Thus, the threshold value is an
Here, recurrences only exist within the layer.
In the Hinton diagram, filled squares are con-
                                                                      activation function parameter of a neu-
centrated around the diagonal in the height of                        ron. From the biological point of view
the feedforward blocks, but the diagonal is left                      this sounds most plausible, but it is com-
uncovered.                                                            plicated to access the activation function
                                                                      at runtime in order to train the threshold
                                                                      value.
                                                                      But threshold values Θj1 , . . . , Θjn for neu-
                                                                      rons j1 , j2 , . . . , jn can also be realized as
                                                                      connecting weight of a continuously fir-
                                                                      ing neuron: For this purpose an addi-
                                                                      tional bias neuron whose output value



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                    43
Chapter 3 Components of artificial neural networks (fundamental)                                                        dkriesel.com


                                                                                  is always 1 is integrated in the network
                                                                                  and connected to the neurons j1 , j2 , . . . , jn .
                                                                                  These new connections get the weights
                                                                                  −Θj1 , . . . , −Θjn , i.e. they get the negative
                                                                                  threshold values.

                                                                                  Definition 3.15. A bias neuron is a
                                                                                  neuron whose output value is always 1 and
                                                                                  which is represented by

                                                                                                         @ABC
                                                                                                         GFED
                    89:;
                    ?>=<       o                             89:;
                                                          / ?>=<                                          BIAS .
                     @ 1O >^ Ti >TTTTT jjjjj5 @ 2O >^ >
                                    >> jTjTjTjT                    >>
                                      >>jj
                                      j               TTTT           >            It is used to represent neuron biases as con-
                        jj         jj   >                 T   TTTT >>>
 89:;
 ?>=<                                       89:;
                                          / ?>=<                           89:;
                                                                         / ?>=<
                    j  j                 >>
        ju jjj
                  j                                                TTTT>)       nection weights, which enables any weight-
   3 >^ Ti jo TTT                               4   o
                                               @ >^ >                jjj45 @ 5    training algorithm to train the biases at
             >> TTTT                                              j j
               >>         TTTT                        >>      jjj
                 >>                     TTTT jjjj>>j>jj                           the same time.

                    89:;
                    ?>=<                                     89:;
                                                           / ?>=<
                   >>                          jjTT     >
                       ju jjjjj TTTTT>)   
                       6         o                             7
                                                                                  Then the threshold value of the neurons
                                                                                  j1 , j2 , . . . , jn is set to 0. Now the thresh-
                 1       2       3       4       5        6       7
                                                                                  old values are implemented as connection
         1                                                                        weights (fig. 3.9 on page 46) and can di-
         2                                                                        rectly be trained together with the con-
         3                                                                        nection weights, which considerably facil-
         4                                                                        itates the learning process.
         5
         6                                      In other words: Instead of including the
         7                                      threshold value in the activation function,
                                                it is now included in the propagation func-
Figure 3.8: A completely linked network with tion. Or even shorter: The threshold value
symmetric connections and without direct recur-
                                                is subtracted from the network input, i.e.
rences. In the Hinton diagram only the diagonal
is left blank.
                                                it is part of the network input. More for-
                                                mally:
                                                                                                                                           bias neuron
                                                                                                                                           replaces
                                                                                  Let j1 , j2 , . . . , jn be neurons with thresh-         thresh. value
                                                                                  old values Θj1 , . . . , Θjn . By inserting a            with weights
                                                                                  bias neuron whose output value is always
                                                                                  1, generating connections between the said
                                                                                  bias neuron and the neurons j1 , j2 , . . . , jn
                                                                                  and      weighting           these  connections
                                                                                  wBIAS,j1 , . . . , wBIAS,jn with −Θj1 , . . . , −Θjn ,
                                                                                  we can set Θj1 = . . . = Θjn = 0 and



44                                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                           3.6 Orders of activation


receive an equivalent neural network
whose threshold values are realized by            PQRS
                                                  WVUT
                                                   ||c,x||      GFED
                                                                @ABC
                                                                            HIJK
                                                                             ONML
                                                                               Σ        WVUT
                                                                                        PQRS
                                                                                          Σ
connection weights.                                 Gauß                                L|H

                                            PQRS
neuron is the fact that it is much easier WVUT            PQRS
                                                          WVUT           HIJK
                                                                         ONML     @ABC
                                                                                  GFED
Undoubtedly, the advantage of the bias
                                              Σ              Σ             Σ       BIAS
                                             Tanh          Fermi          fact
to implement it in the network. One dis-
advantage is that the representation of the
network already becomes quite ugly with Figure 3.10: Different types of neurons that will
only a few neurons, let alone with a great appear in the following text.
number of them. By the way, a bias neu-
ron is often referred to as on neuron.

From now on, the bias neuron is omit-             3.6 Take care of the order in
ted for clarity in the following illustrations,
but we know that it exists and that the               which neuron activations
threshold values can simply be treated as             are calculated
weights because of it.
                                                  For a neural network it is very important
   SNIPE: In Snipe, a bias neuron was imple-
   mented instead of neuron-individual biases.
                                                  in which order the individual neurons re-
   The neuron index of the bias neuron is 0.      ceive and process the input and output the
                                                  results. Here, we distinguish two model
                                                  classes:


3.5 Representing neurons                          3.6.1 Synchronous activation

We have already seen that we can either           All neurons change their values syn-
write its name or its threshold value into        chronously, i.e. they simultaneously cal-
a neuron. Another useful representation,          culate network inputs, activation and out-
which we will use several times in the            put, and pass them on. Synchronous ac-
following, is to illustrate neurons accord-       tivation corresponds closest to its biolog-
ing to their type of data processing. See         ical counterpart, but it is – if to be im-
fig. 3.10 for some examples without fur-          plemented in hardware – only useful on
ther explanation – the different types of         certain parallel computers and especially
neurons are explained as soon as we need          not for feedforward networks. This order
them.                                             of activation is the most generic and can
                                                  be used with networks of arbitrary topol-
                                                  ogy.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                             45
               Chapter 3 Components of artificial neural networks (fundamental)                          dkriesel.com




                                              @ABC
                                              GFED                     GFED
                                                                       @ABC                     89:;
                                                                                              / ?>=<
                                                                                                 
                                                Θ1 B                    BIAS T
                                                                            AA TTTTT−Θ 1          0
                                             ||     BB                        AA    TTTT
                                           ||         BB                      −ΘA2A      −Θ3 TTT
                                          |             BB
                                                                                    ?>=<
                                                                                    89:;                    ?>=<
                                                                                                            89:;
                                         |                                         AA               TTTT
                                @ABC
                                GFED                       GFED
                                                           @ABC
                                        |                 BB
                                    |~ |                                                                TTT* 
                                 Θ2                          Θ3                      0                      0


                                                                                                          



               Figure 3.9: Two equivalent neural networks, one without bias neuron on the left, one with bias
               neuron on the right. The neuron threshold values can be found in the neurons, the connecting
               weights at the connections. Furthermore, I omitted the weights of the already existing connections
               (represented by dotted lines on the right side).




               Definition 3.16 (Synchronous activa-                of time. For this, there exist different or-
               tion). All neurons of a network calculate           ders, some of which I want to introduce in
biologically
   plausible   network inputs at the same time by means            the following:                                       easier to
               of the propagation function, activation by                                                               implement
               means of the activation function and out-
               put by means of the output function. Af-
               ter that the activation cycle is complete.          3.6.2.1 Random order

                    SNIPE: When implementing in software,
                    one could model this very general activa-      Definition 3.17 (Random order of acti-
                    tion order by every time step calculating      vation). With random order of acti-
                    and caching every single network input,
                    and after that calculating all activations.    vation a neuron i is randomly chosen and
                    This is exactly how it is done in Snipe, be-   its neti , ai and oi are updated. For n neu-
                    cause Snipe has to be able to realize arbi-    rons a cycle is the n-fold execution of this
                    trary network topologies.                      step. Obviously, some neurons are repeat-
                                                                   edly updated during one cycle, and others,
                                                                   however, not at all.
               3.6.2 Asynchronous activation

               Here, the neurons do not change their val- Apparently, this order of activation is not
               ues simultaneously but at different points always useful.



               46                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                                           3.6 Orders of activation


             3.6.2.2 Random permutation                       since otherwise there is no order of activa-
                                                              tion. Thus, in feedforward networks (for
             With random permutation each neuron              which the procedure is very reasonable)
             is chosen exactly once, but in random or-        the input neurons would be updated first,
             der, during one cycle.                           then the inner neurons and finally the out-
                                                              put neurons. This may save us a lot of
             Definition 3.18 (Random permutation).            time: Given a synchronous activation or-
              Initially, a permutation of the neurons is      der, a feedforward network with n layers
             calculated randomly and therefore defines        of neurons would need n full propagation
             the order of activation. Then the neurons        cycles in order to enable input data to
             are successively processed in this order.        have influence on the output of the net-
                                                              work. Given the topological activation or-
             This order of activation is as well used         der, we just need one single propagation.
             rarely because firstly, the order is gener-      However, not every network topology al-
             ally useless and, secondly, it is very time-     lows for finding a special activation order
             consuming to compute a new permutation           that enables saving time.
             for every cycle. A Hopfield network (chap-
                                                                 SNIPE: Those who want to use Snipe
             ter 8) is a topology nominally having a
                                                                 for implementing feedforward networks
             random or a randomly permuted order of              may save some calculation time by us-
             activation. But note that in practice, for          ing the feature fastprop (mentioned
             the previously mentioned reasons, a fixed           within the documentation of the class
             order of activation is preferred.                   NeuralNetworkDescriptor. Once fastprop
                                                                 is enabled, it will cause the data propaga-
             For all orders either the previous neuron           tion to be carried out in a slightly different
                                                                 way. In the standard mode, all net inputs
             activations at time t or, if already existing,      are calculated first, followed by all activa-
             the neuron activations at time t + 1, for           tions. In the fastprop mode, for every neu-
             which we are calculating the activations,           ron, the activation is calculated right after
             can be taken as a starting point.                   the net input. The neuron values are calcu-
                                                                 lated in ascending neuron index order. The
                                                                 neuron numbers are ascending from input
                                                                 to output layer, which provides us with the
             3.6.2.3 Topological order                           perfect topological activation order for feed-
                                                                 forward networks.
             Definition 3.19 (Topological activation).
              With topological order of activation
often very
    useful   the neurons are updated during one cycle 3.6.2.4 Fixed orders of activation
             and according to a fixed order. The order        during implementation
             is defined by the network topology.
                                                       Obviously, fixed orders of activation
             This procedure can only be considered for can be defined as well. Therefore, when
             non-cyclic, i.e. non-recurrent, networks, implementing, for instance, feedforward



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                      47
     Chapter 3 Components of artificial neural networks (fundamental)                               dkriesel.com


     networks it is very popular to determine                   outputs y1 , y2 , . . . , ym . They are regarded
     the order of activation once according to                  as output vector y = (y1 , y2 , . . . , ym ).
     the topology and to use this order without                 Thus, the output dimension is referred
     further verification at runtime. But this is               to as m. Data is output by a neural net-
     not necessarily useful for networks that are               work by the output neurons adopting the
                                                                                                                   Jm
     capable to change their topology.                          components of the output vector in their
                                                                output values.

                                                                   SNIPE: In order to propagate data through
     3.7 Communication with the                                    a NeuralNetwork-instance, the propagate
         outside world: input and                                  method is used. It receives the input vector

         output of data in and
                                                                   as array of doubles, and returns the output
                                                                   vector in the same way.
         from neural networks
                                                                Now we have defined and closely examined
     Finally, let us take a look at the fact that,              the basic components of neural networks –
     of course, many types of neural networks                   without having seen a network in action.
     permit the input of data. Then these data                  But first we will continue with theoretical
     are processed and can produce output.                      explanations and generally describe how a
     Let us, for example, regard the feedfor-                   neural network could learn.
     ward network shown in fig. 3.3 on page 40:
     It has two input neurons and two output
     neurons, which means that it also has two                  Exercises
     numerical inputs x1 , x2 and outputs y1 , y2 .
     As a simplification we summarize the in-
                                                       Exercise 5. Would it be useful (from
     put and output components for n input
                                                       your point of view) to insert one bias neu-
     or output neurons within the vectors x =
                                                       ron in each layer of a layer-based network,
     (x1 , x2 , . . . , xn ) and y = (y1 , y2 , . . . , yn ).
                                                       such as a feedforward network? Discuss
     Definition 3.20 (Input vector). A net- this in relation to the representation and
xI
     work with n input neurons needs n inputs implementation of the network. Will the
     x1 , x2 , . . . , xn . They are considered as in- result of the network change?
     put vector x = (x1 , x2 , . . . , xn ). As a
                                                       Exercise 6. Show for the Fermi function
     consequence, the input dimension is re-
                                                       f (x) as well as for the hyperbolic tangent
     ferred to as n. Data is put into a neural
nI                                                     tanh(x), that their derivatives can be ex-
     network by using the components of the in-
                                                       pressed by the respective functions them-
     put vector as network inputs of the input
                                                       selves so that the two statements
     neurons.
                                                         1. f 0 (x) = f (x) · (1 − f (x)) and
     Definition 3.21 (Output vector). A net-
yI
     work with m output neurons provides m               2. tanh0 (x) = 1 − tanh2 (x)



     48                          D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                            3.7 Input and output of data


are true.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                  49
               Chapter 4
               Fundamentals on learning and training
               samples
                                  Approaches and thoughts of how to teach machines. Should neural networks
                                    be corrected? Should they only be encouraged? Or should they even learn
                                       without any help? Thoughts about what we want to change during the
                                     learning procedure and how we will change it, about the measurement of
                                                                    errors and when we have learned enough.




               As written above, the most interesting         the question of how to implement it. In
               characteristic of neural networks is their     principle, a neural network changes when
               capability to familiarize with problems        its components are changing, as we have
               by means of training and, after sufficient     learned above. Theoretically, a neural net-
               training, to be able to solve unknown prob-    work could learn by
               lems of the same class. This approach is re-
                                                               1. developing new connections,
               ferred to as generalization. Before intro-
               ducing specific learning procedures, I want     2. deleting existing connections,
               to propose some basic principles about the
               learning procedure in this chapter.             3. changing connecting weights,

                                                               4. changing the threshold values of neu-
                                                                  rons,
               4.1 There are different
                                                               5. varying one or more of the three neu-
                   paradigms of learning                          ron functions (remember: activation
                                                                  function, propagation function and
               Learning is a comprehensive term. A                output function),
               learning system changes itself in order to      6. developing new neurons, or
               adapt to e.g. environmental changes. A
               neural network could learn from many            7. deleting existing neurons (and so, of
               things but, of course, there will always be        course, existing connections).
 From what
do we learn?



                                                                                                        51
Chapter 4 Fundamentals on learning and training samples (fundamental)          dkriesel.com


As mentioned above, we assume the               Thus, we let our neural network learn by
change in weight to be the most common          modifying the connecting weights accord-
procedure. Furthermore, deletion of con-        ing to rules that can be formulated as al-
                                                                                               Learning
nections can be realized by additionally        gorithms. Therefore a learning procedure       by changes
taking care that a connection is no longer      is always an algorithm that can easily be      in weight

trained when it is set to 0. Moreover, we       implemented by means of a programming
can develop further connections by setting      language. Later in the text I will assume
a non-existing connection (with the value       that the definition of the term desired out-
0 in the connection matrix) to a value dif-     put which is worth learning is known (and
ferent from 0. As for the modification of       I will define formally what a training pat-
threshold values I refer to the possibility     tern is) and that we have a training set
of implementing them as weights (section        of learning samples. Let a training set be
3.4). Thus, we perform any of the first four    defined as follows:
of the learning paradigms by just training
                                             Definition 4.1 (Training set). A train-
synaptic weights.                                                                              JP
                                             ing set (named P ) is a set of training
The change of neuron functions is difficult patterns, which we use to train our neu-
to implement, not very intuitive and not ral net.
exactly biologically motivated. Therefore
it is not very popular and I will omit this I will now introduce the three essential
topic here. The possibilities to develop or paradigms of learning by presenting the
delete neurons do not only provide well differences between their regarding train-
adjusted weights during the training of a ing sets.
neural network, but also optimize the net-
work topology. Thus, they attract a grow-
ing interest and are often realized by using 4.1.1 Unsupervised learning
evolutionary procedures. But, since we ac-          provides input patterns to the
cept that a large part of learning possibil-        network, but no learning aides
ities can already be covered by changes in
weight, they are also not the subject mat- Unsupervised learning is the biologi-
ter of this text (however, it is planned to cally most plausible method, but is not
extend the text towards those aspects of suitable for all problems. Only the in-
training).                                   put patterns are given; the network tries
                                             to identify similar patterns and to classify
    SNIPE:
                                             them into similar categories.
              Methods    of    the   class
    NeuralNetwork allow for changes in
     connection weights, and addition and
                                                Definition 4.2 (Unsupervised learning).
     removal of both connections and neurons.
     Methods in NeuralNetworkDescriptor          The training set only consists of input
     enable the change of neuron behaviors,     patterns, the network tries by itself to de-
     respectively activation functions per      tect similarities and to generate pattern
     layer.
                                                classes.



52                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
               dkriesel.com                                                        4.1 Paradigms of learning


               Here I want to refer again to the popu- according to their difference. The objec-
               lar example of Kohonen’s self-organising tive is to change the weights to the effect
               maps (chapter 10).                       that the network cannot only associate in-
                                                        put and output patterns independently af-
                                                        ter the training, but can provide plausible
               4.1.2 Reinforcement learning             results to unknown, similar input patterns,
                      methods provide feedback to i.e. it generalises.
                      the network, whether it
                      behaves well or bad                     Definition 4.4 (Supervised learning).
                                                              The training set consists of input patterns
               In reinforcement learning the network with correct results so that the 1network can
               receives a logical or a real value after receive a precise error vector can be re-
               completion of a sequence, which defines turned.
    network
    receives
  reward or    whether the result is right or wrong. Intu-
               itively it is clear that this procedure should This learning procedure is not always bio-
punishment

               be more effective than unsupervised learn- logically plausible, but it is extremely ef-
               ing since the network receives specific crit- fective and therefore very practicable.
               era for problem-solving.
                                                                At first we want to look at the the su-
               Definition 4.3 (Reinforcement learning).
                                                                pervised learning procedures in general,
                The training set consists of input patterns,
                                                                which - in this text - are corresponding
               after completion of a sequence a value is re-
                                                                to the following steps:
               turned to the network indicating whether
               the result was right or wrong and, possibly,     Entering the input pattern (activation of
               how right or wrong it was.                           input neurons),

                                                                Forward propagation of the input by the
               4.1.3 Supervised learning methods                    network, generation of the output,
                     provide training patterns                                                                    learning

                     together with appropriate
                                                                                                                  scheme
                                                                Comparing the output with the desired
                     desired outputs                               output (teaching input), provides er-
                                                                   ror vector (difference vector),
               In supervised learning the training set
               consists of input patterns as well as their Corrections of the network are
               correct results in the form of the precise ac-     calculated based on the error vector,
               tivation of all output neurons. Thus, for
               each training set that is fed into the net- Corrections are applied.
               work the output, for instance, can directly 1 The term error vector will be defined in section
   network
   receives    be compared with the correct solution and      4.2, where mathematical formalisation of learning
    correct    and the network weights can be changed         is discussed.
 results for
   samples


               D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                              53
Chapter 4 Fundamentals on learning and training samples (fundamental)          dkriesel.com


4.1.4 Offline or online learning?                 . How must the weights be modified to
                                                    allow fast and reliable learning?
It must be noted that learning can be
                                                  . How can the success of a learning pro-
offline (a set of training samples is pre-
                                                    cess be measured in an objective way?
sented, then the weights are changed, the
total error is calculated by means of a error     . Is it possible to determine the "best"
function operation or simply accumulated -          learning procedure?
see also section 4.4) or online (after every
                                                  . Is it possible to predict if a learning
sample presented the weights are changed).
                                                    procedure terminates, i.e. whether it
Both procedures have advantages and dis-
                                                    will reach an optimal state after a fi-
advantages, which will be discussed in the
                                                    nite time or if it, for example, will os-
learning procedures section if necessary.
                                                    cillate between different states?
Offline training procedures are also called
batch training procedures since a batch           . How can the learned patterns be
of results is corrected all at once. Such a         stored in the network?
training section of a whole batch of train-
                                                  . Is it possible to avoid that newly
ing samples including the related change
                                                    learned patterns destroy previously
in weight values is called epoch.
                                                    learned associations (the so-called sta-
Definition 4.5 (Offline learning). Sev-             bility/plasticity dilemma)?
eral training patterns are entered into the
                                            We will see that all these questions cannot
network at once, the errors are accumu-
                                            be generally answered but that they have
lated and it learns for all patterns at the                                                     JJJ
                                            to be discussed for each learning procedure
same time.                                                                                      no easy
                                            and each network topology individually.             answers!

Definition 4.6 (Online learning). The
network learns directly from the errors of
each training sample.                           4.2 Training patterns and
                                                    teaching input
4.1.5 Questions you should answer
      before learning                           Before we get to know our first learning
                                                rule, we need to introduce the teaching
                                                input. In (this) case of supervised learn-
The application of such schemes certainly
                                                ing we assume a training set consisting
requires preliminary thoughts about some
                                                of training patterns and the correspond-
questions, which I want to introduce now
                                                ing correct output values we want to see
as a check list and, if possible, answer                                                        desired
                                                at the output neurons after the training.
them in the course of this text:                                                                output
                                                While the network has not finished train-
  . Where does the learning input come          ing, i.e. as long as it is generating wrong
    from and in what form?                      outputs, these output values are referred



54                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
          dkriesel.com                                       4.2 Training patterns and teaching input


          to as teaching input, and that for each neu-      as well as simple preprocessing of the
          ron individually. Thus, for a neuron j with       training data.
          the incorrect output oj , tj is the teaching
          input, which means it is the correct or de- Definition 4.9 (Error vector). For sev-
          sired output for a training pattern p.         eral output neurons Ω1 , Ω2 , . . . , Ωn the dif-
                                                                                                             JEp
                                                         ference between output vector and teach-
          Definition 4.7 (Training patterns). A ing input under a training input p
  pI
          training pattern is an input vector p
          with the components p1 , p2 , . . . , pn whose                    
                                                                               t1 − y1
                                                                                         
          desired output is known. By entering the                    Ep =       ..
                                                                                   .
                                                                                        
          training pattern into the network we re-
                                                                                         
                                                                               tn − yn
          ceive an output that can be compared with
          the teaching input, which is the desired is referred to as error vector, sometimes
          output. The set of training patterns is it is also called difference vector. De-
          called P . It contains a finite number of or- pending on whether you are learning of-
          dered pairs(p, t) of training patterns with fline or online, the difference vector refers
          corresponding desired output.                  to a specific training pattern, or to the er-
                                                            ror of a set of training patterns which is
          Training patterns are often simply called normalized in a certain way.
          patterns, that is why they are referred
          to as p. In the literature as well as in Now I want to briefly summarize the vec-
          this text they are called synonymously pat- tors we have yet defined. There is the
          terns, training samples etc.
                                                            input vector x, which can be entered into
          Definition 4.8 (Teaching input). Let j                 the neural network. Depending on
  tI
          be an output neuron. The teaching in-                  the type of network being used the
          put tj is the desired and correct value j              neural network will output an
desired
output    should output after the input of a certain
          training pattern. Analogously to the vec- output vector y. Basically, the
          tor p the teaching inputs t1 , t2 , . . . , tn of training sample p is nothing more than
          the neurons can also be combined into a                an input vector. We only use it for
          vector t. t always refers to a specific train-         training purposes because we know
          ing pattern p and is, as already mentioned,            the corresponding
          contained in the set P of the training pat-
          terns.                                            teaching input t which is nothing more
                                                                 than the desired output vector to the
             SNIPE: Classes that are relevant                    training sample. The
             for training data are located in
             the package training.         The class      error vector Ep is the difference between
             TrainingSampleLesson allows for storage           the teaching input t and the actural
             of training patterns and teaching inputs,
                                                               output y.



          D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                             55
             Chapter 4 Fundamentals on learning and training samples (fundamental)              dkriesel.com


             So, what x and y are for the general net-
             work operation are p and t for the network
Important!
             training - and during training we try to
             bring y as close to t as possible. One ad-
             vice concerning notation: We referred to
             the output values of a neuron i as oi . Thus,
             the output of an output neuron Ω is called
             oΩ . But the output values of a network are
             referred to as yΩ . Certainly, these network
             outputs are only neuron outputs, too, but
             they are outputs of output neurons. In
             this respect
                                yΩ = oΩ
             is true.



             4.3 Using training samples

             We have seen how we can learn in prin-
             ciple and which steps are required to do
             so. Now we should take a look at the se-
             lection of training data and the learning
             curve. After successful learning it is par-
             ticularly interesting whether the network
             has only memorized – i.e. whether it can
             use our training samples to quite exactly
             produce the right output but to provide
             wrong answers for all other problems of
             the same class.

             Suppose that we want the network to train
             a mapping R2 → B1 and therefor use the
             training samples from fig. 4.1: Then there
             could be a chance that, finally, the net-
             work will exactly mark the colored areas
             around the training samples with the out-       Figure 4.1: Visualization of training results of
             put 1 (fig. 4.1, top), and otherwise will       the same training set on networks with a capacity
                                                             being too high (top), correct (middle) or too low
             output 0 . Thus, it has sufficient storage
                                                             (bottom).
             capacity to concentrate on the six training



             56                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                 4.3 Using training samples


samples with the output 1. This implies This means, that these data are included
an oversized network with too much free in the training, even if they are not used
storage capacity.                          explicitly for the training. The solution
                                           is a third set of validation data used only
On the other hand a network could have
                                           for validation after a supposably success-
insufficient capacity (fig. 4.1, bottom) –
                                           ful training.
this rough presentation of input data does
not correspond to the good generalization
                                           By training less patterns, we obviously
performance we desire. Thus, we have to
                                           withhold information from the network
find the balance (fig. 4.1, middle).
                                           and risk to worsen the learning perfor-
                                           mance. But this text is not about 100%
4.3.1 It is useful to divide the set of exact reproduction of given samples but
        training samples                   about successful generalization and ap-
                                           proximation of a whole function – for
                                           which it can definitely be useful to train
An often proposed solution for these prob-
                                           less information into the network.
lems is to divide, the training set into
  . one training set really used to train ,
  . and one verification set to test our 4.3.2 Order of pattern
    progress                                   representation
– provided that there are enough train-
ing samples. The usual division relations    You can find different strategies to choose
are, for instance, 70% for training data     the order of pattern presentation: If pat-
and 30% for verification data (randomly      terns are presented in random sequence,
chosen). We can finish the training when     there is no guarantee that the patterns
the network provides good results on the     are learned equally well (however, this is
training data as well as on the verification the standard method). Always the same
data.                                        sequence of patterns, on the other hand,
                                             provokes that the patterns will be memo-
   SNIPE: The method splitLesson within      rized when using recurrent networks (later,
   the class TrainingSampleLesson allows for
                                             we will learn more about this type of net-
   splitting a TrainingSampleLesson with re-
   spect to a given ratio.                   works). A random permutation would
                                             solve both problems, but it is – as already
                                             mentioned – very time-consuming to cal-
But note: If the verification data provide culate such a permutation.
poor results, do not modify the network
structure until these data provide good re-     SNIPE: The method shuffleSamples lo-
sults – otherwise you run the risk of tai-      cated in the class TrainingSampleLesson
loring the network to the verification data.    permutes a lesson.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      57
           Chapter 4 Fundamentals on learning and training samples (fundamental)                dkriesel.com


           4.4 Learning curve and error                     Generally, the root mean square is com-
               measurement                                  monly used since it considers extreme out-
                                                            liers to a greater extent.

           The learning curve indicates the progress        Definition 4.12 (Root mean square).
   norm    of the error, which can be determined in         The root mean square of two vectors t and
      to   various ways. The motivation to create a         y is defined as
compare
           learning curve is that such a curve can in-
                                                                               Ω∈O (tΩ    − yΩ )2
                                                                          sP
           dicate whether the network is progressing             Errp =                             .   (4.3)
           or not. For this, the error should be nor-                               |O|
           malized, i.e. represent a distance measure
           between the correct and the current out-         As for offline learning, the total error in
           put of the network. For example, we can          the course of one training epoch is inter-
           take the same pattern-specific, squared er-      esting and useful, too:
           ror with a prefactor, which we are also go-
                                                                           Err =         Errp           (4.4)
                                                                                   X
           ing to use to derive the backpropagation
                                                                                   p∈P
           of error (let Ω be output neurons and O
           the set of output neurons):                    Definition 4.13 (Total error). The total
                             1 X                          error Err is based on all training samples,
                    Errp =         (tΩ − yΩ )2      (4.1) that means it is generated offline.                   JErr
                             2 Ω∈O

           Definition 4.10 (Specific error). The Analogously we can generate a total RMS
           specific error Errp is based on a single and a total Euclidean distance in the
Errp I
           training sample, which means it is gener- course of a whole epoch. Of course, it is
           ated online.                                possible to use other types of error mea-
                                                       surement. To get used to further error
           Additionally, the root mean square (ab- measurement methods, I suggest to have a
           breviated: RMS) and the Euclidean look into the technical report of Prechelt
           distance are often used.                    [Pre94]. In this report, both error mea-
                                                       surement methods and sample problems
           The Euclidean distance (generalization of are discussed (this is why there will be a
           the theorem of Pythagoras) is useful for simmilar suggestion during the discussion
           lower dimensions where we can still visual- of exemplary problems).
           ize its usefulness.
                                                               SNIPE: There are several static meth-
           Definition 4.11 (Euclidean distance).               ods representing different methods of er-
           The Euclidean distance between two vec-             ror measurement implemented in the class
           tors t and y is defined as                          ErrorMeasurement.
                             sX
                   Errp =           (tΩ − yΩ )2 .   (4.2)   Depending on our method of error mea-
                              Ω∈O                           surement our learning curve certainly



           58                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                    4.4 Learning curve and error measurement


changes, too. A perfect learning curve         depends on a more objective view on the
looks like a negative exponential func-        comparison of several learning curves.
tion, that means it is proportional to e−t
(fig. 4.2 on the following page). Thus, the    Confidence in the results, for example, is
representation of the learning curve can be    boosted, when the network always reaches
                                                                                               objectivity
illustrated by means of a logarithmic scale    nearly the same final error-rate for differ-
(fig. 4.2, second diagram from the bot-        ent random initializations – so repeated
tom) – with the said scaling combination       initialization and training will provide a
a descending line implies an exponential       more objective result.
descent of the error.
                                               On the other hand, it can be possible that
With the network doing a good job, the         a curve descending fast in the beginning
problems being not too difficult and the       can, after a longer time of learning, be
logarithmic representation of Err you can      overtaken by another curve: This can indi-
see - metaphorically speaking - a descend-     cate that either the learning rate of the
ing line that often forms "spikes" at the      worse curve was too high or the worse
bottom – here, we reach the limit of the       curve itself simply got stuck in a local min-
64-bit resolution of our computer and our      imum, but was the first to find it.
network has actually learned the optimum
of what it is capable of learning.             Remember: Larger error values are worse
                                               than the small ones.
Typical learning curves can show a few flat
areas as well, i.e. they can show some
                                               But, in any case, note: Many people only
steps, which is no sign of a malfunctioning
                                               generate a learning curve in respect of the
learning process. As we can also see in fig.
                                               training data (and then they are surprised
4.2, a well-suited representation can make
                                               that only a few things will work) – but for
any slightly decreasing learning curve look
                                               reasons of objectivity and clarity it should
good – so just be cautious when reading
                                               not be forgotten to plot the verification
the literature.
                                               data on a second learning curve, which
                                               generally provides values that are slightly
                                               worse and with stronger oscillation. But
4.4.1 When do we stop learning?                with good generalization the curve can de-
                                               crease, too.
Now, the big question is: When do we
stop learning? Generally, the training is      When the network eventually begins to
stopped when the user in front of the learn-   memorize the samples, the shape of the
ing computer "thinks" the error was small      learning curve can provide an indication:
enough. Indeed, there is no easy answer        If the learning curve of the verification
and thus I can once again only give you        samples is suddenly and rapidly rising
something to think about, which, however,      while the learning curve of the verification



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          59
Chapter 4 Fundamentals on learning and training samples (fundamental)                                          dkriesel.com




          0.00025                                                               0.0002
                                                                               0.00018
           0.0002                                                              0.00016
                                                                               0.00014
          0.00015                                                              0.00012
 Fehler




                                                                      Fehler
                                                                                0.0001
           0.0001                                                               8e−005
                                                                                6e−005
           5e−005                                                               4e−005
                                                                                2e−005
                   0                                                                    0
                       0   100 200 300 400 500 600 700 800 900 1000                         1   10            100      1000
                                         Epoche                                                      Epoche
               1                                                                    1

          1e−005                                                               1e−005

          1e−010                                                               1e−010

          1e−015                                                               1e−015
 Fehler




                                                                      Fehler




          1e−020                                                               1e−020

          1e−025                                                               1e−025

          1e−030                                                               1e−030

          1e−035                                                               1e−035
                   0       100 200 300 400 500 600 700 800 900 1000                     1       10            100      1000
                                         Epoche                                                      Epoche


Figure 4.2: All four illustrations show the same (idealized, because very smooth) learning curve.
Note the alternating logarithmic and linear scalings! Also note the small "inaccurate spikes" visible
in the sharp bend of the curve in the first and second diagram from bottom.




60                                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                            4.5 Gradient optimization procedures


data is continuously falling, this could indi-   of its norm |g|. Thus, the gradient is a
cate memorizing and a generalization get-        generalization of the derivative for multi-
ting poorer and poorer. At this point it         dimensional functions. Accordingly, the
could be decided whether the network has         negative gradient −g exactly points to-
already learned well enough at the next          wards the steepest descent. The gradient
point of the two curves, and maybe the           operator ∇ is referred to as nabla op-
final point of learning is to be applied         erator, the overall notation of the the
                                                                                                             J∇
                                                                                                             gradient is
here (this procedure is called early stop-       gradient g of the point (x, y) of a two-                    multi-dim.
ping).                                           dimensional function f being g(x, y) =                      derivative
                                                 ∇f (x, y).
Once again I want to remind you that they
are all acting as indicators and not to draw Definition 4.14 (Gradient). Let g be
If-Then conclusions.                         a gradient. Then g is a vector with n
                                             components that is defined for any point
                                             of a (differential) n-dimensional function
                                             f (x1 , x2 , . . . , xn ). The gradient operator
4.5 Gradient optimization                    notation is defined as
      procedures                                   g(x1 , x2 , . . . , xn ) = ∇f (x1 , x2 , . . . , xn ).
                                                 g directs from any point of f towards
In order to establish the mathematical ba-
                                                 the steepest ascent from this point, with
sis for some of the following learning pro-
                                                 |g| corresponding to the degree of this as-
cedures I want to explain briefly what is
                                                 cent.
meant by gradient descent: the backpro-
pagation of error learning procedure, for        Gradient descent means to going downhill
example, involves this mathematical basis        in small steps from any starting point of
and thus inherits the advantages and dis-        our function towards the gradient g (which
advantages of the gradient descent.              means, vividly speaking, the direction to
Gradient descent procedures are generally        which a ball would roll from the starting
used where we want to maximize or mini-          point), with the size of the steps being pro-
mize n-dimensional functions. Due to clar-       portional to |g| (the steeper the descent,
ity the illustration (fig. 4.3 on the next       the longer the steps). Therefore, we move
page) shows only two dimensions, but prin-       slowly on a flat plateau, and on a steep as-
cipally there is no limit to the number of       cent we run downhill rapidly. If we came
dimensions.                                      into a valley, we would - depending on the
                                                 size of our steps - jump over it or we would
The gradient is a vector g that is de-           return into the valley across the opposite
fined for any differentiable point of a func-    hillside in order to come closer and closer
tion, that points from this point exactly        to the deepest point of the valley by walk-
towards the steepest ascent and indicates        ing back and forth, similar to our ball mov-
the gradient in this direction by means          ing within a round bowl.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                         61
              Chapter 4 Fundamentals on learning and training samples (fundamental)           dkriesel.com




              Figure 4.3: Visualization of the gradient descent on a two-dimensional error function. We
              move forward in the opposite direction of g, i.e. with the steepest descent towards the lowest
              point, with the step width being proportional to |g| (the steeper the descent, the faster the
              steps). On the left the area is shown in 3D, on the right the steps over the contour lines are
              shown in 2D. Here it is obvious how a movement is made in the opposite direction of g towards
              the minimum of the function and continuously slows down proportionally to |g|. Source:
              http://webster.fhs-hagenberg.ac.at/staff/sdreisei/Teaching/WS2001-2002/
              PatternClassification/graddescent.pdf




              Definition 4.15 (Gradient descent).              4.5.1 Gradient procedures
     We go
              Let f be an n-dimensional function and                 incorporate several problems
towards the   s = (s1 , s2 , . . . , sn ) the given starting
   gradient   point. Gradient descent means going              As already implied in section 4.5, the gra-
              from f (s) against the direction of g, i.e.      dient descent (and therefore the backpro-
              towards −g with steps of the size of |g|         pagation) is promising but not foolproof.
              towards smaller and smaller values of f .        One problem, is that the result does not
                                                               always reveal if an error has occurred.
                                                                                                               gradient
              Gradient descent procedures are not an er-                                                       descent
              rorless optimization procedure at all (as                                                        with errors

              we will see in the following sections) – how-    4.5.1.1 Often, gradient descents
              ever, they work still well on many prob-                 converge against suboptimal
              lems, which makes them an optimization                   minima
              paradigm that is frequently used. Anyway,
              let us have a look on their potential disad-     Every gradient descent procedure can, for
              vantages so we can keep them in mind a           example, get stuck within a local mini-
              bit.                                             mum (part a of fig. 4.4 on the facing page).



              62                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                             4.5 Gradient optimization procedures




Figure 4.4: Possible errors during a gradient descent: a) Detecting bad minima, b) Quasi-standstill
with small gradient, c) Oscillation in canyons, d) Leaving good minima.




This problem is increasing proportionally 4.5.1.3 Even if good minima are
to the size of the error surface, and there           reached, they may be left
is no universal solution. In reality, one             afterwards
cannot know if the optimal minimum is
reached and considers a training success-
ful, if an acceptable minimum is found.     On the other hand the gradient is very
                                            large at a steep slope so that large steps
                                            can be made and a good minimum can pos-
                                            sibly be missed (part d of fig. 4.4).
4.5.1.2 Flat plataeus on the error
          surface may cause training
          slowness
                                            4.5.1.4 Steep canyons in the error
                                                      surface may cause oscillations
When passing a flat plateau, for instance,
the gradient also becomes negligibly small
because there is hardly a descent (part b A sudden alternation from one very strong
of fig. 4.4), which requires many further negative gradient to a very strong positive
steps. A hypothetically possible gradient one can even result in oscillation (part c
of 0 would completely stop the descent.     of fig. 4.4). In nature, such an error does
                                            not occur very often so that we can think
                                            about the possibilities b and d.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                 63
Chapter 4 Fundamentals on learning and training samples (fundamental)             dkriesel.com


4.6 Exemplary problems allow                                i1      i2   i3   Ω
                                                            0       0    0    1
    for testing self-coded
                                                            0       0    1    0
    learning strategies                                     0       1    0    0
                                                            0       1    1    1
                                                            1       0    0    0
We looked at learning from the formal
                                                            1       0    1    1
point of view – not much yet but a little.
                                                            1       1    0    1
Now it is time to look at a few exemplary
                                                            1       1    1    0
problem you can later use to test imple-
mented networks and learning rules.        Table 4.1: Illustration of the parity function
                                               with three inputs.

4.6.1 Boolean functions

A popular example is the one that did          Another favourite example for singlelayer
not work in the nineteen-sixties: the XOR      perceptrons are the boolean functions
function (B2 → B1 ). We need a hidden          AND and OR.
neuron layer, which we have discussed in
detail. Thus, we need at least two neu-
rons in the inner layer. Let the activation    4.6.2 The parity function
function in all layers (except in the input
layer, of course) be the hyperbolic tangent.The parity function maps a set of bits to 1
Trivially, we now expect the outputs 1.0    or 0, depending on whether an even num-
or −1.0, depending on whether the func-     ber of input bits is set to 1 or not. Ba-
tion XOR outputs 1 or 0 - and exactly       sically, this is the function Bn → B1 . It
here is where the first beginner’s mistake  is characterized by easy learnability up to
occurs.                                     approx. n = 3 (shown in table 4.1), but
                                            the learning effort rapidly increases from
For outputs close to 1 or -1, i.e. close to n = 4. The reader may create a score ta-
the limits of the hyperbolic tangent (or ble for the 2-bit parity function. What is
in case of the Fermi function 0 or 1), we conspicuous?
need very large network inputs. The only
chance to reach these network inputs are
large weights, which have to be learned: 4.6.3 The 2-spiral problem
The learning process is largely extended.
Therefore it is wiser to enter the teaching As a training sample for a function let
inputs 0.9 or −0.9 as desired outputs or us take two spirals coiled into each other
to be satisfied when the network outputs (fig. 4.5 on the facing page) with the
those values instead of 1 and −1.           function certainly representing a mapping



64                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                            4.6 Exemplary problems




Figure 4.5: Illustration of the training samples   Figure 4.6: Illustration of training samples for
of the 2-spiral problem                            the checkerboard problem




R2 → B1 . One of the spirals is assigned           suitable for this kind of problems than the
to the output value 1, the other spiral to         MLP).
0. Here, memorizing does not help. The             The 2-spiral problem is very similar to the
network has to understand the mapping it-          checkerboard problem, only that, mathe-
self. This example can be solved by means          matically speaking, the first problem is us-
of an MLP, too.                                    ing polar coordinates instead of Cartesian
                                                   coordinates. I just want to introduce as
                                                   an example one last trivial case: the iden-
4.6.4 The checkerboard problem                     tity.

We again create a two-dimensional func-
tion of the form R2 → B1 and specify               4.6.5 The identity function
checkered training samples (fig. 4.6) with
one colored field representing 1 and all the       By using linear activation functions the
rest of them representing 0. The difficulty        identity mapping from R1 to R1 (of course
increases proportionally to the size of the        only within the parameters of the used ac-
function: While a 3×3 field is easy to learn,      tivation function) is no problem for the
the larger fields are more difficult (here         network, but we put some obstacles in its
we eventually use methods that are more            way by using our sigmoid functions so that



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                 65
           Chapter 4 Fundamentals on learning and training samples (fundamental)             dkriesel.com


           it would be difficult for the network to                   ∆wi,j ∼ ηoi aj         (4.5)
           learn the identity. Just try it for the fun
           of it.                                      with ∆wi,j being the change in weight
                                                       from i to j , which is proportional to the
           Now, it is time to hava a look at our first following factors:                                    J∆wi,j
           mathematical learning rule.
                                                         . the output oi of the predecessor neu-
                                                            ron i, as well as,
           4.6.6 There are lots of other
                                                              . the activation aj of the successor neu-
                 exemplary problems
                                                                ron j,
           For lots and lots of further exemplary prob-   . a constant η, i.e. the learning rate,
           lems, I want to recommend the technical          which will be discussed in section
           report written by prechelt [Pre94] which         5.4.3.
           also has been named in the sections about
           error measurement procedures..               The changes in weight ∆wi,j are simply
                                                        added to the weight wi,j .
                                                           Why am I speaking twice about activation,
           4.7 The Hebbian learning rule
                                                           but in the formula I am using oi and aj , i.e.
               is the basis for most                       the output of neuron of neuron i and the ac-
               other learning rules                        tivation of neuron j? Remember that the
                                                           identity is often used as output function
           In 1949, Donald O. Hebb formulated              and therefore ai and oi of a neuron are of-
           the Hebbian rule [Heb49] which is the ba-       ten the same. Besides, Hebb postulated
           sis for most of the more complicated learn-     his rule long before the specification of
           ing rules we will discuss in this text. We      technical neurons. Considering that this
           distinguish between the original form and       learning rule was preferred in binary acti-
           the more general form, which is a kind of       vations, it is clear that with the possible
           principle for other learning rules.             activations (1, 0) the weights will either in-
                                                           crease or remain constant. Sooner or later
                                                                                                             weights
                                                           they would go ad infinitum, since they can        go ad
           4.7.1 Original rule                             only be corrected "upwards" when an error         infinitum

                                                           occurs. This can be compensated by using
           Definition 4.16 (Hebbian rule). "If neu-        the activations (-1,1)2 . Thus, the weights
           ron j receives an input from neuron i and       are decreased when the activation of the
           if both neurons are strongly active at the      predecessor neuron dissents from the one
           same time, then increase the weight wi,j        of the successor neuron, otherwise they are
           (i.e. the strength of the connection be-        increased.
           tween i and j)." Mathematically speaking, 2 But that is no longer the "original version" of the
           the rule is:                                Hebbian rule.
   early
form of
the rule

           66                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                         4.7 Hebbian rule


4.7.2 Generalized form                               Exercises

Most of the learning rules discussed before Exercise 7. Calculate the average value
are a specialization of the mathematically µ and the standard deviation σ for the fol-
more general form [MR86] of the Hebbian lowing data points.
rule.
                                                         p1 = (2, 2, 2)
Definition 4.17 (Hebbian rule, more gen-                 p2 = (3, 3, 3)
eral). The generalized form of the
Hebbian Rule only specifies the propor-                  p3 = (4, 4, 4)
tionality of the change in weight to the                 p4 = (6, 0, 0)
product of two undefined functions, but                  p5 = (0, 6, 0)
with defined input values.                               p6 = (0, 0, 6)
   ∆wi,j = η · h(oi , wi,j ) · g(aj , tj )   (4.6)


Thus, the product of the functions

  . g(aj , tj ) and

  . h(oi , wi,j )

  . as well as the constant learning rate
    η

results in the change in weight. As you
can see, h receives the output of the pre-
decessor cell oi as well as the weight from
predecessor to successor wi,j while g ex-
pects the actual and desired activation of
the successor aj and tj (here t stands for
the aforementioned teaching input). As al-
ready mentioned g and h are not specified
in this general definition. Therefore, we
will now return to the path of specializa-
tion we discussed before equation 4.6. Af-
ter we have had a short picture of what
a learning rule could look like and of our
thoughts about learning itself, we will be
introduced to our first network paradigm
including the learning procedure.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                    67
          Part II

Supervised learning network
         paradigms




                              69
Chapter 5
The perceptron, backpropagation and its
variants
                  A classic among the neural networks. If we talk about a neural network, then
                        in the majority of cases we speak about a percepton or a variation of it.
                    Perceptrons are multilayer networks without recurrence and with fixed input
                   and output layers. Description of a perceptron, its limits and extensions that
                  should avoid the limitations. Derivation of learning procedures and discussion
                                                                                of their problems.




As already mentioned in the history of neu-     ble output values (e.g. {0, 1} or {−1, 1}).
ral networks, the perceptron was described      Thus, a binary threshold function is used
by Frank Rosenblatt in 1958 [Ros58].            as activation function, depending on the
Initially, Rosenblatt defined the already       threshold value Θ of the output neuron.
discussed weighted sum and a non-linear
activation function as components of the      In a way, the binary activation function
perceptron.                                   represents an IF query which can also
                                              be negated by means of negative weights.
There is no established definition for a per- The perceptron can thus be used to ac-
ceptron, but most of the time the term complish true logical information process-
is used to describe a feedforward network ing.
with shortcut connections. This network
has a layer of scanner neurons (retina) Whether this method is reasonable is an-
with statically weighted connections to other matter – of course, this is not the
the following layer and is called input easiest way to achieve Boolean logic. I just
layer (fig. 5.1 on the next page); but the want to illustrate that perceptrons can
weights of all other layers are allowed to be be used as simple logical components and
changed. All neurons subordinate to the that, theoretically speaking, any Boolean
retina are pattern detectors. Here we ini- function can be realized by means of per-
tially use a binary perceptron with every ceptrons being connected in series or in-
output neuron having exactly two possi- terconnected in a sophisticated way. But



                                                                                              71
            Kapitel 5 Das Perceptron                                                                           dkriesel.com




Chapter 5 The perceptron, backpropagation and its variants                                                dkriesel.com




                                                         GFED
                                                         @ABC        GFED
                                                                     @ABC              )GFED
                                                                                        @ABC             @ABC
                                                                                                      + )GFED           @ABC
                                                                                                                    ,+ )GFED
                                                            |        "  { us          #  {u           #  |           " 
                                                               u
                                                           Osr O      @                                               
                                                                 OOO
                                                                    OOO @@@                          ~~        ooooo
                                                                        OOO @@                     ~~       ooo
                                                                               OOO @@           ~~~ ooooo

                                                                                     PQRS
                                                                                     WVUT
                                                                                  OOO@       ~ o
                                                                                     ' Σ ~ wooo
                                                                                       L|H
                                                                                          



                                                          i1 PPP GFED
                                                         GFED
                                                         @ABC      @ABC
                                                                    i2 C        @ABC
                                                                                GFEDi3           @ABC
                                                                                                 GFED
                                                                                                   i4       @ABC
                                                                                                            GFED
                                                                                                             i
                                                                PPP     CC                      {{      nnnn 5
                                                                   PPP    CC                   {      nn
                                                                      PPP CC                 {{ nnn
                                                                         PPP CC            {{ nnn
                                                                                   89:; vn
                                                                                 ( ?>=<
                                                                             PPPC!  {} {n{nnnn
                                                                                         Ω

                                                                                              

                       5.1: of
             Architecture
Figure 5.1:Abbildung            a perceptron
                            Aufbau             with onemit
                                     eines Perceptrons    layer  ofSchicht
                                                             einer  variablevariabler
                                                                              connections    in different
                                                                                      Verbindungen         views.
                                                                                                      in verschiede-
           nen Ansichten.
The solid-drawn            Die durchgezogene
                  weight layer                  Gewichtsschicht
                                 in the two illustrations           den unteren
                                                           on thein bottom   can beiden  Abbildungen ist trainier-
                                                                                  be trained.
Left side: bar.
            Example of scanning information in the eye.
Right side,Oben:
             upper     Beispiel
                   Ampart:      der Informationsabtastung
                            Drawing    of the same example      Auge.indicated fixed-weight layer using the
                                                            im with
           Mitte: Skizze desselben mit eingezeichneter fester Gewichtsschicht unter Verwendung der definier-
defined designs of the functional descriptions for neurons.
           ten funktionsbeschreibenden Designs für Neurone.
Right side,    lower part: Without indicated fixed-weight layer, with the name of each neuron
           Unten: Ohne eingezeichnete feste Gewichtsschicht, mit Benennung der einzelnen Neuronen nach
corresponding   to our
           unserer     convention.
                   Konvention.        The fixed-weight
                                 Wir werden              layer will noim
                                             die feste Gewichtschicht   longer  be taken
                                                                          weiteren  Verlaufinto
                                                                                             der account   in the
                                                                                                 Arbeit nicht  mehr
course of this  work.
           betrachten.


            70                  D. Kriesel – Ein kleiner Überblick über Neuronale Netze (EPSILON-DE)




72                          D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                dkriesel.com


                we will see that this is not possible without   Now that we know the components of a
                connecting them serially. Before providing      perceptron we should be able to define
                the definition of the perceptron, I want to     it.
                define some types of neurons used in this
                chapter.                                        Definition 5.3 (Perceptron). The per-
                                                                ceptron (fig. 5.1 on the facing page) is1 a
                Definition 5.1 (Input neuron). An in-           feedforward network containing a retina
                put neuron is an identity neuron. It            that is used only for data acquisition and
                exactly forwards the information received.      which has fixed-weighted connections with
                Thus, it represents the identity function,      the first neuron layer (input layer). The
                                                                fixed-weight layer is followed by at least
input neuron
only forwards   which should be indicated by the symbol
                . Therefore the input neuron is repre-         one trainable weight layer. One neuron
                                     @ABC
                sented by the symbol GFED
         data
                                                                layer is completely linked with the follow-
                                       .
                                                                ing layer. The first layer of the percep-
                                                                tron consists of the input neurons defined
                Definition 5.2 (Information process-            above.
                ing neuron). Information processing
                neurons somehow process the input infor-
                mation, i.e. do not represent the identity      A feedforward network often contains
                function. A binary neuron sums up all           shortcuts which does not exactly corre-
                inputs by using the weighted sum as prop-       spond to the original description and there-
                agation function, which we want to illus-       fore is not included in the definition. We
                trate by the sign Σ. Then the activation        can see that the retina is not included in
                function of the neuron is the binary thresh-    the lower part of fig. 5.1. As a matter
                old function, which can be illustrated by       of fact the first neuron layer is often un-
                L|H  . This leads us to the complete de-        derstood (simplified and sufficient for this
                piction of information processing neurons,      method) as input layer, because this layer
                       PQRS
                namely WVUT
                                                                                                                         retina is
                         Σ                                      only forwards the input values. The retina
                        L|H . Other neurons that use            itself and the static weights behind it are
                                                                                                                         unconsidered

                the weighted sum as propagation function        no longer mentioned or displayed, since
                but the activation functions hyperbolic tan-    they do not process information in any
                gent or Fermi function, or with a sepa-         case. So, the depiction of a perceptron
                rately defined activation function fact , are   starts with the input neurons.
                similarly represented by


                      PQRS
                      WVUT
                        Σ          PQRS
                                   WVUT
                                     Σ          HIJK
                                                ONML
                                                  Σ
                                                     .          1 It may confuse some readers that I claim that there
                        Tanh        Fermi        fact             is no definition of a perceptron but then define the
                                                                  perceptron in the following section. I therefore
                                                                  suggest keeping my definition in the back of your
                These neurons are also referred to as             mind and just take it for granted in the course of
                Fermi neurons or Tanh neuron.                     this work.




                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                   73
               Chapter 5 The perceptron, backpropagation and its variants                                         dkriesel.com


                    SNIPE:              The            methods
                    setSettingsTopologyFeedForward

                                                                                      @ABC
                                                                                      GFED        @ABC
                                                                                                  GFED            @ABC
                                                                                                                  GFED
                    and the variation -WithShortcuts in
                    a      NeuralNetworkDescriptor-Instance
                    apply settings to a descriptor, which
                                                                                       BIAS        i1               i2
                                                                                                                 
                    are appropriate for feedforward networks
                                                                                           wBIAS,Ωwi1 ,Ω wi2 ,Ω
                                                                                                           
                                                                                                   ?>=<
                                                                                                   89:;
                                                                                                      
                    or feedforward networks with shortcuts.
                    The respective kinds of connections are
                    allowed, all others are not, and fastprop is                                    Ω
                    activated.
                                                                                                     


               5.1 The singlelayer                                       Figure 5.2: A singlelayer perceptron with two in-
                   perceptron provides only                              put neurons and one output neuron. The net-
                                                                         work returns the output by means of the ar-
                   one trainable weight layer                            row leaving the network. The trainable layer of
                                                                         weights is situated in the center (labeled). As a
                                                                         reminder, the bias neuron is again included here.
               Here, connections with trainable weights                  Although the weight wBIAS,Ω is a normal weight
               go from the input layer to an output                      and also treated like this, I have represented it
               neuron Ω, which returns the information                   by a dotted line – which significantly increases
                                                                         the clarity of larger networks. In future, the bias
1 trainable
       layer   whether the pattern entered at the input
               neurons was recognized or not. Thus, a                    neuron will no longer be included.
               singlelayer perception (abbreviated SLP)
               has only one level of trainable weights
               (fig. 5.1 on page 72).

               Definition 5.4 (Singlelayer perceptron).

                                                          i1 @PUPUUU GFED
                                                         GFED
               perceptron having only one layer of vari- @ABC          @ABC                GFED
                                                                                           @ABC                       @ABC
                                                                                                                      GFED      @ABC
                                                                                                                               iGFED
                A singlelayer perceptron (SLP) is a
                                                                        i2 P                  i3                       i4        i5
                                                               @@PPPUPUUUU AAPAPPPP}} AAAnnnn}n} iiiinininin~n~
               able weights and one layer of output neu-                   U             P
                                                                  @@ PPP UUAUA}} PP nn AA}i}ii nnn ~~
                                                                          PP }UAUU nPnP ii}iA nn
                                                                                                           n           i
                                                                    @@                                                       ~
               rons Ω. The technical view of an SLP is                @@  P}}P}PnPnAnAinAUinUiU iUiPUiP}U}P}PnPnAnAAn  ~~~
                                                                       @ABC
                                                                       GFED                GFED
                                                                                           @ABC                       @ABC
                                                                                                                      GFED
                                                                                     n i
                                                                                       P
                                                                            ~}vnit niii P'                   U
                                                                                                             n    P
                                                                                                      ~}nw n UUUP* (      ~~
               shown in fig. 5.2.                                       Ω1                   Ω2                        Ω3

               Certainly, the existence of several output                                   
               neurons Ω1 , Ω2 , . . . , Ωn does not consider-
               ably change the concept of the perceptron
Important!
               (fig. 5.3): A perceptron with several out- Figure 5.3: Singlelayer perceptron with several
                                                               output neurons
               put neurons can also be regarded as sev-
               eral different perceptrons with the same
               input.



               74                           D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                   5.1 The singlelayer perceptron


                                                  5.1.1 Perceptron learning algorithm
         GFED
         @ABC
          A                       GFED
                                   @ABC
                                                       and convergence theorem
              AA                  }}
                A               }}
                 1AA          }1
                     GFED
                     @ABC
                    AA       }
                          ~}}                     The original perceptron learning algo-
                      1.5                         rithm with binary neuron activation func-
                                                  tion is described in alg. 1. It has been
                                                 proven that the algorithm converges in
                                                  finite time – so in finite time the per-
         @ABC
         GFED
          A                         GFED
                                     @ABC
                                       
                                                  ceptron can learn anything it can repre-
              AA                    }}            sent (perceptron convergence theorem,
                A                 }}
                 1AA          1                   [Ros62]). But please do not get your hopes
                             }}
                     @ABC
                     GFED
                    AA
                          ~}}                     up too soon! What the perceptron is capa-
                      0.5                         ble to represent will be explored later.

                                               During the exploration of linear separabil-
                                                ity of problems we will cover the fact that
                                                at least the singlelayer perceptron unfor-
Figure 5.4: Two singlelayer perceptrons for
                                                tunately cannot represent a lot of prob-
Boolean functions. The upper singlelayer per-
ceptron realizes an AND, the lower one realizes
                                                lems.
an OR. The activation function of the informa-
tion processing neuron is the binary threshold
function. Where available, the threshold values   5.1.2 The delta rule as a gradient
are written into the neurons.                           based learning strategy for
                                                        SLPs

                                                  In the following we deviate from our bi-
                                                  nary threshold value as activation function
                                                  because at least for backpropagation of er-
The Boolean functions AND and OR shown
                                                  ror we need, as you will see, a differen-
in fig. 5.4 are trivial examples that can eas-                                                    fact now differ-
                                                  tiable or even a semi-linear activation func-
ily be composed.
                                                                                                  entiable
                                                  tion. For the now following delta rule (like
                                                  backpropagation derived in [MR86]) it is
Now we want to know how to train a single-        not always necessary but useful. This fact,
layer perceptron. We will therefore at first      however, will also be pointed out in the
take a look at the perceptron learning al-        appropriate part of this work. Compared
gorithm and then we will look at the delta        with the aforementioned perceptron learn-
rule.                                             ing algorithm, the delta rule has the ad-
                                                  vantage to be suitable for non-binary acti-
                                                  vation functions and, being far away from



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                             75
Chapter 5 The perceptron, backpropagation and its variants              dkriesel.com




 1: while ∃p ∈ P and error too large do
 2:    Input p into the network, calculate output y {P set of training patterns}
 3:    for all output neurons Ω do
 4:      if yΩ = tΩ then
 5:         Output is okay, no correction of weights
 6:      else
 7:         if yΩ = 0 then
 8:            for all input neurons i do
 9:              wi,Ω := wi,Ω + oi {...increase weight towards Ω by oi }
10:            end for
11:         end if
12:         if yΩ = 1 then
13:            for all input neurons i do
14:              wi,Ω := wi,Ω − oi {...decrease weight towards Ω by oi }
15:            end for
16:         end if
17:      end if
18:    end for
19: end while
Algorithm 1: Perceptron learning algorithm. The perceptron learning algorithm
reduces the weights to output neurons that return 1 instead of 0, and in the inverse
case increases weights.




76                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                  5.1 The singlelayer perceptron


the learning target, to automatically learn of the network is approximately the de-
faster.                                        sired output t, i.e. formally it is true
                                               that
Suppose that we have a singlelayer percep-
tron with randomly set weights which we               ∀p : y ≈ t or ∀p : Ep ≈ 0.
want to teach a function by means of train-
ing samples. The set of these training sam-
                                               This means we first have to understand the
ples is called P . It contains, as already de-
                                               total error Err as a function of the weights:
fined, the pairs (p, t) of the training sam-
                                               The total error increases or decreases de-
ples p and the associated teaching input t.
                                               pending on how we change the weights.
I also want to remind you that
                                              Definition 5.5 (Error function). The er-
  . x is the input vector and
                                              ror function
  . y is the output vector of a neural net-
                                                                                                       JErr(W )
    work,                                                       Err : W → R

  . output neurons are referred to as regards the set2 of weights W as a vector
     Ω1 , Ω2 , . . . , Ω|O| ,               and maps the values onto the normalized                    error as
                                            output error (normalized because other-                    function
  . i is the input and                      wise not all errors can be mapped onto
                                            one single e ∈ R to perform a gradient de-
  . o is the output of a neuron.
                                            scent). It is obvious that a specific error
Additionally, we defined that               function can analogously be generated
                                            for a single pattern p.                                    JErrp (W )
  . the error vector Ep represents the dif-
     ference (t−y) under a certain training
                                            As already shown in section 4.5, gradient
     sample p.
                                            descent procedures calculate the gradient
  . Furthermore, let O be the set of out- of an arbitrary but finite-dimensional func-
     put neurons and                        tion (here: of the error function Err(W ))
                                            and move down against the direction of
  . I be the set of input neurons.          the gradient until a minimum is reached.
                                            Err(W ) is defined on the set of all weights
Another naming convention shall be that,
                                            which we here regard as the vector W .
for example, for an output o and a teach-
                                            So we try to decrease or to minimize the
ing input t an additional index p may be
                                            error by simply tweaking the weights –
set in order to indicate that these values
                                            thus one receives information about how
are pattern-specific. Sometimes this will
                                            to change the weights (the change in all
considerably enhance clarity.
                                               2 Following the tradition of the literature, I previ-
Now our learning target will certainly be,       ously defined W as a weight matrix. I am aware
that for all training samples the output y       of this conflict but it should not bother us here.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                 77
Chapter 5 The perceptron, backpropagation and its variants                              dkriesel.com
5
4
3                                                     Thus, we tweak every single weight and ob-
2                                                     serve how the error function changes, i.e.
1
                                                      we derive the error function according to
                                                      a weight wi,Ω and obtain the value ∆wi,Ω
0

                                                      of how to change this weight.
    −2
         −1                                    2
                                          1
              0                      0
         w1
                                                                               ∂Err(W )
                   1            −1       w2
                                                                ∆wi,Ω = −η
                       2   −2
                                                                                        .        (5.3)
                                                                                ∂wi,Ω
Figure 5.5: Exemplary error surface of a neural
network with two trainable connections w1 und
w2 . Generally, neural networks have more than
two connections, but this would have made the         Now the following question arises: How
illustration too complex. And most of the time        is our error function defined exactly? It
the error surface is too craggy, which complicates    is not good if many results are far away
the search for the minimum.                           from the desired ones; the error function
                                                      should then provide large values – on the
                                                      other hand, it is similarly bad if many
                                                      results are close to the desired ones but
weights is referred to as ∆W ) by calcu-              there exists an extremely far outlying re-
lating the gradient ∇Err(W ) of the error             sult. The squared distance between the
function Err(W ):                                     output vector y and the teaching input t
                                                      appears adequate to our needs. It provides
                  ∆W ∼ −∇Err(W ).             (5.1)   the error Errp that is specific for a train-
                                                      ing sample p over the output of all output
Due to this relation there is a proportional-         neurons Ω:
ity constant η for which equality holds (η
will soon get another meaning and a real
practical use beyond the mere meaning of
a proportionality constant. I just ask the                             1 X
                                                         Errp (W ) =         (tp,Ω − yp,Ω )2 .   (5.4)
reader to be patient for a while.):                                    2 Ω∈O

                  ∆W = −η∇Err(W ).            (5.2)

To simplify further analysis, we now                  Thus, we calculate the squared difference
rewrite the gradient of the error-function            of the components of the vectors t and
according to all weights as an usual par-             y, given the pattern p, and sum up these
tial derivative according to a single weight          squares. The summation of the specific er-
wi,Ω (the only variable weights exists be-            rors Errp (W ) of all patterns p then yields
tween the hidden and the output layer Ω).             the definition of the error Err and there-



78                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                         5.1 The singlelayer perceptron


fore the definition of the error function results from the sum of the specific er-
Err(W ):                                  rors):

 Err(W ) =           Errp (W )                 (5.5)                      ∂Err(W )
               X
                                                            ∆wi,Ω = −η                              (5.7)
               p∈P                                                         ∂wi,Ω
                            sum over all p
                                                                             ∂Errp (W )
                                                                    =                               (5.8)
                                                                      X
              z     }|            {                                     −η            .
            1 X X                                                             ∂wi,Ω
                   (tp,Ω − yp,Ω )2  .
                                                                      p∈P
          =
               2 p∈P       Ω∈O
                       |            {z         }       Once again I want to think about the ques-
                                                       tion of how a neural network processes
                              sum over all Ω
                                               (5.6)
                                                       data. Basically, the data is only trans-
The observant reader will certainly wonder             ferred through a function, the result of the
where the factor 21 in equation 5.4 on the             function is sent through another one, and
preceding page suddenly came from and                  so on. If we ignore the output function,
why there is no root in the equation, as               the path of the neuron outputs oi1 and oi2 ,
this formula looks very similar to the Eu-             which the neurons i1 and i2 entered into a
clidean distance. Both facts result from               neuron Ω, initially is the propagation func-
simple pragmatics: Our intention is to                 tion (here weighted sum), from which the
minimize the error. Because the root func-             network input is going to be received. This
tion decreases with its argument, we can               is then sent through the activation func-
simply omit it for reasons of calculation              tion of the neuron Ω so that we receive
and implementation efforts, since we do                the output of this neuron which is at the
not need it for minimization. Similarly, it            same time a component of the output vec-
does not matter if the term to be mini-                tor y:
mized is divided by 2: Therefore I am al-
lowed to multiply by 21 . This is just done                         netΩ → fact
so that it cancels with a 2 in the course of                               = fact (netΩ )
our calculation.                                                           = oΩ
Now we want to continue deriving the                                       = yΩ .
delta rule for linear activation functions.
We have already discussed that we tweak                As we can see, this output results from
the individual weights wi,Ω a bit and see              many nested functions:
how the error Err(W ) is changing – which
                                                         oΩ = fact (netΩ )                          (5.9)
corresponds to the derivative of the er-
ror function Err(W ) according to the very                  = fact (oi1 · wi1 ,Ω + oi2 · wi2 ,Ω ). (5.10)
same weight wi,Ω . This derivative cor-
responds to the sum of the derivatives                 It is clear that we could break down the
of all specific errors Errp according to               output into the single input neurons (this
this weight (since the total error Err(W )             is unnecessary here, since they do not



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                       79
Chapter 5 The perceptron, backpropagation and its variants                            dkriesel.com


process information in an SLP). Thus,            Due to the requirement at the beginning of
we want to calculate the derivatives of          the derivation, we only have a linear acti-
equation 5.8 on the preceding page and           vation function fact , therefore we can just
due to the nested functions we can apply         as well look at the change of the network
the chain rule to factorize the derivative       input when wi,Ω is changing:
∂Errp (W )
           in equation 5.8 on the previous
                                                     ∂Errp (W )               i∈I (op,i wi,Ω )
  ∂wi,Ω
page.                                                                     ∂
                                                                              P
                                                                = −δp,Ω ·                         .
                                                       ∂wi,Ω                      ∂wi,Ω
     ∂Errp (W )   ∂Errp (W ) ∂op,Ω                                                            (5.14)
                =           ·       .   (5.11)
       ∂wi,Ω        ∂op,Ω     ∂wi,Ω
                                                                                          (op,i wi,Ω )
                                                                                      P
                                                                                  ∂
Let us take a look at the first multiplica- The resulting derivative
                                                                            i∈I
                                                                              ∂wi,Ω
tive factor of the above equation 5.11 P    can  now   be   simplified:   The  function
which represents the derivative of the spe-      (o  w
                                              i∈I p,i i,Ω )  to be  derived consists of
cific error Errp (W ) according to the out- many summands, and only the sum-
put, i.e. the change of the error Errp mand op,i wi,Ω contains the variable wi,Ω ,
with an output op,Ω : The examination according        to which we derive.         Thus,
                                                         (op,i wi,Ω )
                                             P
                                                 ∂
of Errp (equation 5.4 on page 78) clearly        i∈I
                                                  ∂wi,Ω      = op,i and therefore:
shows that this change is exactly the dif-
ference between teaching input and out-
put (tp,Ω − op,Ω ) (remember: Since Ω is an          ∂Errp (W )
output neuron, op,Ω = yp,Ω ). The closer                        = −δp,Ω · op,i     (5.15)
                                                       ∂wi,Ω
the output is to the teaching input, the
                                                                = −op,i · δp,Ω .   (5.16)
smaller is the specific error. Thus we can
replace one by the other. This difference
is also called δp,Ω (which is the reason for We insert this in equation 5.8 on the previ-
the name delta rule):                        ous page, which results in our modification
                                             rule for a weight wi,Ω :
    ∂Errp (W )                      ∂op,Ω
                = −(tp,Ω − op,Ω ) ·
                                                    ∆wi,Ω = η ·                    (5.17)
                                                                X
       ∂wi,Ω                        ∂wi,Ω                           op,i · δp,Ω .
                                        (5.12)                          p∈P

                            ∂op,Ω
                = −δp,Ω ·               (5.13)   However: From the very beginning the
                            ∂wi,Ω                derivation has been intended as an offline
                                                 rule by means of the question of how to
The second multiplicative factor of equa-        add the errors of all patterns and how to
tion 5.11 and of the following one is the        learn them after all patterns have been
derivative of the output specific to the pat-    represented. Although this approach is
tern p of the neuron Ω according to the          mathematically correct, the implementa-
weight wi,Ω . So how does op,Ω change            tion is far more time-consuming and, as
when the weight from i to Ω is changed?          we will see later in this chapter, partially



80                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                         5.2 Linear separability


needs a lot of compuational effort during        In. 1      In. 2      Output
training.                                           0          0       0
                                                    0          1       1
The "online-learning version" of the delta
                                                    1          0       1
rule simply omits the summation and
                                                    1          1       0
learning is realized immediately after the
presentation of each pattern, this also sim- Table 5.1: Definition of the logical XOR. The
plifies the notation (which is no longer nec- input values are shown of the left, the output
essarily related to a pattern p):             values on the right.

            ∆wi,Ω = η · oi · δΩ .      (5.18)

This version of the delta rule shall be used
for the following definition:                     . to the difference between the current
Definition 5.6 (Delta rule). If we deter-           activation or output aΩ or oΩ and the
mine, analogously to the aforementioned             corresponding teaching input tΩ . We
derivation, that the function h of the Heb-         want to refer to this factor as δΩ ,
                                                    which is also referred to as "Delta".
                                                                                               Jδ
bian theory (equation 4.6 on page 67) only
provides the output oi of the predecessor
                                                Apparently the delta rule only applies for
neuron i and if the function g is the differ-
                                                SLPs, since the formula is always related
ence between the desired activation tΩ and
                                                to the teaching input, and there is no
the actual activation aΩ , we will receive                                                     delta rule
                                                teaching input for the inner processing lay-
the delta rule, also known as Widrow-
                                                                                               only for SLP
                                                ers of neurons.
Hoff rule:

 ∆wi,Ω = η · oi · (tΩ − aΩ ) = ηoi δΩ (5.19)

If we use the desired output (instead of the
                                                5.2 A SLP is only capable of
activation) as teaching input, and there-           representing linearly
fore the output function of the output neu-         separable data
rons does not represent an identity, we ob-
tain
                                              Let f be the XOR function which expects
  ∆wi,Ω = η · oi · (tΩ − oΩ ) = ηoi δΩ (5.20) two binary inputs and generates a binary
                                              output (for the precise definition see ta-
and δΩ then corresponds to the difference ble 5.1).
between tΩ and oΩ .
                                              Let us try to represent the XOR func-
In the case of the delta rule, the change tion by means of an SLP with two input
of all weights to an output neuron Ω is neurons i1 , i2 and one output neuron Ω
proportional                                  (fig. 5.6 on the following page).



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          81
Chapter 5 The perceptron, backpropagation and its variants                           dkriesel.com




           GFED
           @ABC                              GFED
                                             @ABC
                                               
            i1 B                               i2
                BB                           |
                  B                       |||
                        BB
                   wi1 ,Ω           w
                                    ||
                                      i2 ,Ω


                            ?>=<
                            89:;
                          BB
                                |~ |
                             Ω

                              
                           XOR?


Figure 5.6: Sketch of a singlelayer perceptron
that shall represent the XOR function - which is
impossible.




Here we use the weighted sum as propaga-
tion function, a binary activation function
with the threshold value Θ and the iden-
tity as output function. Depending on i1
and i2 , Ω has to output the value 1 if the
following holds:

     netΩ = oi1 wi1 ,Ω + oi2 wi2 ,Ω ≥ ΘΩ     Figure 5.7: Linear separation of n = 2 inputs of
                                                    (5.21)
                                             the input neurons i1 and i2 by a 1-dimensional
We assume a positive weight wi2 ,Ω , the in- straight line. A and B show the corners belong-
equality 5.21 is then equivalent to          ing to the sets of the XOR function that are to
                                             be separated.
                  1
       o i1 ≥            (ΘΩ − oi2 wi2 ,Ω )         (5.22)
                wi1 ,Ω

With a constant threshold value ΘΩ , the
right part of inequation 5.22 is a straight
line through a coordinate system defined
by the possible outputs oi1 und oi2 of the
input neurons i1 and i2 (fig. 5.7).
For a (as required for inequation 5.22) pos-
itive wi2 ,Ω the output neuron Ω fires for



82                                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                dkriesel.com                                                                 5.2 Linear separability


                 n   number of           lin.              share
                     binary              separable
                     functions           ones
                 1   4                   4                 100%
                 2   16                  14                87.5%
                 3   256                 104               40.6%
                 4   65, 536             1, 772            2.7%
                 5   4.3 · 109           94, 572           0.002%
                 6   1.8 · 1019          5, 028, 134       ≈ 0%

                Table 5.2: Number of functions concerning n bi-
                nary inputs, and number and proportion of the
                functions thereof which can be linearly separated.
                In accordance with [Zel94, Wid89, Was89].


                                                                     Figure 5.8: Linear separation of n = 3 inputs
                                                                     from input neurons i1 , i2 and i3 by 2-dimensional
                                                                     plane.
                input combinations lying above the gener-
                ated straight line. For a negative wi2 ,Ω it
                would fire for all input combinations lying
                below the straight line. Note that only the
                four corners of the unit square are possi-
                ble inputs because the XOR function only
                knows binary inputs.
                                                                     Unfortunately, it seems that the percent-
                In order to solve the XOR problem, we                age of the linearly separable problems
                have to turn and move the straight line so           rapidly decreases with increasing n (see
                that input set A = {(0, 0), (1, 1)} is sepa-         table 5.2), which limits the functionality
                rated from input set B = {(0, 1), (1, 0)} –          of the SLP. Additionally, tests for linear
                                                                                                                          few tasks
                                                                                                                          are linearly
                this is, obviously, impossible.                      separability are difficult. Thus, for more           separable

                                                                     difficult tasks with more inputs we need
                Generally, the input parameters of n many            something more powerful than SLP. The
                input neurons can be represented in an n-            XOR problem itself is one of these tasks,
                dimensional cube which is separated by an            since a perceptron that is supposed to rep-
 SLP cannot
do everything   SLP through an (n−1)-dimensional hyper-              resent the XOR function already needs a
                plane (fig. 5.8). Only sets that can be sep-         hidden layer (fig. 5.9 on the next page).
                arated by such a hyperplane, i.e. which
                are linearly separable, can be classified
                by an SLP.



                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                     83
              Chapter 5 The perceptron, backpropagation and its variants                       dkriesel.com



                       GFED
                       @ABC                            GFED
                                                       @ABC
                                                                 part of fig. 5.10 on the facing page). A
                        A                                      multilayer perceptron represents an uni-
                          11 AA                        }
                            11 A                    }}
                                                      }          versal function approximator, which
                              11 1AA              }1             is proven by the Theorem of Cybenko
                                         @ABC
                                  111 GFED
                                      A
                                11 A ~}}         }
                                          1.5 1                  [Cyb89].
                                     11
                                       11                        Another trainable weight layer proceeds
                                         11−2
                                         @ABC
                                         GFED
                                                              analogously, now with the convex poly-
                                          0.5                    gons. Those can be added, subtracted or
                                                                 somehow processed with other operations
                                                                (lower part of fig. 5.10 on the next page).
                                      XOR
                                                             Generally, it can be mathematically
                                                             proven that even a multilayer perceptron
              Figure 5.9: Neural network realizing the XOR with one layer of hidden neurons can ar-
              function. Threshold values (as far as they are bitrarily precisely approximate functions
              existing) are located within the neurons.
                                                             with only finitely many discontinuities as
                                                             well as their first derivatives. Unfortu-
                                                             nately, this proof is not constructive and
                                                             therefore it is left to us to find the correct
              5.3 A multilayer perceptron                    number of neurons and weights.
                    contains more trainable               In the following we want to use a
                    weight layers                         widespread abbreviated form for different
                                                          multilayer perceptrons: We denote a two-
                                                          stage perceptron with 5 neurons in the in-
              A perceptron with two or more trainable
                                                          put layer, 3 neurons in the hidden layer
              weight layers (called multilayer perceptron
                                                          and 4 neurons in the output layer as a 5-
              or MLP) is more powerful than an SLP. As
                                                          3-4-MLP.
              we know, a singlelayer perceptron can di-
              vide the input space by means of a hyper- Definition 5.7 (Multilayer perceptron).
              plane (in a two-dimensional input space Perceptrons with more than one layer of
              by means of a straight line). A two- variably weighted connections are referred
              stage perceptron (two trainable weight lay- to as multilayer perceptrons (MLP).
more planes
              ers, three neuron layers) can classify con- An n-layer or n-stage perceptron has
              vex polygons by further processing these thereby exactly n variable weight layers
              straight lines, e.g. in the form "recognize and n + 1 neuron layers (the retina is dis-
              patterns lying above straight line 1, be- regarded here) with neuron layer 1 being
              low straight line 2 and below straight line the input layer.
              3". Thus, we – metaphorically speaking
              - took an SLP with several output neu- Since three-stage perceptrons can classify
              rons and "attached" another SLP (upper sets of any form by combining and sepa-                   3-stage
                                                                                                               MLP is
                                                                                                               sufficient

              84                            D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                         5.3 The multilayer perceptron


                                           @ABC
                                           GFED
                                            i1 UU                     @ABC
                                                                    jGFED
                                                                        i2
                                           @@@UUUUUUjUjjjjjj @@@
                                               @@jjjj UUUU                @@
                                       jjjjjjj@@@              UUUUUUU @@@
                            GFED
                            @ABC                      GFED
                                                      @ABC                           @ABC
                                                                                     GFED
                                                    @                    UUUU @
                                tjjjjjj                                     U*
                             h1 PP                       h2                       o h3
                                      PPP                                     oo o
                                          PPP                              ooo
                                             PPP                       ooo
                                                PPP                 oo
                                                        89:;
                                                      ' ?>=<
                                                   PPP  oooo
                                                             wo
                                                         Ω

                                                                  

                          @ABC
                          GFED
                            i1 @                                                             GFED
                                                                                             @ABC
                                                                                              i2 @
                         ~~     @ @@                                                      ~ ~      @@
                       ~~           @@                                                  ~~           @@
                     ~~               @                                               ~~                @@

            GFED
            @ABC          @ABC
                          GFED            @ABC
                                          GFED                            @ABC
                                                                          GFED               @ABC
                                                                                           ) GFED            @ABC
                                                                                                           * GFED
                    ~                  @@                                           ~~                    @@
                ~~~t          
                                u              w                        '       ~ ~            
             h1 PP          h2 @           h3                               h4                h5          n   h6
                     PPP        @                                                           ~         nn n
                         PPP      @@                                                      ~         n
                             PPP @@                                                     ~~       nnn
                                PPP @@                                                ~~ nnnnn
                                          @ABC
                                        ' GFED                            @ABC
                                                                     -*, GFED
                                     PPP@                                    ~~~nnn~
                                                 t                                nw
                                           h7 @rq                           h8
                                               @@                         ~
                                                   @@             ~~~
                                                     @@          ~
                                                               ~~
                                                         89:;
                                                         ?>=<
                                                       @@
                                                          ~ ~
                                                          Ω

                                                              




Figure 5.10: We know that an SLP represents a straight line. With 2 trainable weight layers,
several straight lines can be combined to form convex polygons (above). By using 3 trainable
weight layers several polygons can be formed into arbitrary sets (below).




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                     85
Chapter 5 The perceptron, backpropagation and its variants                              dkriesel.com


     n     classifiable sets                        5.4 Backpropagation of error
     1     hyperplane
                                                        generalizes the delta rule
     2     convex polygon
     3     any set                                      to allow for MLP training
     4     any set as well, i.e. no
           advantage
                                                    Next, I want to derive and explain the
Table 5.3: Representation of which perceptron       backpropagation of error learning rule
can classify which types of sets with n being the   (abbreviated: backpropagation, backprop
number of trainable weight layers.                  or BP), which can be used to train multi-
                                                    stage perceptrons with semi-linear 3 activa-
                                                    tion functions. Binary threshold functions
                                                    and other non-differentiable functions are
                                                    no longer supported, but that doesn’t mat-
                                                    ter: We have seen that the Fermi func-
                                                    tion or the hyperbolic tangent can arbi-
                                                    trarily approximate the binary threshold
rating arbitrarily many convex polygons,            function by means of a temperature pa-
another step will not be advantageous               rameter T . To a large extent I will fol-
with respect to function representations.           low the derivation according to [Zel94] and
Be cautious when reading the literature:            [MR86]. Once again I want to point out
There are many different definitions of             that this procedure had previously been
what is counted as a layer. Some sources            published by Paul Werbos in [Wer74]
count the neuron layers, some count the             but had consideraby less readers than in
weight layers. Some sources include the             [MR86].
retina, some the trainable weight layers.
Some exclude (for some reason) the out-             Backpropagation is a gradient descent pro-
put neuron layer. In this work, I chose             cedure (including all strengths and weak-
the definition that provides, in my opinion,        nesses of the gradient descent) with the
the most information about the learning             error function Err(W ) receiving all n
capabilities – and I will use it cosistently.       weights as arguments (fig. 5.5 on page 78)
Remember: An n-stage perceptron has ex-             and assigning them to the output error, i.e.
actly n trainable weight layers. You can            being n-dimensional. On Err(W ) a point
find a summary of which perceptrons can             of small error or even a point of the small-
classify which types of sets in table 5.3.          est error is sought by means of the gradi-
We now want to face the challenge of train-         ent descent. Thus, in analogy to the delta
ing perceptrons with more than one weight           rule, backpropagation trains the weights
layer.                                              of the neural network. And it is exactly

                                                    3 Semilinear functions are monotonous and differen-
                                                      tiable – but generally they are not linear.




86                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
           dkriesel.com                                                       5.4 Backpropagation of error


           the delta rule or its variable δi for a neu-
           ron i which is expanded from one trainable
           weight layer to several ones by backpropa-
                                                     ()*+L
                                                     /.-,        ()*+=
                                                                 /.-,            ()*+
                                                                                 /.-,                     ?>=<
                                                                                                          89:;
           gation.
                                                            LLL                                ...          k K
                                                               LLL ===
                                                                      =
                                                                                                     ppppp
                                                                   LLL ==                          pp
                                                                                              wk,hp
           5.4.1 The derivation is similar to                          LLL ==               pp
                                                                               HIJK
                                                                               ONML
                                                                           LL=&         pp
                  the one of the delta rule, but                                  Σ
                                                                                      wppp
                                                                            r fact NNNNN
                                                                                               h               H
                  with a generalized delta                                rr                N
                                                                       rr                  N
                                                                rr rrr                     wh,lN
                                                                                                   NNN
                                                     ()*+
                                                     /.-,        ()*+
                                                                 /.-,            ()*+
                                                                                 /.-,                     ?>=<
                                                                                                          89:;
                                                               r                                    NNN
           Let us define in advance that the network     x rrrr                                       N'
           input of the individual neurons i results                                           ...          l  L
           from the weighted sum. Furthermore, as
           with the derivation of the delta rule, let
           op,i , netp,i etc. be defined as the already
           familiar oi , neti , etc. under the input pat-
           tern p we used for the training. Let the           Figure 5.11: Illustration of the position of our
           output function be the identity again, thus        neuron h within the neural network. It is lying in
                                                              layer H, the preceding layer is K, the subsequent
           oi = fact (netp,i ) holds for any neuron i.
                                                              layer is L.
           Since this is a generalization of the delta
           rule, we use the same formula framework
           as with the delta rule (equation 5.20 on
general-
 ization   page 81). As already indicated, we have
    of δ   to generalize the variable δ for every neu-     differences are, as already mentioned, in
           ron.                                            the generalized δ). We initially derive the
                                                           error function Err according to a weight
           First of all: Where is the neuron for which w .
                                                             k,h
           we want to calculate δ? It is obvious to
           select an arbitrary inner neuron h having             ∂Err(wk,h )    ∂Err ∂neth
                                                                             =         ·         (5.23)
           a set K of predecessor neurons k as well                ∂wk,h       ∂net ∂wk,h
                                                                               | {z h}
           as a set of L successor neurons l, which                             =−δh
           are also inner neurons (see fig. 5.11). It
           is therefore irrelevant whether the prede-
                                                           The first factor of equation 5.23 is −δh ,
           cessor neurons are already the input neu-
                                                           which we will deal with later in this text.
           rons.
                                                           The numerator of the second factor of the
           Now we perform the same derivation as equation includes the network input, i.e.
           for the delta rule and split functions by the weighted sum is included in the numer-
           means the chain rule. I will not discuss ator so that we can immediately derive it.
           this derivation in great detail, but the prin- Again, all summands of the sum drop out
           cipal is similar to that of the delta rule (the apart from the summand containing wk,h .



           D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                   87
Chapter 5 The perceptron, backpropagation and its variants                     dkriesel.com


This summand is referred to as wk,h · ok . If According to the definition of the multi-
we calculate the derivative, the output of dimensional chain rule, we immediately ob-
neuron k becomes:                             tain equation 5.31:
      ∂neth   ∂           wk,h ok
                  P
                                                     ∂Err X      ∂Err ∂netl
                                                                                   
            =       k∈K
                                      (5.24)     −       =     −      ·                 (5.31)
      ∂wk,h           ∂wk,h                          ∂oh         ∂netl ∂oh
                                                           l∈L
             = ok                     (5.25)

                                               The sum in equation 5.31 contains two fac-
As promised, we will now discuss the −δh       tors. Now we want to discuss these factors
of equation 5.23 on the previous page,         being added over the subsequent layer L.
which is split up again according of the       We simply calculate the second factor in
chain rule:                                    the following equation 5.33:
                 ∂Err
           δh = −                                     ∂netl   ∂ h∈H wh,l · oh
                                                                 P
                                      (5.26)                =                           (5.32)
                 ∂neth                                 ∂oh         ∂oh
                 ∂Err ∂oh                                   = wh,l
              =−      ·               (5.27)                                            (5.33)
                 ∂oh ∂neth


The derivation of the output according to The same applies for the first factor accord-
the network input (the second factor in ing to the definition of our δ:
equation 5.27) clearly equals the deriva-
                                                          ∂Err
tion of the activation function according              −         = δl             (5.34)
                                                         ∂netl
to the network input:

            ∂oh    ∂fact (neth )               Now we insert:
                 =                    (5.28)
           ∂neth     ∂neth
                                                                ∂Err X
                  = fact 0 (neth )    (5.29)             ⇒−         =     δl wh,l       (5.35)
                                                                ∂oh   l∈L
Consider this an important passage! We
now analogously derive the first factor in     You can find a graphic version of the δ
equation 5.27. Therefore, we have to point     generalization including all splittings in
out that the derivation of the error func-     fig. 5.12 on the facing page.
tion according to the output of an inner
neuron layer depends on the vector of all        The reader might already have noticed
network inputs of the next following layer.      that some intermediate results were shown
This is reflected in equation 5.30:              in frames. Exactly those intermediate re-
                                                 sults were highlighted in that way, which
           ∂Err(netl1 , . . . , netl|L| )
                                          (5.30) are a factor in the change in weight of
   ∂Err
 −      =−
   ∂oh              ∂oh                          wk,h . If the aforementioned equations are



88                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                      5.4 Backpropagation of error




                                      δh


                                      ∂Err
                                    − ∂net h



                                                      
                     ∂oh
                    ∂neth                          − ∂Err
                                                     ∂oh


                                                                           
                  0 (net )
                 fact                 ∂Err
                                    − ∂net
                                                                     P   ∂netl
                        h                 l                          l∈L ∂oh


                                                                     P
                                                                 ∂          wh,l ·oh
                                       δl                                h∈H
                                                                          ∂oh




                                                                         wh,l




Figure 5.12: Graphical representation of the equations (by equal signs) and chain rule splittings
(by arrows) in the framework of the backpropagation derivation. The leaves of the tree reflect the
final results from the generalization of δ, which are framed in the derivation.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                89
Chapter 5 The perceptron, backpropagation and its variants                                            dkriesel.com


combined with the highlighted intermedi-                             . the difference between teaching
ate results, the outcome of this will be the                           input tp,h and output yp,h of the
wanted change in weight ∆wk,h to                                       successor neuron h.
                                                                                                                        Teach. Input
      ∆wk,h = ηok δh with
                                                                                                                        changed for
                                                       (5.36)     In this case, backpropagation is work-                the outer
                                                                  ing on two neuron layers, the output
          δh =        (neth )             (δl wh,l )
                  0
                                    X                                                                                   weight layer
                 fact           ·
                                    l∈L
                                                                  layer with the successor neuron h and
                                                                  the preceding layer with the predeces-
– of course only in case of h being an inner                      sor neuron k.
neuron (otherweise there would not be a
subsequent layer L).                                            2. If h is an inner, hidden neuron, then

The case of h being an output neuron has                             δp,h = fact
                                                                             0
                                                                                 (netp,h ) ·         (δp,l · wh,l )
                                                                                          X

already been discussed during the deriva-                                                      l∈L
tion of the delta rule. All in all, the re-                                                                    (5.39)
sult is the generalization of the delta rule,                     holds. I want to explicitly mention
called backpropagation of error:                                  that backpropagation is now working
                                                                                                                        back-
                                                                                                                        propagation
 ∆wk,h(= ηok δh with                                              on three layers. Here, neuron k is                    for inner

                                                                  the predecessor of the connection to
                                                                                                                        layers
         0 (net ) · (t − y ) (h outside)
        fact
 δh =           h
         0 (net ) · P
                      h     h
                                                                  be changed with the weight wk,h , the
        fact    h     l∈L (δl wh,l ) (h inside)
                                                                  neuron h is the successor of the con-
                                                       (5.37)
                                                                  nection to be changed and the neu-
In contrast to the delta rule, δ is treated                       rons l are lying in the layer follow-
differently depending on whether h is an                          ing the successor neuron. Thus, ac-
output or an inner (i.e. hidden) neuron:                          cording to our training pattern p, the
                                                                  weight wk,h from k to h is proportion-
 1. If h is an output neuron, then
                                                                  ally changed according to
        δp,h = fact
                0
                    (netp,h ) · (tp,h − yp,h )
                                                                     . the learning rate η,
                                                       (5.38)
                                                                     . the output of the predecessor
     Thus, under our training pattern p
                                                                       neuron op,k ,
     the weight wk,h from k to h is changed
     proportionally according to                                     . the gradient of the activation
                                                                       function at the position of the
       . the learning rate η,
                                                                       network input of the successor
       . the output op,k of the predeces-                                       0 (net
                                                                       neuron fact    p,h ),
         sor neuron k,
                                                                     . as well as, and this is the
       . the gradient of the activation                                difference, according to the
         function at the position of the                               weighted sum of the changes in
         network input of the successor                                weight to all neurons following h,
                  0 (net
         neuron fact     p,h ) and                                       l∈L (δp,l · wh,l ).
                                                                       P




90                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                  5.4 Backpropagation of error


Definition 5.8 (Backpropagation). If we 5.4.2 Heading back: Boiling
summarize formulas 5.38 on the preceding                   backpropagation down to
page and 5.39 on the facing page, we re-                   delta rule
ceive the following final formula for back-
propagation (the identifiers p are om- As explained above, the delta rule is a
mited for reasons of clarity):                      special case of backpropagation for one-
                                                    stage perceptrons and linear activation
 ∆wk,h(= ηok δh with                                functions – I want to briefly explain this
                                                                                                 backprop
         fact (neth ) · (th − yh ) (h outside) circumstance and develop the delta rule
           0                                                                                     expands
 δh =      0 (net ) · P                             out of backpropagation in order to aug-
                          l∈L (δl wh,l ) (h inside)
                                                                                                 delta rule
         fact     h
                                            (5.40)
                                                    ment the understanding of both rules. We
                                                    have seen that backpropagation is defined
                                                    by
   SNIPE: An online variant of backpro-
   pagation is implemented in the method         ∆wk,h(= ηok δh with
   trainBackpropagationOfError within the                0 (net ) · (t − y ) (h outside)
                                                        fact
   class NeuralNetwork.                          δh =           h
                                                         0 (net ) · P
                                                                      h    h
                                                        fact    h        (δ
                                                                      l∈L l wh,l ) (h inside)
                                                                                        (5.41)

It is obvious that backpropagation ini-        Since we only use it for one-stage percep-
tially processes the last weight layer di-     trons, the second part of backpropagation
rectly by means of the teaching input and      (light-colored) is omitted without substitu-
then works backwards from layer to layer       tion. The result is:
while considering each preceding change in
                                                      ∆wk,h = ηok δh with
weights. Thus, the teaching input leaves                                                (5.42)
                                                            0 (net ) · (t − o )
                                                      δh = fact
traces in all weight layers. Here I describe                      h      h   h

the first (delta rule) and the second part     Furthermore, we only want to use linear
of backpropagation (generalized delta rule     activation functions so that fact 0   (light-
on more layers) in one go, which may meet      colored) is constant.      As is generally
the requirements of the matter but not         known, constants can be combined, and
of the research. The first part is obvious,    therefore we directly merge the constant
which you will soon see in the framework       derivative fact
                                                            0  and (being constant for at
of a mathematical gimmick. Decades of          least one lerning cycle) the learning rate η
development time and work lie between the      (also light-colored) in η. Thus, the result
first and the second, recursive part. Like     is:
many groundbreaking inventions, it was
not until its development that it was recog-      ∆wk,h = ηok δh = ηok · (th − oh )     (5.43)
nized how plausible this invention was.
                                               This exactly corresponds to the delta rule
                                               definition.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                            91
            Chapter 5 The perceptron, backpropagation and its variants                      dkriesel.com


            5.4.3 The selection of the learning             5.4.3.1 Variation of the learning rate
                  rate has heavy influence on                       over time
                  the learning process
                                                            During training, another stylistic device
            In the meantime we have often seen that         can be a variable learning rate: In the
            the change in weight is, in any case, pro-      beginning, a large learning rate leads to
            portional to the learning rate η. Thus, the     good results, but later it results in inac-
            selection of η is crucial for the behaviour     curate learning. A smaller learning rate
            of backpropagation and for learning proce-      is more time-consuming, but the result is
            dures in general.                               more precise. Thus, during the learning
                                                            process the learning rate needs to be de-
how fast
  will be
learned?    Definition 5.9 (Learning rate). Speed           creased by one order of magnitude once or
            and accuracy of a learning procedure can        repeatedly.
            always be controlled by and are always pro-
            portional to a learning rate which is writ- A common error (which also seems to be a
            ten as η.                                   very neat solution at first glance) is to con-
   ηI
                                                        tinually decrease the learning rate. Here
                                                        it quickly happens that the descent of the
            If the value of the chosen η is too large, learning rate is larger than the ascent of
            the jumps on the error surface are also a hill of the error function we are climb-
            too large and, for example, narrow valleys ing. The result is that we simply get stuck
            could simply be jumped over. Addition- at this ascent. Solution: Rather reduce
            ally, the movements across the error sur- the learning rate gradually as mentioned
            face would be very uncontrolled. Thus, a above.
            small η is the desired input, which, how-
            ever, can cost a huge, often unacceptable
            amount of time. Experience shows that
            good learning rate values are in the range 5.4.3.2 Different layers – Different
            of                                                   learning rates

                          0.01 ≤ η ≤ 0.9.
                                                            The farer we move away from the out-
            The selection of η significantly depends on     put layer during the learning process, the
            the problem, the network and the training       slower backpropagation is learning. Thus,
            data, so that it is barely possible to give     it is a good idea to select a larger learning
            practical advise. But for instance it is pop-   rate for the weight layers close to the in-
            ular to start with a relatively large η, e.g.   put layer than for the weight layers close
            0.9, and to slowly decrease it down to 0.1.     to the output layer.
            For simpler problems η can often be kept
            constant.



            92                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                dkriesel.com                                                5.5 Resilient backpropagation


                5.5 Resilient backpropagation                      are adapted for each time step of
                    is an extension to                             Rprop. To account for the temporal
                                                                   change, we have to correctly call it
                    backpropagation of error                       ηi,j (t). This not only enables more
                                                                   focused learning, also the problem of
                We have just raised two backpropagation-           an increasingly slowed down learning
                specific properties that can occasionally be       throughout the layers is solved in an
                a problem (in addition to those which are          elegant way.
                already caused by gradient descent itself):
                On the one hand, users of backpropaga-
                tion can choose a bad learning rate. On Weight change: When using backpropa-
                the other hand, the further the weights are       gation, weights are changed propor-
                from the output layer, the slower backpro-        tionally to the gradient of the error
                pagation learns. For this reason, Mar-            function. At first glance, this is really
                tin Riedmiller et al. enhanced back-              intuitive. However, we incorporate ev-
                propagation and called their version re-          ery jagged feature of the error surface
                silient backpropagation (short Rprop)             into the weight changes. It is at least
                [RB93, Rie94]. I want to compare back-            questionable, whether this is always
                propagation and Rprop, without explic-            useful. Here, Rprop takes other ways
                itly declaring one version superior to the        as well: the amount of weight change
                other. Before actually dealing with formu-        ∆wi,j simply directly corresponds to
                las, let us informally compare the two pri-       the automatically adjusted learning
                mary ideas behind Rprop (and their con-           rate ηi,j . Thus the change in weight is
                sequences) to the already familiar backpro-       not proportional to the gradient, it is
                pagation.                                         only influenced by the sign of the gra-
                                                                  dient. Until now we still do not know
                Learning rates: Backpropagation uses by           how exactly the ηi,j are adapted at
                      default a learning rate η, which is se-     run time, but let me anticipate that
                      lected by the user, and applies to the      the resulting process looks consider-
                                                                                                              Much
                      entire network. It remains static un-       ably less rugged than an error func-        smoother learning
                      til it is manually changed. We have         tion.
                      already explored the disadvantages of
                      this approach. Here, Rprop pursues a
                      completely different approach: there In contrast to backprop the weight update
                      is no global learning rate. First, each step is replaced and an additional step
                      weight wi,j has its own learning rate for the adjustment of the learning rate is
                      ηi,j , and second, these learning rates added. Now how exactly are these ideas
One learning-
     rate per
      weight          are not chosen by the user, but are au- being implemented?
     ηi,j I         tomatically set by Rprop itself. Third,
   automatic        the weight changes are not static but
learning rate
  adjustment


                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         93
                   Chapter 5 The perceptron, backpropagation and its variants                          dkriesel.com


                   5.5.1 Weight changes are not                     Definition      5.10    (Weight change in
                         proportional to the gradient               Rprop).

                                                                                   −ηi,j (t),    if g(t) > 0
                                                                                   
                                                                                   
                                                                                   
                   Let us first consider the change in weight.       ∆wi,j (t) =     +ηi,j (t),   if g(t) < 0 (5.44)
                   We have already noticed that the weight-                        0             otherwise.
                                                                                   
                                                                                   
                   specific learning rates directly serve as ab-
                   solute values for the changes of the re-
                                                                    We now know how the weights are changed
                   spective weights. There remains the ques-
                                                                    – now remains the question how the learn-
                   tion of where the sign comes from – this
                                                                    ing rates are adjusted. Finally, once we
                   is a point at which the gradient comes
                                                                    have understood the overall system, we
                   into play. As with the derivation of back-
                                                                    will deal with the remaining details like ini-
                   propagation, we derive the error function
                                                                    tialization and some specific constants.
                   Err(W ) by the individual weights wi,j and
                                             )
                   obtain gradients ∂Err(W
                                        ∂wi,j . Now, the big
                   difference: rather than multiplicatively         5.5.2 Many dynamically adjusted
                   incorporating the absolute value of the                learning rates instead of one
                   gradient into the weight change, we con-               static
                   sider only the sign of the gradient. The
                   gradient hence no longer determines the          To adjust the learning rate ηi,j , we again
        gradient
determines only    strength, but only the direction of the          have to consider the associated gradients
direction of the   weight change.                                   g of two time steps: the gradient that has
        updates
                                                                    just passed (t − 1) and the current one
                   If the sign of the gradient ∂Err(W   )
                                                          is pos-   (t). Again, only the sign of the gradient
                                                  ∂wi,j
                                                                    matters, and we now must ask ourselves:
                   itive, we must decrease the weight wi,j .
                                                                    What can happen to the sign over two time
                   So the weight is reduced by ηi,j . If the
                                                                    steps? It can stay the same, and it can
                   sign of the gradient is negative, the weight
                                                                    flip.
                   needs to be increased. So ηi,j is added to
                   it. If the gradient is exactly 0, nothing        If the sign changes from g(t − 1) to g(t),
                   happens at all. Let us now create a for-         we have skipped a local minimum in the
                   mula from this colloquial description. The       gradient. Hence, the last update was too
                   corresponding terms are affixed with a (t)       large and ηi,j (t) has to be reduced as com-
                   to show that everything happens at the           pared to the previous ηi,j (t − 1). One can
                   same time step. This might decrease clar-        say, that the search needs to be more accu-
                   ity at first glance, but is nevertheless im-     rate. In mathematical terms, we obtain a
                   portant because we will soon look at an-         new ηi,j (t) by multiplying the old ηi,j (t−1)
                   other formula that operates on different         with a constant η ↓ , which is between 1 and
                   time steps. Instead, we shorten the gra-         0. In this case we know that in the last           Jη↓
                                         )
                   dient to: g = ∂Err(W
                                    ∂wi,j .                         time step (t − 1) something went wrong –



                   94                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                                                    5.5 Resilient backpropagation


              hence we additionally reset the weight up- 5.5.3 We are still missing a few
              date for the weight wi,j at time step (t) to         details to use Rprop in
              0, so that it not applied at all (not shown          practice
              in the following formula).
                                                            A few minor issues remain unanswered,
                                                            namely
              However, if the sign remains the same, one
              can perform a (careful!) increase of ηi,j to   1. How large are η ↑ and η ↓ (i.e. how
              get past shallow areas of the error function.     much are learning rates reinforced or
              Here we obtain our new ηi,j (t) by multiply-      weakened)?
              ing the old ηi,j (t − 1) with a constant η ↑
    η↑I                                                      2. How to choose ηi,j (0) (i.e. how are
              which is greater than 1.
                                                                the weight-specific learning rates ini-
                                                                tialized)?4
              Definition 5.11 (Adaptation of learning
              rates in Rprop).                               3. What are the upper and lower bounds
                                                                 ηmin and ηmax for ηi,j set?
                                                                                                                      Jηmin
                       η ηi,j (t − 1),  g(t − 1)g(t) > 0     We now answer these questions with a
                       
                         ↑
                       
                                                                                                                     Jηmax
              ηi,j (t) = η ηi,j (t − 1), g(t − 1)g(t) < 0
                          ↓                                   quick motivation. The initial value for the
                        η (t − 1)                            learning rates should be somewhere in the
                                         otherwise.
                        
                        
                                                              order of the initialization of the weights.
                          i,j
                                                     (5.45)
                                                              ηi,j (0) = 0.1 has proven to be a good
                                                              choice. The authors of the Rprop paper
                                                              explain in an obvious way that this value
                                                              – as long as it is positive and without an ex-
              Caution: This also implies that Rprop is
Rprop only                                                    orbitantly high absolute value – does not
              exclusively designed for offline. If the gra-
    learns
                                                              need to be dealt with very critically, as
    offline   dients do not have a certain continuity, the
                                                              it will be quickly overridden by the auto-
              learning process slows down to the lowest
                                                              matic adaptation anyway.
              rates (and remains there). When learning
              online, one changes – loosely speaking –        Equally uncritical is ηmax , for which they
              the error function with each new epoch,         recommend, without further mathemati-
              since it is based on only one training pat-     cal justification, a value of 50 which is used
              tern. This may be often well applicable         throughout most of the literature. One
              in backpropagation and it is very often         can set this parameter to lower values in
              even faster than the offline version, which     order to allow only very cautious updates.
              is why it is used there frequently. It lacks,   Small update steps should be allowed in
              however, a clear mathematical motivation,       any case, so we set ηmin = 10−6 .
              and that is exactly what we need here.
                                                              4 Protipp: since the ηi,j can be changed only by
                                                                multiplication, 0 would be a rather suboptimal ini-
                                                                tialization :-)




              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                  95
                 Chapter 5 The perceptron, backpropagation and its variants                            dkriesel.com


                 Now we have left only the parameters η ↑             SNIPE: In Snipe resilient backpropa-
                 and η ↓ . Let us start with η ↓ : If this value      gation is supported via the method
                 is used, we have skipped a minimum, from             trainResilientBackpropagation of the
                 which we do not know where exactly it lies           class NeuralNetwork. Furthermore, you
                                                                      can also use an additional improvement
                 on the skipped track. Analogous to the               to resilient propagation, which is, however,
                 procedure of binary search, where the tar-           not dealt with in this work. There are get-
                 get object is often skipped as well, we as-          ters and setters for the different parameters
                 sume it was in the middle of the skipped             of Rprop.
                 track. So we need to halve the learning
                 rate, which is why the canonical choice
                 η ↓ = 0.5 is being selected. If the value
                 of η ↑ is used, learning rates shall be in-
                                                                   5.6 Backpropagation has
                 creased with caution. Here we cannot gen-             often been extended and
                 eralize the principle of binary search and            altered besides Rprop
                 simply use the value 2.0, otherwise the
                 learning rate update will end up consist-
                 ing almost exclusively of changes in direc-       Backpropagation has often been extended.
                 tion. Independent of the particular prob-         Many of these extensions can simply be im-
                 lems, a value of η ↑ = 1.2 has proven to          plemented as optional features of backpro-
                 be promising. Slight changes of this value        pagation in order to have a larger scope for
                 have not significantly affected the rate of       testing. In the following I want to briefly
                 convergence. This fact allowed for setting        describe some of them.
                 this value as a constant as well.

                                                                   5.6.1 Adding momentum to
                 With advancing computational capabili-                  learning
                 ties of computers one can observe a more
                 and more widespread distribution of net-
                                                                   Let us assume to descent a steep slope
                 works that consist of a big number of lay-
                                                                   on skis - what prevents us from immedi-
                 ers, i.e. deep networks. For such net-
 Rprop is very                                                     ately stopping at the edge of the slope
                 works it is crucial to prefer Rprop over the
                                                                   to the plateau? Exactly - our momen-
     good for
deep networks    original backpropagation, because back-
                                                                   tum. With backpropagation the momen-
                 prop, as already indicated, learns very
                                                                   tum term [RHW86b] is responsible for the
                 slowly at weights wich are far from the
                                                                   fact that a kind of moment of inertia
                 output layer. For problems with a smaller
                                                                   (momentum) is added to every step size
                 number of layers, I would recommend test-
                                                                   (fig. 5.13 on the next page), by always
                 ing the more widespread backpropagation
                                                                   adding a fraction of the previous change
                 (with both offline and online learning) and
                                                                   to every new change in weight:
                 the less common Rprop equivalently.
                                                                   (∆p wi,j )now = ηop,i δp,j +α·(∆p wi,j )previous .



                 96                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                    5.6 Further variations and extensions to backpropagation


             Of course, this notation is only used for
             a better understanding. Generally, as al-
             ready defined by the concept of time, when
             referring to the current cycle as (t), then
             the previous cycle is identified by (t − 1),
             which is continued successively. And now
             we come to the formal definition of the mo-
             mentum term:

             Definition 5.12 (Momentum term). The
moment of
   inertia   variation of backpropagation by means of
             the momentum term is defined as fol-
             lows:

              ∆wi,j (t) = ηoi δj + α · ∆wi,j (t − 1) (5.46) Figure 5.13: We want to execute the gradient
                                                        descent like a skier crossing a slope, who would
                                                        hardly stop immediately at the edge to the
             We accelerate on plateaus (avoiding quasi- plateau.
             standstill on plateaus) and slow down on
             craggy surfaces (preventing oscillations).
             Moreover, the effect of inertia can be var-
             ied via the prefactor α, common val-
     αI
             ues are between 0.6 und 0.9. Addition-         function the derivative outside of the close
             ally, the momentum enables the positive        proximity of Θ is nearly 0. This results
             effect that our skier swings back and          in the fact that it becomes very difficult
             forth several times in a minimum, and fi-      to move neurons away from the limits of
             nally lands in the minimum. Despite its        the activation (flat spots), which could ex-   neurons
             nice one-dimensional appearance, the oth-      tremely extend the learning time. This         get stuck
             erwise very rare error of leaving good min-    problem can be dealt with by modifying
             ima unfortunately occurs more frequently       the derivative, for example by adding a
             because of the momentum term – which           constant (e.g. 0.1), which is called flat
             means that this is again no optimal solu-      spot elimination or – more colloquial –
             tion (but we are by now accustomed to          fudging.
             this condition).
                                                        It is an interesting observation, that suc-
                                                        cess has also been achieved by using deriva-
             5.6.2 Flat spot elimination prevents tives defined as constants [Fah88]. A nice
                     neurons from getting stuck         example making use of this effect is the
                                                        fast hyperbolic tangent approximation by
             It must be pointed out that with the hy- Anguita et al. introduced in section 3.2.6
             perbolic tangent as well as with the Fermi on page 37. In the outer regions of it’s (as



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         97
Chapter 5 The perceptron, backpropagation and its variants                dkriesel.com


well approximated and accelerated) deriva- 5.6.4 Weight decay: Punishment of
tive, it makes use of a small constant.          large weights

                                             The weight decay according to Paul
5.6.3 The second derivative can be           Werbos [Wer88] is a modification that ex-
      used, too                              tends the error by a term punishing large
                                             weights. So the error under weight de-
According to David Parker [Par87], cay
Second order backpropagation also us-                           ErrWD
ese the second gradient, i.e. the second
multi-dimensional derivative of the error does not only increase proportionally to
                                                                                          JErrWD
function, to obtain more precise estimates the actual error but also proportionally to
of the correct ∆wi,j . Even higher deriva- the square of the weights. As a result the
tives only rarely improve the estimations. network is keeping the weights small dur-
Thus, less training cycles are needed but ing learning.
those require much more computational ef-
                                                                    1 X
fort.                                           ErrWD = Err + β ·           (w)2 (5.47)
                                                                    2 w∈W
In general, we use further derivatives (i.e.                     |      {z
                                                                    punishment
                                                                               }
Hessian matrices, since the functions are
multidimensional) for higher order meth- This approach is inspired by nature where
ods. As expected, the procedures reduce synaptic weights cannot become infinitely
the number of learning epochs, but signifi- strong as well. Additionally, due to these
                                                                                          keep weights
cantly increase the computational effort of small weights, the error function often       small
the individual epochs. So in the end these shows weaker fluctuations, allowing easier
procedures often need more learning time and more controlled learning.
than backpropagation.
                                             The prefactor 12 again resulted from sim-
The quickpropagation learning proce- ple pragmatics. The factor β controls the
dure [Fah88] uses the second derivative of strength of punishment: Values from 0.001      Jβ
the error propagation and locally under- to 0.02 are often used here.
stands the error function to be a parabola.
We analytically determine the vertex (i.e.
the lowest point) of the said parabola and 5.6.5 Cutting networks down:
directly jump to this point. Thus, this              Pruning and Optimal Brain
learning procedure is a second-order proce-          Damage
dure. Of course, this does not work with
error surfaces that cannot locally be ap- If we have executed the weight decay long
proximated by a parabola (certainly it is enough and notice that for a neuron in
not always possible to directly say whether the input layer all successor weights are
                                                                                          prune the
this is the case).                           0 or close to 0, we can remove the neuron,   network




98                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                            5.7 Initial configuration of a multilayer perceptron


hence losing this neuron and some weights        5.7 Getting started – Initial
and thereby reduce the possibility that the          configuration of a
network will memorize. This procedure is
called pruning.                                      multilayer perceptron
Such a method to detect and delete un-
                                                 After having discussed the backpropaga-
necessary weights and neurons is referred
                                                 tion of error learning procedure and know-
to as optimal brain damage [lCDS90].
                                                 ing how to train an existing network, it
I only want to describe it briefly: The
                                                 would be useful to consider how to imple-
mean error per output neuron is composed
                                                 ment such a network.
of two competing terms. While one term,
as usual, considers the difference between
output and teaching input, the other one         5.7.1 Number of layers: Two or
tries to "press" a weight towards 0. If a
                                                       three may often do the job,
weight is strongly needed to minimize the
                                                       but more are also used
error, the first term will win. If this is not
the case, the second term will win. Neu-
rons which only have zero weights can be   Let us begin with the trivial circumstance
pruned again in the end.                   that a network should have one layer of in-
                                           put neurons and one layer of output neu-
There are many other variations of back- rons, which results in at least two layers.
prop and whole books only about this
subject, but since my aim is to offer an Additionally, we need – as we have already
overview of neural networks, I just want learned during the examination of linear
to mention the variations above as a moti- separability – at least one hidden layer of
vation to read on.                         neurons, if our problem is not linearly sep-
                                           arable (which is, as we have seen, very
For some of these extensions it is obvi- likely).
ous that they cannot only be applied to
                                           It is possible, as already mentioned, to
feedforward networks with backpropaga-
                                           mathematically prove that this MLP with
tion learning procedures.
                                           one hidden neuron layer is already capable
We have gotten to know backpropagation of approximating arbitrary functions with
                                                            5
and feedforward topology – now we have any accuracy – but it is necessary not
to learn how to build a neural network. It only to discuss the representability of a
is of course impossible to fully communi- problem by means of a perceptron but also
cate this experience in the framework of the learnability. Representability means
this work. To obtain at least some of that a perceptron can, in principle, realize
this knowledge, I now advise you to deal 5 Note: We have not indicated the number of neu-
with some of the exemplary problems from     rons in the hidden layer, we only mentioned the
4.6.                                         hypothetical possibility.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          99
Chapter 5 The perceptron, backpropagation and its variants                  dkriesel.com


a mapping - but learnability means that neurons should be used. Thus, the most
we are also able to teach it.              useful approach is to initially train with
                                           only a few neurons and to repeatedly train
In this respect, experience shows that two
                                           new networks with more neurons until the
hidden neuron layers (or three trainable
                                           result significantly improves and, particu-
weight layers) can be very useful to solve
                                           larly, the generalization performance is not
a problem, since many problems can be
                                           affected (bottom-up approach).
represented by a hidden layer but are very
difficult to learn.
One should keep in mind that any ad-           5.7.3 Selecting an activation
ditional layer generates additional sub-             function
minima of the error function in which we
can get stuck. All these things consid-        Another very important parameter for the
ered, a promising way is to try it with        way of information processing of a neural
one hidden layer at first and if that fails,   network is the selection of an activa-
retry with two layers. Only if that fails,     tion function. The activation function
one should consider more layers. However,      for input neurons is fixed to the identity
given the increasing calculation power of      function, since they do not process infor-
current computers, deep networks with          mation.
a lot of layers are also used with success.
                                            The first question to be asked is whether
                                            we actually want to use the same acti-
5.7.2 The number of neurons has             vation function in the hidden layer and
         to be tested                       in the ouput layer – no one prevents us
                                            from choosing different functions. Gener-
The number of neurons (apart from input ally, the activation function is the same for
and output layer, where the number of in- all hidden neurons as well as for the output
put and output neurons is already defined neurons respectively.
by the problem statement) principally cor-
responds to the number of free parameters For tasks of function approximation it
of the problem to be represented.           has been found reasonable to use the hy-
                                            perbolic tangent (left part of fig. 5.14 on
Since we have already discussed the net-
                                            page 102) as activation function of the hid-
work capacity with respect to memorizing
                                            den neurons, while a linear activation func-
or a too imprecise problem representation,
                                            tion is used in the output. The latter is
it is clear that our goal is to have as few
                                            absolutely necessary so that we do not gen-
free parameters as possible but as many as
                                            erate a limited output intervall. Contrary
necessary.
                                            to the input layer which uses linear acti-
But we also know that there is no stan- vation functions as well, the output layer
dard solution for the question of how many still processes information, because it has



100                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
            dkriesel.com                             5.8 The 8-3-8 encoding problem and related problems


            threshold values. However, linear activa-     range of random values could be the in-
            tion functions in the output can also cause   terval [−0.5; 0.5] not including 0 or values
            huge learning steps and jumping over good     very close to 0. This random initialization
            minima in the error surface. This can be      has a nice side effect: Chances are that
            avoided by setting the learning rate to very  the average of network inputs is close to 0,
            small values in the output layer.             a value that hits (in most activation func-
                                                          tions) the region of the greatest derivative,
            An unlimited output interval is not essen- allowing for strong learning impulses right
            tial for pattern recognition tasks6 . If from the start of learning.
            the hyperbolic tangent is used in any case,
            the output interval will be a bit larger. Un-    SNIPE: In Snipe, weights are initial-
            like with the hyperbolic tangent, with the       ized randomly (if a synapse initial-
            Fermi function (right part of fig. 5.14 on       ization is wanted).     The maximum
            the following page) it is difficult to learn     absolute weight value of a synapse
                                                             initialized at random can be set in
            something far from the threshold value           a NeuralNetworkDescriptor using the
            (where its result is close to 0). However,       method setSynapseInitialRange.
            here a lot of freedom is given for selecting
            an activation function. But generally, the
            disadvantage of sigmoid functions is the
            fact that they hardly learn something for 5.8 The 8-3-8 encoding
            values far from thei threshold value, unless
                                                                 problem and related
            the network is modified.
                                                                        problems

            5.7.4 Weights should be initialized
                                                                  The 8-3-8 encoding problem is a clas-
                  with small, randomly chosen
                                                                  sic among the multilayer perceptron test
                  values
                                                                  training problems.             In our MLP we
                                                                  have an input layer with eight neurons
            The initialization of weights is not as triv-         i1 , i2 , . . . , i8 , an output layer with eight
            ial as one might think. If they are simply            neurons Ω1 , Ω2 , . . . , Ω8 and one hidden
            initialized with 0, there will be no change           layer with three neurons. Thus, this net-
            in weights at all. If they are all initialized        work represents a function B8 → B8 . Now
            by the same value, they will all change               the training task is that an input of a value
            equally during training. The simple so-               1 into the neuron ij should lead to an out-
            lution of this problem is called symme-               put of a value 1 from the neuron Ωj (only
            try breaking, which is the initialization             one neuron should be activated, which re-
            of weights with small random values. The              sults in 8 training samples.
random
  initial
            6 Generally, pattern recognition is understood as a
weights
              special case of function approximation with a few   During the analysis of the trained network
              discrete output possibilities.                      we will see that the network with the 3



            D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                   101
Chapter 5 The perceptron, backpropagation and its variants                                                   dkriesel.com


                             Hyperbolic Tangent                          Fermi Function with Temperature Parameter
                  1                                                 1
                 0.8
                 0.6                                               0.8
                 0.4
                 0.2                                               0.6
      tanh(x)




                                                            f(x)
                  0
                −0.2                                               0.4
                −0.4
                −0.6                                               0.2
                −0.8
                 −1                                                 0
                       −4   −2       0            2   4                  −4       −2         0         2        4
                                     x                                                      x



Figure 5.14: As a reminder the illustration of the hyperbolic tangent (left) and the Fermi function
(right). The Fermi function was expanded by a temperature parameter. The original Fermi function
is thereby represented by dark colors, the temperature parameter of the modified Fermi functions
                                                1
are, ordered ascending by steepness, 12 , 15 , 10     1
                                                  and 25 .




hidden neurons represents some kind of bi-    dimensionality for encoder problems like
nary encoding and that the above map-         the above.
ping is possible (assumed training time:
≈ 104 epochs). Thus, our network is a ma- An 8-1-8 network, however, does not work,
chine in which the input is first encoded since the possibility that the output of one
and afterwards decoded again.              neuron is compensated by another one is
Analogously, we can train a 1024-10-1024 essential, and if there is only one hidden
encoding problem. But is it possible to neuron, there is certainly no compensatory
improve the efficiency of this procedure? neuron.
Could there be, for example, a 1024-9-
1024- or an 8-2-8-encoding network?
                                                          Exercises
Yes, even that is possible, since the net-
work does not depend on binary encodings:
                                                          Exercise 8. Fig. 5.4 on page 75 shows
Thus, an 8-2-8 network is sufficient for our
                                                          a small network for the boolean functions
problem. But the encoding of the network
                                                          AND and OR. Write tables with all computa-
is far more difficult to understand (fig. 5.15
                                                          tional parameters of neural networks (e.g.
on the next page) and the training of the
                                                          network input, activation etc.). Perform
networks requires a lot more time.
                                                          the calculations for the four possible in-
   SNIPE:        The      static      method              puts of the networks and write down the
   getEncoderSampleLesson in the class                    values of these variables for each input. Do
   TrainingSampleLesson allows for creating               the same for the XOR network (fig. 5.9 on
   simple training sample lessons of arbitrary
                                                          page 84).



102                              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                           5.8 The 8-3-8 encoding problem and related problems


                                                     Exercise 9.
                                                      1. List all boolean functions B3 → B1 ,
                                                          that are linearly separable and char-
                                                          acterize them exactly.
                                                      2. List those that are not linearly sepa-
                                                          rable and characterize them exactly,
                                                          too.

                                                     Exercise 10. A simple 2-1 network shall
                                                     be trained with one single pattern by
                                                     means of backpropagation of error and
                                                     η = 0.1. Verify if the error
                                                                           1
                                                               Err = Errp = (t − y)2
                                                                           2
                                                     converges and if so, at what value. How
                                                     does the error curve look like? Let the
                                                     pattern (p, t) be defined by p = (p1 , p2 ) =
                                                     (0.3, 0.7) and tΩ = 0.4. Randomly initalize
                                                     the weights in the interval [1; −1].

                                                     Exercise 11. A one-stage perceptron
                                                     with two input neurons, bias neuron
Figure 5.15: Illustration of the functionality of    and binary threshold function as activa-
8-2-8 network encoding. The marked points rep-       tion function divides the two-dimensional
resent the vectors of the inner neuron activation    space into two regions by means of a
associated to the samples. As you can see, it        straight line g. Analytically calculate a
is possible to find inner activation formations so   set of weight values for such a perceptron
that each point can be separated from the rest       so that the following set P of the 6 pat-
of the points by a straight line. The illustration
                                                     terns of the form (p1 , p2 , tΩ ) with ε  1 is
shows an exemplary separation of one point.
                                                     correctly classified.

                                                                P ={(0, 0, −1);
                                                                    (2, −1, 1);
                                                                    (7 + ε, 3 − ε, 1);
                                                                    (7 − ε, 3 + ε, −1);
                                                                    (0, −2 − ε, 1);
                                                                    (0 − ε, −2, −1)}



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                103
Chapter 5 The perceptron, backpropagation and its variants                  dkriesel.com


Exercise 12. Calculate in a comprehen-
sible way one vector ∆W of all changes in
weight by means of the backpropagation of
error procedure with η = 1. Let a 2-2-1
MLP with bias neuron be given and let the
pattern be defined by

       p = (p1 , p2 , tΩ ) = (2, 0, 0.1).

For all weights with the target Ω the ini-
tial value of the weights should be 1. For
all other weights the initial value should
be 0.5. What is conspicuous about the
changes?




104                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
Chapter 6
Radial basis functions
                  RBF networks approximate functions by stretching and compressing Gaussian
                  bells and then summing them spatially shifted. Description of their functions
                            and their learning process. Comparison with multilayer perceptrons.




According to Poggio and Girosi [PG89] 6.1 Components and
radial basis function networks (RBF net-            structure of an RBF
works) are a paradigm of neural networks,
which was developed considerably later              network
than that of perceptrons. Like percep-
trons, the RBF networks are built in layers. Initially, we want to discuss colloquially
But in this case, they have exactly three and then define some concepts concerning
layers, i.e. only one single layer of hidden RBF networks.
neurons.
                                             Output neurons: In an RBF network the
Like perceptrons, the networks have a             output neurons only contain the iden-
feedforward structure and their layers are        tity as activation function and one
completely linked. Here, the input layer          weighted sum as propagation func-
again does not participate in information         tion. Thus, they do little more than
processing.     The RBF networks are -            adding all input values and returning
like MLPs - universal function approxima-         the sum.
tors.
                                             Hidden neurons are also called RBF neu-
Despite all things in common: What is the         rons (as well as the layer in which
difference between RBF networks and per-          they are located is referred to as RBF
ceptrons? The difference lies in the infor-       layer). As propagation function, each
mation processing itself and in the compu-        hidden neuron calculates a norm that
tational rules within the neurons outside         represents the distance between the
of the input layer. So, in a moment we            input to the network and the so-called
will define a so far unknown type of neu-         position of the neuron (center). This
rons.                                             is inserted into a radial activation



                                                                                           105
                Chapter 6 Radial basis functions                                             dkriesel.com


                     function which calculates and outputs RBF output neurons. Each layer is com-
                                                                                                             3 layers,
                     the activation of the neuron.           pletely linked with the following one, short-   feedforward
                                                             cuts do not exist (fig. 6.1 on the next page)
                Definition 6.1 (RBF input neuron). Def- – it is a feedforward topology. The connec-
       input
                inition and representation is identical to tions between input layer and RBF layer
    is linear   the definition 5.1 on page 73 of the input are unweighted, i.e. they only transmit
       again    neuron.                                      the input. The connections between RBF
                Definition 6.2 (Center of an RBF neu- layer and output layer are weighted. The
                ron). The center ch of an RBF neuron original definition of an RBF network only
                h is the point in the input space where referred to an output neuron, but – in anal-
       cI
                the RBF neuron is located . In general, ogy to the perceptrons – it is apparent that
                the closer the input vector is to the center such a definition can be generalized. A
    Position
in the input
       space    vector of an RBF neuron, the higher is its bias neuron is not used in RBF networks.
                activation.                                  The set of input neurons shall be repre-
                                                             sented by I, the set of hidden neurons by
                Definition 6.3 (RBF neuron). The so- H and the set of output neurons by O.                   JI, H, O
                called RBF neurons h have a propaga-
                tion function fprop that determines the dis-
                tance between the center ch of a neuron Therefore, the inner neurons are called ra-
                and the input vector y. This distance rep- dial basis neurons because from their def-
Important!

                resents the network input. Then the net- inition follows directly that all input vec-
                work input is sent through a radial basis tors with the same distance from the cen-
                function fact which returns the activation ter of a neuron also produce the same out-
                or the output of the neuron. RBF neurons put value (fig. 6.2 on page 108).
                                              PQRS
                are represented by the symbol WVUT
                                               ||c,x||
                                                Gauß
                                                       .

                Definition 6.4 (RBF output neuron). 6.2 Information processing of
                  RBF output neurons Ω use the                     an RBF network
                weighted sum as propagation function
                fprop , and the identity as activation func-
 only sums                                                   Now the question is, what can be realized
                tion fact . They are represented by the sym-
                     HIJK
                bol ONML
                                                             by such a network and what is its purpose.
        up
                        Σ
                           .                                Let us go over the RBF network from top
                                                             to bottom: An RBF network receives the
                Definition 6.5 (RBF network). An input by means of the unweighted con-
                RBF network has exactly three layers in nections. Then the input vector is sent
                the following order: The input layer con- through a norm so that the result is a
                sisting of input neurons, the hidden layer scalar. This scalar (which, by the way, can
                (also called RBF layer) consisting of RBF only be positive due to the norm) is pro-
                neurons and the output layer consisting of cessed by a radial basis function, for exam-



                106                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                   6.2 Information processing of an RBF network




                        @ABC
                        GFED                                              GFED
                                                                          @ABC
                                                                             
                           ERVRVRVVV                                 h                                     i1 , i2 , . . . , i|I|
                      y          EE RRRVVVVV hhhhhlhlhlhlyly                        EE
                  y yy                EE RhRhRhRhVhVlVlVlVl yy                         EE
                                                                                         EE
               y y                    h hEEhh lRlRlR VVyyVVV
                                         h                                                  EE
             yy               hh   h        E l l        R  R  y
                                                             yRRR      V   VV
         y| yhhhhhhhh
                           h               llEE"                                 VVVVVV EE"
 PQRS
 WVUT                 PQRS
                      WVUT                     WVUT
                                               PQRS                    PQRS
                                                                       WVUT                     PQRS
                                                                                                WVUT
                                        l l              y
                                                         |  y      R                       VVV+
  ||c,x|| sh           ||c,x||
                                    vll          ||c,x||
                                                                    R(
                                                                          ||c,x||                ||c,x||
            V
            QV
   Gauß C QQ VVV         Gauß C QQ  Q                                                       h hm Gauß      h1 , h2 , . . . , h|H|
                    V                             Gauß C           mm                    hh  m
            CC QQQ VVVV CC QQQ{{                            C mm {                  hhh mm {
                                                                           Gauß

              CC QQQQ VVVVCVC {{ QQQQ mmmmCmCC {h{h{hhhhmhmmmm {{{
                 CC          QQQ {V{CVCVVVmmmQQQhhh{h{hCC mmm                              {
                   CC              Q{Q{Q mCmCm hVhVhVhV Q{Q{Q mCmCm                   {{{
                      HIJK
                      ONML                      HIJK
                                                ONML                     HIJK
                                                                       * ONML
                      C!           {      m
                                          Q   hh
                                               C
                                }{ mhmhmhhQhQQ( !         V
                                                           {V    m
                                                                 Q     C
                                                        }{ mmVmVVQVQVQ( !          }{{
                          Σ vm    th                Σ vm                     Σ
                                                                                                        Ω1 , Ω2 , . . . , Ω|O|


                                                                         


Figure 6.1: An exemplary RBF network with two input neurons, five hidden neurons and three
output neurons. The connections to the hidden neurons are not weighted, they only transmit the
input. Right of the illustration you can find the names of the neurons, which coincide with the
names of the MLP neurons: Input neurons are called i, hidden neurons are called h and output
neurons are called Ω. The associated sets are referred to as I, H and O.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                   107
                  Chapter 6 Radial basis functions                                                    dkriesel.com


                                                                      ging, compressing and removing Gaussian
                                                                      bells and subsequently accumulating them.
                                                                      Here, the parameters for the superposition
                                                                      of the Gaussian bells are in the weights
                                                                      of the connections between the RBF layer
                                                                      and the output layer.

                                                                      Furthermore, the network architecture of-
                                                                      fers the possibility to freely define or train
                                                                      height and width of the Gaussian bells –
                                                                      due to which the network paradigm be-
                                                                      comes even more versatile. We will get
                                                                      to know methods and approches for this
                                                                      later.


                  Figure 6.2: Let ch be the center of an RBF neu-
                  ron h. Then the activation function fact h is ra-   6.2.1 Information processing in
                  dially symmetric around ch .                              RBF neurons

                                                                      RBF neurons process information by using
                                                                      norms and radial basis functions
                  ple by a Gaussian bell (fig. 6.3 on the next
         input
                  page) .                                      At first, let us take as an example a sim-
                  The output values of the different neurons ple 1-4-1 RBF network. It is apparent
   → distance
→ Gaussian bell
       → sum      of the RBF layer or of the different Gaus- that we will receive a one-dimensional out-
                  sian bells are added within the third layer: put which can be represented as a func-
    → output

                  basically, in relation to the whole input tion (fig. 6.4 on the facing page). Ad-
                  space, Gaussian bells are added here.        ditionally, the network includes the cen-
                                                               ters c1 , c2 , . . . , c4 of the four inner neurons
                  Suppose that we have a second, a third h1 , h2 , . . . , h4 , and therefore it has Gaus-
                  and a fourth RBF neuron and therefore sian bells which are finally added within
                  four differently located centers. Each of the output neuron Ω. The network also
                  these neurons now measures another dis- possesses four values σ1 , σ2 , . . . , σ4 which
                  tance from the input to its own center influence the width of the Gaussian bells.
                  and de facto provides different values, even On the contrary, the height of the Gaus-
                  if the Gaussian bell is the same. Since sian bell is influenced by the subsequent
                  these values are finally simply accumu- weights, since the individual output val-
                  lated in the output layer, one can easily ues of the bells are multiplied by those
                  see that any surface can be shaped by drag- weights.



                  108                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                       6.2 Information processing of an RBF network




                                                                              h(r)
                                        Gaussian in 1D                                                Gaussian in 2D
           1                                                              1
                                                                        0.8
          0.8                                                           0.6
                                                                        0.4
          0.6                                                           0.2
   h(r)




                                                                          0
          0.4

                                                                            −2
          0.2                                                                                                                          2
                                                                                 −1                                                1
                                                                                              0                            0
                                                                                      x
                                                                                                  1              −1            y
           0                                                                                                −2
                −2   −1.5   −1      −0.5      0    0.5   1   1.5   2
                                              r


Figure 6.3: Two individual one- or two-dimensional Gaussian bells. In both cases σ = 0.4 holds
                                                                    p distance r to the center (0, 0)
and the centers of the Gaussian bells lie in the coordinate origin. The
is simply calculated according to the Pythagorean theorem: r = x2 + y 2 .




                                  1.4

                                  1.2

                                   1

                                  0.8

                                  0.6

                                  0.4
                            y




                                  0.2

                                   0

                                 −0.2

                                 −0.4

                                 −0.6
                                             −2          0          2                     4             6              8
                                                                        x




Figure 6.4: Four different Gaussian bells in one-dimensional space generated by means of RBF
neurons are added by an output neuron of the RBF network. The Gaussian bells have different
heights, widths and positions. Their centers c1 , c2 , . . . , c4 are located at 0, 1, 3, 4, the widths
σ1 , σ2 , . . . , σ4 at 0.4, 1, 0.2, 0.8. You can see a two-dimensional example in fig. 6.5 on the following
page.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                            109
Chapter 6 Radial basis functions                                                                                                                                       dkriesel.com

                    h(r)                                                                                h(r)
                                                 Gaussian 1                                                                        Gaussian 2
                2                                                                                   2
              1.5                                                                                 1.5
                1                                                                                   1
              0.5                                                                                 0.5
                0                                                                                   0
             −0.5                                                                                −0.5
              −1                                                                                  −1


              −2                                                                        2         −2
                       −1                                                                                                                                          2
                                                                                1                          −1                                                  1
                            x    0                                  0                                           x   0                                 0
                                        1                    −1             y                                              1                    −1         y
                                                    −2                                                                               −2
                    h(r)                                                                                h(r)
                                                 Gaussian 3                                                                        Gaussian 4
                2                                                                                   2
              1.5                                                                                 1.5
                1                                                                                   1
              0.5                                                                                 0.5
                0                                                                                   0
             −0.5                                                                                −0.5
              −1                                                                                  −1


              −2                                                                         2        −2                                                               2
                       −1                                                       1                          −1                                                  1
                            x    0                                  0                                           x   0                                 0
                                        1                    −1             y                                              1                    −1         y
                                                    −2                                                                               −2



                           PQRS
                           WVUT
                            ||c,x||            PQRS
                                               WVUT
                                                ||c,x||                        PQRS
                                                                               WVUT
                                                                                ||c,x||         WVUT
                                                                                                PQRS
                                                                                                 ||c,x||
                                        QQQ      Gauß A                                       m
                                                                                             m Gauß
                            Gauß
                                           QQQ          AA                    }}
                                                                                 Gauß
                                                                                          mmm
                                              QQQ         AA                 }          mm
                                                  QQQ
                                                      QQQ AAA              }}      mmm
                                                          QQQ AA        }}} mmmmm
                                                                HIJK
                                                                ONML
                                                             QQ(    }~ }mmm
                                                                  Σ vm
                                                                                             



                                                                                             

                                                                                    Sum of the 4 Gaussians
                           2
                        1.75
                         1.5
                        1.25
                           1
                        0.75
                         0.5
                        0.25
                           0
                       −0.25
                        −0.5
                       −0.75
                          −1




                            −2
                                 −1.5                                                                                                                          2
                                            −1                                                                                                       1.5
                                                 −0.5                                                                                           1
                                                         0                                                                            0.5
                                                                                                                               0
                                             x                0.5                                                   −0.5
                                                                        1                                      −1                           y
                                                                            1.5                   −1.5
                                                                                       2 −2




Figure 6.5: Four different Gaussian bells in two-dimensional space generated p by means of RBF
neurons are added by an output neuron of the RBF network. Once again r = x2 + y 2 applies for
the distance. The heights w, widths σ and centers c = (x, y) are: w1 = 1, σ1 = 0.4, c1 = (0.5, 0.5),
w2 = −1, σ2 = 0.6, c2 = (1.15, −1.15), w3 = 1.5, σ3 = 0.2, c3 = (−0.5, −1), w4 = 0.8, σ4 =
1.4, c4 = (−2, 0).


110                                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
       dkriesel.com                                       6.2 Information processing of an RBF network


       Since we use a norm to calculate the dis-              activation function fact , and hence the ac-
       tance between the input vector and the                 tivation functions should not be referred
       center of a neuron h, we have different                to as fact simultaneously. One solution
       choices: Often the Euclidian norm is cho-              would be to number the activation func-
       sen to calculate the distance:                         tions like fact 1 , fact 2 , . . . , fact |H| with H be-
                                                              ing the set of hidden neurons. But as a
                 rh = ||x − ch ||                     (6.1)   result the explanation would be very con-
                                                              fusing. So I simply use the name fact for
                           sX
                       =           (xi − ch,i )2      (6.2)
                             i∈I
                                                              all activation functions and regard σ and
                                                              c as variables that are defined for individ-
       Remember: The input vector was referred                ual neurons but no directly included in the
       to as x. Here, the index i runs through                activation function.
       the input neurons and thereby through the
       input vector components and the neuron                 The reader will certainly notice that in the
       center components. As we can see, the                  literature the Gaussian bell is often nor-
       Euclidean distance generates the squared               malized by a multiplicative factor. We
       differences of all vector components, adds             can, however, avoid this factor because
       them and extracts the root of the sum.                 we are multiplying anyway with the subse-
       In two-dimensional space this corresponds              quent weights and consecutive multiplica-
       to the Pythagorean theorem. From the                   tions, first by a normalization factor and
       definition of a norm directly follows that             then by the connections’ weights, would
       the distance can only be positive. Strictly            only yield different factors there. We do
       speaking, we hence only use the positive               not need this factor (especially because for
       part of the activation function. By the                our purpose the integral of the Gaussian
       way, activation functions other than the               bell must not always be 1) and therefore
       Gaussian bell are possible. Normally, func-            simply leave it out.
       tions that are monotonically decreasing
       over the interval [0; ∞] are chosen.
                                                              6.2.2 Some analytical thoughts
       Now that we know the distance rh be-             prior to the training
rh I
       tween the input vector x and the center
       ch of the RBF neuron h, this distance has The output y of an RBF output neuron
                                                               Ω
       to be passed through the activation func- Ω results from combining the functions of
       tion. Here we use, as already mentioned, an RBF neuron to
       a Gaussian bell:
                                                                  yΩ =         wh,Ω · fact (||x − ch ||) .      (6.4)
                                                                        X
                                           −r 2
                                              h
                                           2σ 2                          h∈H
                      fact (rh ) = e         h        (6.3)

       It is obvious that both the center ch and Suppose that similar to the multilayer per-
       the width σh can be seen as part of the ceptron we have a set P , that contains |P |



       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                          111
            Chapter 6 Radial basis functions                                                        dkriesel.com


            training samples (p, t). Then we obtain
            |P | functions of the form
                                                                                    T =M ·G                 (6.6)
               yΩ =         wh,Ω · fact (||p − ch ||) ,   (6.5)               −1
                                                                                   ·T =M      −1
                      X
                                                                    ⇔     M                        · M · G (6.7)
                                                                          M −1 · T = E · G
                      h∈H
                                                                    ⇔                                       (6.8)

            i.e. one function for each training sam-                ⇔     M   −1
                                                                                   · T = G,                 (6.9)
            ple.
                                                                  where
            Of course, with this effort we are aiming               . T is the vector of the teaching
            at letting the output y for all training                  inputs for all training samples,
                                                                                                                    JT
            patterns p converge to the corresponding
            teaching input t.                                       . M is the |P | × |H| matrix of
                                                                      the outputs of all |H| RBF neu-
                                                                                                                    JM
                                                                      rons to |P | samples (remember:
                                                                      |P | = |H|, the matrix is squared
            6.2.2.1 Weights can simply be                             and we can therefore attempt to
                    computed as solution of a                         invert it),
                    system of equations
                                                                    . G is the vector of the desired
                                                                      weights and
                                                                                                                    JG
            Thus, we have |P | equations. Now let us                . E is a unit matrix with the same
            assume that the widths σ1 , σ2 , . . . , σk , the         size as G.
                                                                                                                    JE
            centers c1 , c2 , . . . , ck and the training sam-
            ples p including the teaching input t are             Mathematically speaking, we can sim-
            given. We are looking for the weights wh,Ω            ply calculate the weights: In the case
            with |H| weights for one output neuron                of |P | = |H| there is exactly one RBF
            Ω. Thus, our problem can be seen as a                 neuron available per training sample.
            system of equations since the only thing              This means, that the network exactly
            we want to change at the moment are the               meets the |P | existing nodes after hav-
            weights.                                              ing calculated the weights, i.e. it per-
                                                                  forms a precise interpolation. To
            This demands a distinction of cases con-              calculate such an equation we cer-
            cerning the number of training samples |P |           tainly do not need an RBF network,
            and the number of RBF neurons |H|:                    and therefore we can proceed to the
                                                                  next case.
            |P | = |H|: If the number of RBF neurons              Exact interpolation must not be mis-
                 equals the number of patterns, i.e.              taken for the memorizing ability men-
                 |P | = |H|, the equation can be re-              tioned with the MLPs: First, we are
                 duced to a matrix multiplication                 not talking about the training of RBF
  simply
calculate
 weights

            112                        D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                              6.2 Information processing of an RBF network


    networks at the moment. Second,                 have to find the solution M of a ma-
    it could be advantageous for us and             trix multiplication
    might in fact be intended if the net-
                                                                      T = M · G.               (6.10)
    work exactly interpolates between the
    nodes.                                          The problem is that this time we can-
|P | < |H|: The system of equations is              not invert the |P | × |H| matrix M be-
     under-determined, there are more               cause it is not a square matrix (here,
     RBF neurons than training samples,                  6 |H| is true). Here, we have
                                                    |P | =
     i.e. |P | < |H|. Certainly, this case          to use the Moore-Penrose pseudo
     normally does not occur very often.            inverse M + which is defined by
                                                                                                         JM +
     In this case, there is a huge variety               M + = (M T · M )−1 · M T              (6.11)
     of solutions which we do not need in
     such detail. We can select one set of          Although the Moore-Penrose pseudo
     weights out of many obviously possi-           inverse is not the inverse of a matrix,
     ble ones.                                      it can be used similarly in this case1 .
                                                    We get equations that are very similar
|P | > |H|: But most interesting for fur-           to those in the case of |P | = |H|:
     ther discussion is the case if there
     are significantly more training sam-                             T =M ·G                  (6.12)
                                                                  +            +
     ples than RBF neurons, that means                  ⇔     M ·T =M ·M ·G                    (6.13)
     |P | > |H|. Thus, we again want                    ⇔         +
                                                              M ·T =E·G                        (6.14)
     to use the generalization capability of            ⇔         +
                                                              M ·T =G                          (6.15)
     the neural network.
                                                    Another reason for the use of the
    If we have more training samples than           Moore-Penrose pseudo inverse is the
    RBF neurons, we cannot assume that              fact that it minimizes the squared
    every training sample is exactly hit.           error (which is our goal): The esti-
    So, if we cannot exactly hit the points         mate of the vector G in equation 6.15
    and therefore cannot just interpolate           corresponds to the Gauss-Markov
    as in the aforementioned ideal case             model known from statistics, which
    with |P | = |H|, we must try to find            is used to minimize the squared error.
    a function that approximates our                In the aforementioned equations 6.11
    training set P as closely as possible:          and the following ones please do not
    As with the MLP we try to reduce                mistake the T in M T (of the trans-
    the sum of the squared error to a min-          pose of the matrix M ) for the T of
    imum.                                           the vector of all teaching inputs.
                                               1 Particularly, M + = M −1 is true if M is invertible.
    How do we continue the calculation           I do not want to go into detail of the reasons for
    in the case of |P | > |H|? As above,         these circumstances and applications of M + - they
    to solve the system of equations, we         can easily be found in literature for linear algebra.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                  113
              Chapter 6 Radial basis functions                                               dkriesel.com


              6.2.2.2 The generalization on several          and very time-consuming (matrix inver-
                      outputs is trivial and not quite       sions require a lot of computational ef-
                      computationally expensive              fort).

                                                           Furthermore, our Moore-Penrose pseudo-
              We have found a mathematically exact
                                                           inverse is, in spite of numeric stabil-
              way to directly calculate the weights.
                                                           ity, no guarantee that the output vector
              What will happen if there are several out-                                                     M + complex
                                                           corresponds to the teaching vector, be-
              put neurons, i.e. |O| > 1, with O being, as                                                    and imprecise
                                                           cause such extensive computations can be
              usual, the set of the output neurons Ω? In
                                                           prone to many inaccuracies, even though
              this case, as we have already indicated, it
                                                           the calculation is mathematically correct:
              does not change much: The additional out-
                                                           Our computers can only provide us with
              put neurons have their own set of weights
                                                           (nonetheless very good) approximations of
              while we do not change the σ and c of the
                                                           the pseudo-inverse matrices. This means
              RBF layer. Thus, in an RBF network it is
                                                           that we also get only approximations of
              easy for given σ and c to realize a lot of
                                                           the correct weights (maybe with a lot of
              output neurons since we only have to cal-
                                                           accumulated numerical errors) and there-
              culate the individual vector of weights
                                                           fore only an approximation (maybe very
                            GΩ = M + · TΩ           (6.16) rough or even unrecognizable) of the de-
                                                           sired output.
              for every new output neuron Ω, whereas         If we have enough computing power to an-
              the matrix M + , which generally requires      alytically determine a weight vector, we
              a lot of computational effort, always stays    should use it nevertheless only as an initial
inexpensive
              the same: So it is quite inexpensive – at      value for our learning process, which leads
     output   least concerning the computational com-        us to the real training methods – but oth-
  dimension   plexity – to add more output neurons.          erwise it would be boring, wouldn’t it?


              6.2.2.3 Computational effort and
                      accuracy                               6.3 Combinations of equation
                                                                 system and gradient
              For realistic problems it normally applies         strategies are useful for
              that there are considerably more training
              samples than RBF neurons, i.e. |P | 
                                                                 training
              |H|: You can, without any difficulty, use
              106 training samples, if you like. Theoreti-   Analogous to the MLP we perform a gra-
              cally, we could find the terms for the math-   dient descent to find the suitable weights
              ematically correct solution on the black-      by means of the already well known delta        retraining
              board (after a very long time), but such       rule. Here, backpropagation is unneces-         delta rule
              calculations often seem to be imprecise        sary since we only have to train one single



              114                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                                6.3 Training of RBF networks


             weight layer – which requires less comput- 6.3.1 It is not always trivial to
             ing time.                                        determine centers and widths
                                                                  of RBF neurons
             We know that the delta rule is
                                                             It is obvious that the approximation accu-
                       ∆wh,Ω = η · δΩ · oh ,          (6.17) racy of RBF networks can be increased by
                                                             adapting the widths and positions of the
             in which we now insert as follows:              Gaussian bells in the input space to the
                                                             problem that needs to be approximated.
               ∆wh,Ω = η · (tΩ − yΩ ) · fact (||p − ch ||)   There  are several methods to deal with the
                                                      (6.18) centers c and the widths σ of the Gaussian    vary
                                                             bells:                                        σ and c


                                                            Fixed selection: The centers and widths
             Here again I explicitly want to mention            can be selected in a fixed manner and
             that it is very popular to divide the train-       regardless of the training samples –
             ing into two phases by analytically com-           this is what we have assumed until
             puting a set of weights and then refining          now.
             it by training with the delta rule.
                                                            Conditional, fixed selection: Again cen-
             There is still the question whether to learn      ters and widths are selected fixedly,
             offline or online. Here, the answer is sim-       but we have previous knowledge
             ilar to the answer for the multilayer per-        about the functions to be approxi-
             ceptron: Initially, one often trains online       mated and comply with it.
  training
in phases    (faster movement across the error surface).
             Then, after having approximated the so- Adaptive to the learning process: This
             lution, the errors are once again accumu-    is definitely the most elegant variant,
             lated and, for a more precise approxima-     but certainly the most challenging
             tion, one trains offline in a third learn-   one, too.       A realization of this
             ing phase. However, similar to the MLPs,     approach will not be discussed in
             you can be successful by using many meth-    this chapter but it can be found in
             ods.                                         connection with another network
                                                          topology (section 10.6.1).
             As already indicated, in an RBF network
             not only the weights between the hidden
                                                          6.3.1.1 Fixed selection
             and the output layer can be optimized. So
             let us now take a look at the possibility to
             vary σ and c.                                In any case, the goal is to cover the in-
                                                          put space as evenly as possible. Here,
                                                          widths of 32 of the distance between the



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       115
                 Chapter 6 Radial basis functions                                                      dkriesel.com


                                                                    responsible for the fact that six- to ten-
                                                                    dimensional problems in RBF networks
                                                                    are already called "high-dimensional" (an
                                                                    MLP, for example, does not cause any
                                                                    problems here).


                                                                    6.3.1.2 Conditional, fixed selection

                                                                Suppose that our training samples are not
                                                                evenly distributed across the input space.
                                                                It then seems obvious to arrange the cen-
                                                                ters and sigmas of the RBF neurons by
                                                                means of the pattern distribution. So the
                                                                training patterns can be analyzed by statis-
                 Figure 6.6: Example for an even coverage of a tical techniques such as a cluster analysis,
                 two-dimensional input space by applying radial and so it can be determined whether there
                 basis functions.                               are statistical factors according to which
                                                                we should distribute the centers and sig-
                                                                mas (fig. 6.7 on the facing page).

                 centers can be selected so that the Gaus-               A more trivial alternative would be to
                 sian bells overlap by approx. "one third"2              set |H| centers on positions randomly se-
                 (fig. 6.6). The closer the bells are set the            lected from the set of patterns. So this
                 more precise but the more time-consuming                method would allow for every training pat-
                 the whole thing becomes.                                tern p to be directly in the center of a neu-
                                                                         ron (fig. 6.8 on the next page). This is
                 This may seem to be very inelegant, but not yet very elegant but a good solution
                 in the field of function approximation we when time is an issue. Generally, for this
                 cannot avoid even coverage. Here it is method the widths are fixedly selected.
                 useless if the function to be approximated
                 is precisely represented at some positions If we have reason to believe that the set
                 but at other positions the return value is of training samples is clustered, we can
                 only 0. However, the high input dimen- use clustering methods to determine them.
                 sion requires a great many RBF neurons, There are different methods to determine
         input
                 which increases the computational effort clusters in an arbitrarily dimensional set
    dimension    exponentially with the dimension – and is of points. We will be introduced to some
very expensive
                  2 It is apparent that a Gaussian bell is mathemati-
                                                                         of them in excursus A. One neural cluster-
                    cally infinitely wide, therefore I ask the reader to ing method are the so-called ROLFs (sec-
                    apologize this sloppy formulation.                   tion A.5), and self-organizing maps are



                 116                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                      6.3 Training of RBF networks




Figure 6.7: Example of an uneven coverage of
a two-dimensional input space, of which we
have previous knowledge, by applying radial ba-
sis functions.




also useful in connection with determin-
ing the position of RBF neurons (section
                                                  Figure 6.8: Example of an uneven coverage of
10.6.1). Using ROLFs, one can also receive
                                                  a two-dimensional input space by applying radial
indicators for useful radii of the RBF neu-       basis functions. The widths were fixedly selected,
rons. Learning vector quantisation (chap-         the centers of the neurons were randomly dis-
ter 9) has also provided good results. All        tributed throughout the training patterns. This
these methods have nothing to do with             distribution can certainly lead to slightly unrepre-
the RBF networks themselves but are only          sentative results, which can be seen at the single
used to generate some previous knowledge.         data point down to the left.
Therefore we will not discuss them in this
chapter but independently in the indicated
chapters.
Another approach is to use the approved
methods: We could slightly move the po-
sitions of the centers and observe how our
error function Err is changing – a gradient
descent, as already known from the MLPs.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                  117
Chapter 6 Radial basis functions                                             dkriesel.com


In a similar manner we could look how the In the following text, only simple mecha-
error depends on the values σ. Analogous nisms are sketched. For more information,
to the derivation of backpropagation we I refer to [Fri94].
derive

      ∂Err(σh ch )         ∂Err(σh ch )       6.4.1 Neurons are added to places
                     and                .
         ∂σh                  ∂ch                   with large error values
Since the derivation of these terms corre- After generating this initial configuration
sponds to the derivation of backpropaga- the vector of the weights G is analytically
tion we do not want to discuss it here.    calculated. Then all specific errors Errp
                                           concerning the set P of the training sam-
But experience shows that no convincing
                                           ples are calculated and the maximum spe-
results are obtained by regarding how the
                                           cific error
error behaves depending on the centers
                                                           max(Errp )
and sigmas. Even if mathematics claim                        P
that such methods are promising, the gra- is sought.
dient descent, as we already know, leads
to problems with very craggy error sur- The extension of the network is simple:
faces.                                     We replace this maximum error with a new
                                                                                             replace
                                           RBF neuron. Of course, we have to exer-           error with
And that is the crucial point: Naturally, cise care in doing this: IF the σ are small,       neuron

RBF networks generate very craggy er- the neurons will only influence each other
ror surfaces because, if we considerably if the distance between them is short. But
change a c or a σ, we will significantly if the σ are large, the already exisiting
change the appearance of the error func- neurons are considerably influenced by the
tion.                                      new neuron because of the overlapping of
                                           the Gaussian bells.
                                              So it is obvious that we will adjust the al-
6.4 Growing RBF networks                      ready existing RBF neurons when adding
                                              the new neuron.
    automatically adjust the
    neuron density                            To put it simply, this adjustment is made
                                              by moving the centers c of the other neu-
                                              rons away from the new neuron and re-
In growing RBF networks, the number           ducing their width σ a bit. Then the
|H| of RBF neurons is not constant. A         current output vector y of the network is
certain number |H| of neurons as well as      compared to the teaching input t and the
their centers ch and widths σh are previ-     weight vector G is improved by means of
ously selected (e.g. by means of a cluster-   training. Subsequently, a new neuron can
ing method) and then extended or reduced.     be inserted if necessary. This method is



118                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
               dkriesel.com                   6.5 Comparing RBF networks and multilayer perceptrons


               particularly suited for function approxima- two paradigms and look at their advan-
               tions.                                      tages and disadvantages.


               6.4.2 Limiting the number of                6.5 Comparing RBF networks
                     neurons                                   and multilayer
                                                               perceptrons
               Here it is mandatory to see that the net-
               work will not grow ad infinitum, which can
               happen very fast. Thus, it is very useful We will compare multilayer perceptrons
               to previously define a maximum number and RBF networks with respect to differ-
               for neurons |H|max .                        ent aspects.
                                                           Input dimension: We must be careful
                                                                with RBF networks in high-
               6.4.3 Less important neurons are                 dimensional functional spaces since
                       deleted                                  the network could very quickly
                                                                require huge memory storage and
               Which leads to the question whether it           computational effort.       Here, a
               is possible to continue learning when this       multilayer   perceptron would    cause
               limit |H|max is reached. The answer is:          less problems because its number of
               this would not stop learning. We only have       neuons does not grow exponentially
               to look for the "most unimportant" neuron        with  the input dimension.
               and delete it. A neuron is, for example,
                                                           Center selection: However, selecting the
               unimportant for the network if there is an-
                                                                centers c for RBF networks is (despite
               other neuron that has a similar function:
                                                                the introduced approaches) still a ma-
               It often occurs that two Gaussian bells ex-
                                                                jor problem. Please use any previous
               actly overlap and at such a position, for
      delete                                                    knowledge you have when applying
               instance, one single neuron with a higher
unimportant
                                                                them. Such problems do not occur
    neurons    Gaussian bell would be appropriate.
                                                                with the MLP.
               But to develop automated procedures in Output dimension: The advantage of
               order to find less relevant neurons is highly RBF networks is that the training is
               problem dependent and we want to leave        not much influenced when the output
               this to the programmer.                       dimension of the network is high.
                                                             For an MLP, a learning procedure
               With RBF networks and multilayer per-
                                                             such as backpropagation thereby will
               ceptrons we have already become ac-
                                                             be very time-consuming.
               quainted with and extensivley discussed
               two network paradigms for similar prob- Extrapolation: Advantage as well as dis-
               lems. Therefore we want to compare these      advantage of RBF networks is the lack



               D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                    119
             Chapter 6 Radial basis functions                                             dkriesel.com


                   of extrapolation capability: An RBF     Exercises
                   network returns the result 0 far away
                   from the centers of the RBF layer. On   Exercise 13. An |I|-|H|-|O| RBF net-
                   the one hand it does not extrapolate,   work with fixed widths and centers of the
                   unlike the MLP it cannot be used        neurons should approximate a target func-
                   for extrapolation (whereby we could     tion u. For this, |P | training samples of
                   never know if the extrapolated values   the form (p, t) of the function u are given.
                   of the MLP are reasonable, but expe-    Let |P | > |H| be true. The weights should
                   rience shows that MLPs are suitable     be analytically determined by means of
                   for that matter). On the other hand,    the Moore-Penrose pseudo inverse. Indi-
                   unlike the MLP the network is capa-     cate the running time behavior regarding
Important!
                   ble to use this 0 to tell us "I don’t   |P | and |O| as precisely as possible.
                   know", which could be an advantage.
                                                           Note: There are methods for matrix mul-
                                                           tiplications and matrix inversions that are
             Lesion tolerance: For the output of an        more efficient than the canonical methods.
                 MLP, it is no so important if a weight    For better estimations, I recommend to
                 or a neuron is missing. It will only      look for such methods (and their complex-
                 worsen a little in total. If a weight     ity). In addition to your complexity calcu-
                 or a neuron is missing in an RBF net-     lations, please indicate the used methods
                 work then large parts of the output       together with their complexity.
                 remain practically uninfluenced. But
                 one part of the output is heavily af-
                 fected because a Gaussian bell is di-
                 rectly missing. Thus, we can choose
                 between a strong local error for lesion
                 and a weak but global error.


             Spread: Here the MLP is "advantaged"
                 since RBF networks are used consid-
                 erably less often – which is not always
                 understood by professionals (at least
                 as far as low-dimensional input spaces
                 are concerned). The MLPs seem to
                 have a considerably longer tradition
                 and they are working too good to take
                 the effort to read some pages of this
                 work about RBF networks) :-).




             120                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
               Chapter 7
               Recurrent perceptron-like networks
                                                        Some thoughts about networks with internal states.




               Generally, recurrent networks are net-        be structured and how network-internal
               works that are capable of influencing them-   states can be generated. Thus, I will
               selves by means of recurrences, e.g. by       briefly introduce two paradigms of recur-
               including the network output in the follow-   rent networks and afterwards roughly out-
               ing computation steps. There are many         line their training.
               types of recurrent networks of nearly arbi-
               trary form, and nearly all of them are re-
               ferred to as recurrent neural networks.       With a recurrent network an input x that
               As a result, for the few paradigms in-        is constant over time may lead to differ-
               troduced here I use the name recurrent        ent results: On the one hand, the network       state
               multilayer perceptrons.                       could converge, i.e. it could transform it-     dynamics
                                                             self into a fixed state and at some time re-
               Apparently, such a recurrent network is ca-   turn a fixed output value y. On the other
               pable to compute more than the ordinary       hand, it could never converge, or at least
               MLP: If the recurrent weights are set to 0,   not until a long time later, so that it can
more capable
  than MLP     the recurrent network will be reduced to      no longer be recognized, and as a conse-
               an ordinary MLP. Additionally, the recur-     quence, y constantly changes.
               rence generates different network-internal
               states so that different inputs can produce
               different outputs in the context of the net- If the network does not converge, it is, for
               work state.                                  example, possible to check if periodicals
                                                            or attractors (fig. 7.1 on the following
               Recurrent networks in themselves have a page) are returned. Here, we can expect
               great dynamic that is mathematically dif- the complete variety of dynamical sys-
               ficult to conceive and has to be discussed tems. That is the reason why I particu-
               extensively. The aim of this chapter is larly want to refer to the literature con-
               only to briefly discuss how recurrences can cerning dynamical systems.



                                                                                                      121
Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)           dkriesel.com


                                              Further discussions could reveal what will
                                              happen if the input of recurrent networks
                                              is changed.

                                              In this chapter the related paradigms of
                                              recurrent networks according to Jordan
                                              and Elman will be introduced.



                                              7.1 Jordan networks

                                              A Jordan network [Jor86] is a multi-
                                              layer perceptron with a set K of so-called
                                              context neurons k1 , k2 , . . . , k|K| . There
                                              is one context neuron per output neuron
                                              (fig. 7.2 on the next page). In principle, a
                                              context neuron just memorizes an output
                                              until it can be processed in the next time       output
                                              step. Therefore, there are weighted con-         neurons
                                              nections between each output neuron and          are buffered

                                              one context neuron. The stored values are
                                              returned to the actual network by means
                                              of complete links between the context neu-
                                              rons and the input layer.

                                              In the originial definition of a Jordan net-
                                              work the context neurons are also recur-
                                              rent to themselves via a connecting weight
      Figure 7.1: The Roessler attractor      λ. But most applications omit this recur-
                                              rence since the Jordan network is already
                                              very dynamic and difficult to analyze, even
                                              without these additional recurrences.

                                              Definition 7.1 (Context neuron). A con-
                                              text neuron k receives the output value of
                                              another neuron i at a time t and then reen-
                                              ters it into the network at a time (t + 1).

                                              Definition 7.2 (Jordan network). A Jor-
                                              dan network is a multilayer perceptron



122                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                 dkriesel.com                                                                                          7.2 Elman networks




                                                @ABC
                                                GFED                                 GFED
                                                                                     @ABC                       @ABC
                                                                                                                GFED      GFED
                                                                                                                          @ABC
                                                                                       
                                                  i1 AUUUU                            i
                                                                                    i 2                          k2        k1
                                               }}         AA UUUUUiiiiiii}} AAA                                   O         O
                                              }               A     i ii  U U     }
                                                                             UU}U}U                AA
                                           }}
                                             }
                                                          i  iiAiAAi               UUUU              A
                                                     ii                        }            UUUU AAA 
                             @ABC
                             GFED                                     GFED
                                                                      @ABC                               @ABC
                                                                                                         GFED
                                          }       i                 A         }
                                 ~}x v ti}iiiiii                     A  { ~}}                      UUU*
                              h1 AUUUU                                 h2 A                         iii} h3
                                         AA UUUUU                    }       AAA iii         i  ii i
                                           AA         UUUU }}}                                        }}
                                             AA               }}UUUUU iiiiAiAiAi                   }}}
                                               AA                       Ui         A
                                                @ABC
                                                GFED                                 GFED
                                                                                     @ABC
                                                             }          iU                        }
                                                       }~ it}iiiiii UUUUUUA*                  }~ }
                                                     @A                                                               BC
                                                  Ω1                                  Ω2


                                                                                      


                 Figure 7.2: Illustration of a Jordan network. The network output is buffered in the context neurons
                 and with the next time step it is entered into the network together with the new input.




                 with one context neuron per output neu-                                   during the next time step (i.e. again a com-
                 ron. The set of context neurons is called                                 plete link on the way back). So the com-
                 K. The context neurons are completely                                     plete information processing part1 of the
                 linked toward the input layer of the net-                                 MLP exists a second time as a "context
                 work.                                                                     version" – which once again considerably
                                                                                           increases dynamics and state variety.
                                                             Compared with Jordan networks the El-
                 7.2 Elman networks                          man networks often have the advantage to
                                                             act more purposeful since every layer can
                 The Elman networks (a variation of access its own context.
                 the Jordan networks) [Elm90] have con-
                 text neurons, too, but one layer of context Definition 7.3 (Elman network). An El-
                 neurons per information processing neu- man network is an MLP with one con-
                 ron layer (fig. 7.3 on the following page). text neuron per information processing
                 Thus, the outputs of each hidden neuron neuron. The set of context neurons is
                 or output neuron are led into the associ- called K. This means that there exists one
nearly every-
      thing is
     buffered    ated context layer (again exactly one con- context layer per information processing
                 text neuron per neuron) and from there it                                 1 Remember: The input layer does not process in-
                 is reentered into the complete neuron layer                                 formation.




                 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                      123
Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)                                                            dkriesel.com




                    @ABC
                    GFED                                       GFED
                                                               @ABC
                                                                 
                       i1 @UUUU                              i 2i
                   ~~          @@ UUUUUiiiiiii~~ @@@
                 ~~                 @ @ i   i i U U        ~
                                                      UUU~~U                @@
               ~~              i    ii@i@i@             ~ UUUU                @@
              ~           ii                           ~            UUUU @@@
 GFED
 @ABC                                       @ABC
                                            GFED                                   @ABC
                                                                                   GFED                  ONML
                                                                                                         HIJK           HIJK
                                                                                                                        ONML            HIJK
                                                                                                                                        ONML
         ~   ~ iiii                        @@      ~  ~                                 zw v
     ~~it tu iii                                 ~~uv                         UUU*
  h1 @UUUU                                    h2 @                              ii  h 3                 4 kh1       5
                                                                                                                         kh              kh
          @@ UUUUU                                                     iiii ~~                                                      5
                                                                                                                          2               3
                                            ~      @@
              @@           UUUU ~~~                    @@ iiiiii               ~~
                                       U                 i
                @@
                   @@               ~~~ UUUUiUiUiiii @@@                   ~ ~~
                    @ABC
                    GFED                                       @ABC
                                                               GFED                                     HIJK
                                                                                                        ONML            HIJK
                                                                                                                        ONML
                           ~ it~u iiiii    i    U   UUUU @        ~ ~wv
                                                            U*
                       Ω1                                       Ω2                                  5
                                                                                                         kΩ              kΩ
                                                                                                           1
                                                                                                                5         2




                                                                 


Figure 7.3: Illustration of an Elman network. The entire information processing part of the network
exists, in a way, twice. The output of each neuron (except for the output of the input neurons)
is buffered and reentered into the associated layer. For the reason of clarity I named the context
neurons on the basis of their models in the actual network, but it is not mandatory to do so.




neuron layer with exactly the same num- 7.3 Training recurrent
ber of context neurons. Every neuron has          networks
a weighted connection to exactly one con-
text neuron while the context layer is com-
pletely linked towards its original layer.  In order to explain the training as compre-
                                            hensible as possible, we have to agree on
                                            some simplifications that do not affect the
                                            learning principle itself.
Now it is interesting to take a look at the
                                                                                       So for the training let us assume that in
training of recurrent networks since, for in-
                                                                                       the beginning the context neurons are ini-
stance, ordinary backpropagation of error
                                                                                       tiated with an input, since otherwise they
cannot work on recurrent networks. Once
                                                                                       would have an undefined input (this is no
again, the style of the following part is
                                                                                       simplification but reality).
rather informal, which means that I will
not use any formal definitions.                                                        Furthermore, we use a Jordan network
                                                                                       without a hidden neuron layer for our
                                                                                       training attempts so that the output neu-



124                                        D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                             7.3 Training recurrent networks


rons can directly provide input. This ap-    but forward-oriented network without re-
proach is a strong simplification because    currences. This enables training a recur-
generally more complicated networks are      rent network with any training strategy
used. But this does not change the learn-    developed for non-recurrent ones. Here,
                                                                                            attach
ing principle.                               the input is entered as teaching input into    the same
                                             every "copy" of the input neurons. This        network

                                             can be done for a discrete number of time
                                                                                            to each
                                                                                            context
                                             steps. These training paradigms are called
7.3.1 Unfolding in time
                                                                                            layer
                                             unfolding in time [MP69]. After the un-
                                             folding a training by means of backpropa-
Remember our actual learning procedure gation of error is possible.
for MLPs, the backpropagation of error,
which backpropagates the delta values. But obviously, for one weight wi,j sev-
So, in case of recurrent networks the eral changing values ∆wi,j are received,
delta values would backpropagate cycli- which can be treated differently: accumu-
cally through the network again and again, lation, averaging etc. A simple accumu-
which makes the training more difficult. lation could possibly result in enormous
On the one hand we cannot know which changes per weight if all changes have the
of the many generated delta values for a same sign. Hence, also the average is not
weight should be selected for training, i.e. to be underestimated. We could also intro-
which values are useful. On the other hand duce a discounting factor, which weakens
we cannot definitely know when learning the influence of ∆wi,j of the past.
should be stopped. The advantage of re- Unfolding in time is particularly useful if
current networks are great state dynamics we receive the impression that the closer
within the network; the disadvantage of past is more important for the network
recurrent networks is that these dynamics than the one being further away. The
are also granted to the training and there- reason for this is that backpropagation
fore make it difficult.                      has only little influence in the layers far-
                                             ther away from the output (remember:
One learning approach would be the at-       the farther we are from the output layer,
tempt to unfold the temporal states of       the smaller the influence of backpropaga-
the network (fig. 7.4 on the next page):     tion).
Recursions are deleted by putting a sim-
ilar network above the context neurons,      Disadvantages: the training of such an un-
i.e. the context neurons are, as a man-      folded network will take a long time since
ner of speaking, the output neurons of       a large number of layers could possibly be
the attached network. More generally spo-    produced. A problem that is no longer
ken, we have to backtrack the recurrences    negligible is the limited computational ac-
and place "‘earlier"’ instances of neurons   curacy of ordinary computers, which is
in the network – thus creating a larger,     exhausted very fast because of so many



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                     125
Chapter 7 Recurrent perceptron-like networks (depends on chapter 5)                                                                 dkriesel.com




                         @ABC
                         GFED         @ABC
                          i1 OUOUUU GFED           GFED
                                                   @ABC                                      GFED
                                                                                             @ABC           GFED
                                                                                                            @ABC
                                                     
                                       i2 @PP        i3 A                                   n} kO 1 iiininin kO 2
                                 OOOUUUU @@PPP            A                              nnn       i n
                                     OOO UUUU@@ PPP         AA                        nn } iii nn
                                          OOO U@U@UU PPP AA nnnniii}i}i}iinnnn
                                             OOO @@ UUUUPPPPnAnAniiii }}nnnn
                                                   @ABC
                                                   GFED                     @ABC
                                                                          * GFED
                                                OO'   nw UniUniUniUPiUPiA'     }~ nw }nn
                                                      @A                                   BC              
                                                          it
                                                     Ω1                      Ω2


                                                                                  




                                                                                   ..                ..              ..                          ..            ..
                                                                                    .                 .               .                           .             .


                                                                                ()*+RVRVV
                                                                                /.-,                 ()*+
                                                                                                     /.-,            /.-,
                                                                                                                     ()*+?                        ()*+
                                                                                                                                                  /.-,        ()*+
                                                                                                                                                              /.-,
                                                                                                                     
                                                                                          RRVRVVV CPCPCPPP                                     oo jjjojojojo
                                                                                               RRRVVVV CC PP ???                             oo     j
                                                                                                      RRR VVVCV PPP ?? ooojojjjjjoooo
                                   ()*+RVRVV
                                   /.-,                    ()*+DQQ
                                                           /.-,                 /.-,
                                                                                ()*+C                                ()*+
                                                                                                                   n/.-,             ()*+
                                                                                                                                      /.-,
                                                                                                           RRR CVCVVVVPPPoo?ojjj  ooo
                                                                                                              RRRC!  ojVojVjVPjVP? oo
                                                                                                                     ( wotj           * ( wo
                                             V
                                            RRRVVVV DDQQQ                             C                          nn       j j jpjpjp
                                                  RRR VVVV DD QQQ                        C                      n  j p
                                                           RRR VVDVDV QQQ CCC nnnnjnjjjjjpjppp
                                                                RRR DVDVVV QQQ CnCnnjjjj  ppp
       @ABC
       GFED         GFED
                    @ABC         @ABC
                                 GFED                                          GFED
                                                                               @ABC                 @ABC
                                                                                      vntjnjj VVQ* (GFED
                                                                      RRR D!  VVVVQnVQnjQjjC!         wpppp
                                                                             R(
        i1 OUOUUU i2 @PP            i                                        n k1            iiin k2
               OOOUUUU @@PPP 3 AAA                                       nnn}}}iiiiinininnn
                   OOO UUUU@@ PPP            A                        n n
                        OOO U@U@UU PPP AA nnnniii}i}ii nnnn
                           OOO @@ UUUUPPPPnAnAniiii }}nnnn
                                 @ABC
                                 GFED                     @ABC
                                                        * GFED
                              OO'   nw UniUniUniUPiUPiA'       }~ nw }nn
                                          it
                                   Ω1                      Ω2


                                                                   


Figure 7.4: Illustration of the unfolding in time with a small exemplary recurrent MLP. Top: The
recurrent MLP. Bottom: The unfolded network. For reasons of clarity, I only added names to
the lowest part of the unfolded network. Dotted arrows leading into the network mark the inputs.
Dotted arrows leading out of the network mark the outputs. Each "network copy" represents a time
step of the network with the most recent time step being at the bottom.




126                                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                               7.3 Training recurrent networks


             nested computations (the farther we are         are chosen suitably: So, for example, neu-
             from the output layer, the smaller the in-      rons and weights can be adjusted and
             fluence of backpropagation, so that this        the network topology can be optimized
             limit is reached). Furthermore, with sev-       (of course the result of learning is not
             eral levels of context neurons this proce-      necessarily a Jordan or Elman network).
             dure could produce very large networks to       With ordinary MLPs, however, evolution-
             be trained.                                     ary strategies are less popular since they
                                                             certainly need a lot more time than a di-
                                                             rected learning procedure such as backpro-
             7.3.2 Teacher forcing                           pagation.

             Other procedures are the equivalent
             teacher forcing and open loop learn-
             ing. They detach the recurrence during
             the learning process: We simply pretend
 teaching
     input   that the recurrence does not exist and ap-
applied at   ply the teaching input to the context neu-
             rons during the training. So, backpropaga-
  context
  neurons
             tion becomes possible, too. Disadvantage:
             with Elman networks a teaching input for
             non-output-neurons is not given.


             7.3.3 Recurrent backpropagation

             Another popular procedure without lim-
             ited time horizon is the recurrent back-
             propagation using methods of differ-
             ential calculus to solve the problem
             [Pin87].


             7.3.4 Training with evolution

             Due to the already long lasting train-
             ing time, evolutionary algorithms have
             proved to be of value, especially with recur-
             rent networks. One reason for this is that
             they are not only unrestricted with respect
             to recurrences but they also have other ad-
             vantages when the mutation mechanisms



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       127
Chapter 8
Hopfield networks
                  In a magnetic field, each particle applies a force to any other particle so that
                    all particles adjust their movements in the energetically most favorable way.
                     This natural mechanism is copied to adjust noisy inputs in order to match
                                                                               their real models.




Another supervised learning example of         encourage each other to continue this rota-
the wide range of neural networks was          tion. As a manner of speaking, our neural
developed by John Hopfield: the so-            network is a cloud of particles
called Hopfield networks [Hop82]. Hop-
field and his physically motivated net-        Based on the fact that the particles auto-
works have contributed a lot to the renais-    matically detect the minima of the energy
sance of neural networks.                      function, Hopfield had the idea to use the
                                               "spin" of the particles to process informa-
                                               tion: Why not letting the particles search
                                               minima on arbitrary functions? Even if we
8.1 Hopfield networks are
                                               only use two of those spins, i.e. a binary
    inspired by particles in a                 activation, we will recognize that the devel-
    magnetic field                             oped Hopfield network shows considerable
                                               dynamics.
The idea for the Hopfield networks origi-
nated from the behavior of particles in a
magnetic field: Every particle "communi- 8.2 In a hopfield network, all
cates" (by means of magnetic forces) with         neurons influence each
every other particle (completely linked)
with each particle trying to reach an ener-
                                                  other symmetrically
getically favorable state (i.e. a minimum
of the energy function). As for the neurons Briefly speaking, a Hopfield network con-
this state is known as activation. Thus, sists of a set K of completely linked neu-
all particles or neurons rotate and thereby rons with binary activation (since we only
                                                                                                     JK


                                                                                             129
             Chapter 8 Hopfield networks                                                                                dkriesel.com


                                                                                           Definition 8.1 (Hopfield network). A
                                 ?>=<
                                 89:;
                                    ↑ iSo S                          89:;
                                                               k6/ 5 ?>=<
                                                                       ↓                   Hopfield network consists of a set K of
                               @ O ^<<<SSSSSSkkkkkk@ O ^<<<                            completely linked neurons without direct
                                          << kkk SSSS                    <<
                                    k   kkk<k<            SSSSS            <            recurrences. The activation function of
                                   kk                                 SSSSS <<<
               89:;
               ?>=<                                89:;
                                                 / ?>=<                             89:;
                                                                                 /4 ?>=<
                                  k            << 
                    ukkkkkk                                                  SS)        the neurons is the binary threshold func-
                 ↑ ^<iSjo SSS                        ↓  o
                                                    @ ^<<                    kkk5 @
                                                                                      ↑
                         << SSSS                         << kkk         kkk               tion with outputs ∈ {1, −1}.
                           <<        SSSS                 < k                
                             <<             SSS         kkk<k              
                               <<   kkSkSkSkSkSS <<<   
                                 ?>=<
                                 89:;                                89:;
                                                                  / ?>=<
                                   ukkkk                 Definition 8.2 (State of a Hopfield net-
                                                            SSS)  
                                    ↓ o                    work). The state of the network con-
                                                                       ↑
                                                           sists of the activation states of all neu-
                                                           rons. Thus, the state of the network can
             Figure 8.1: Illustration of an exemplary Hop- be understood as a binary string z ∈
             field network. The arrows ↑ and ↓ mark the {−1, 1}|K| .
             binary "spin". Due to the completely linked neu-
             rons the layers cannot be separated, which means
             that a Hopfield network simply includes a set of
             neurons.                                                                      8.2.1 Input and output of a
                                                                                                 Hopfield network are
                                                                                                 represented by neuron states


             use two spins), with the weights being   We have learned that a network, i.e. a
             symmetric between the individual neurons set of |K| particles, that is in a state
completely                                            is automatically looking for a minimum.
             and without any neuron being directly con-
    linked
    set of                                            An input pattern of a Hopfield network
             nected to itself (fig. 8.1). Thus, the state
  neurons
             of |K| neurons with two possible states  is exactly such a state: A binary string
             ∈ {−1, 1} can be described by a string   x ∈ {−1, 1}|K| that initializes the neurons.
             x ∈ {−1, 1}|K| .                         Then the network is looking for the min-
                                                      imum to be taken (which we have previ-
             The complete link provides a full square ously defined by the input of training sam-
             matrix of weights between the neurons. ples) on its energy surface.
             The meaning of the weights will be dis-
             cussed in the following. Furthermore, we But when do we know that the minimum
             will soon recognize according to which has been found? This is simple, too: when                                          input and
             rules the neurons are spinning, i.e. are the network stops. It can be proven that a                                       output =

             changing their state.                    Hopfield network with a symmetric weight                                         network

                                                      matrix that has zeros on its diagonal al-
                                                                                                                                       states

             Additionally, the complete link leads to ways converges [CG88], i.e. at some point
                                                                                                                                       always
             the fact that we do not know any input, it will stand still. Then the output is a                                         converges
             output or hidden neurons. Thus, we have binary string y ∈ {−1, 1}|K| , namely the
             to think about how we can input some- state string of the network that has found
             thing into the |K| neurons.              a minimum.



             130                                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                               8.2 Structure and functionality


Now let us take a closer look at the con- Zero weights lead to the two involved
tents of the weight matrix and the rules         neurons not influencing each other.
for the state change of the neurons.
                                             The weights as a whole apparently take
Definition 8.3 (Input and output of the way from the current state of the net-
a Hopfield network). The input of a work towards the next minimum of the en-
Hopfield network is binary string x ∈ ergy function. We now want to discuss
{−1, 1}|K| that initializes the state of the how the neurons follow this way.
network. After the convergence of the
network, the output is the binary string
y ∈ {−1, 1}|K| generated from the new net- 8.2.3 A neuron changes its state
work state.                                        according to the influence of
                                                      the other neurons
8.2.2 Significance of weights                 Once a network has been trained and
                                              initialized with some starting state, the
We have already said that the neurons change of state x of the individual neu-
                                                                  k
change their states, i.e. their direction, rons k occurs according to the scheme
from −1 to 1 or vice versa. These spins oc-
cur dependent on the current states of the
                                                                                    

other neurons and the associated weights.        xk (t) = fact     wj,k · xj (t − 1) (8.1)
                                                                 X

Thus, the weights are capable to control                        j∈K

the complete change of the network. The
                                              in each time step, where the function fact
weights can be positive, negative, or 0.
                                              generally is the binary threshold function
Colloquially speaking, for a weight wi,j be-
                                              (fig. 8.2 on the next page) with threshold
tween two neurons i and j the following
                                              0. Colloquially speaking: a neuron k cal-
holds:
                                              culates the sum of wj,k · xj (t − 1), which
If wi,j is positive, it will try to force the indicates how strong and into which direc-
     two neurons to become equal – the tion the neuron k is forced by the other
     larger they are, the harder the net- neurons j. Thus, the new state of the net-
     work will try. If the neuron i is in work (time t) results from the state of the
     state 1 and the neuron j is in state network at the previous time t − 1. This
     −1, a high positive weight will advise sum is the direction into which the neuron
     the two neurons that it is energeti- k is pushed. Depending on the sign of the
     cally more favorable to be equal.        sum the neuron takes state 1 or −1.
If wi,j is negative, its behavior will be     Another difference between Hopfield net-
     analoguous only that i and j are         works and other already known network
     urged to be different. A neuron i in     topologies is the asynchronous update: A
     state −1 would try to urge a neuron      neuron k is randomly chosen every time,
     j into state 1.                          which then recalculates the activation.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        131
             Chapter 8 Hopfield networks                                                                dkriesel.com


                                   Heaviside Function             a minimum, then there is the question of
                       1                                          how to teach the weights to force the net-
                                                                  work towards a certain minimum.
                      0.5
              f(x)




                                                                  8.3 The weight matrix is
                       0


                     −0.5
                                                                      generated directly out of
                      −1                                              the training patterns
                            −4    −2         0          2   4
                                             x

                                                                  The aim is to generate minima on the
             Figure 8.2: Illustration of the binary threshold mentioned energy surface, so that at an
             function.                                        input the network can converge to them.
                                                         As with many other network paradigms,
                                                         we use a set P of training patterns p ∈
                                                         {1, −1}|K| , representing the minima of our
             Thus, the new activation of the previously
                                                         energy surface.
             changed neurons immediately influences
             the network, i.e. one time step indicates Unlike many other network paradigms, we
             the change of a single neuron.              do not look for the minima of an unknown
             Regardless of the aforementioned random error function but define minima on such a
             selection of the neuron, a Hopfield net- function. The purpose is that the network
             work is often much easier to implement: shall automatically take the closest min-
             The neurons are simply processed one af- imum when the input is presented. For
             ter the other and their activations are re- now this seems unusual, but we will un-
             calculated until no more changes occur.     derstand the whole purpose later.
  random
   neuron
             Definition 8.4 (Change in the state of               Roughly speaking, the training of a Hop-
calculates
      new    a Hopfield network). The change of state             field network is done by training each train-
activation
             of the neurons occurs asynchronously with            ing pattern exactly once using the rule
             the neuron to be updated being randomly              described in the following (Single Shot
             chosen and the new state being generated             Learning), where pi and pj are the states
             by means of this rule:                               of the neurons i and j under p ∈ P :

                                                                               wi,j =
                                                           
                                                                                                               (8.2)
                                                                                        X
                                                                                              pi · pj
                     xk (t) = fact        wj,k · xj (t − 1) .
                                       X
                                                                                        p∈P
                                   j∈J
                                                        This results in the weight matrix W . Col-
             Now that we know how the weights influ- loquially speaking: We initialize the net-
             ence the changes in the states of the neu- work by means of a training pattern and
             rons and force the entire network towards then process weights wi,j one after another.



             132                            D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                              8.4 Autoassociation and traditional application


For each of these weights we verify: Are       0.139 · |K| training samples can be trained
the neurons i, j n the same state or do the    and at the same time maintain their func-
states vary? In the first case we add 1        tion.
to the weight, in the second case we add
−1.                                           Now we know the functionality of Hopfield
This we repeat for each training pattern networks but nothing about their practical
p ∈ P . Finally, the values of the weights use.
wi,j are high when i and j corresponded
with many training patterns. Colloquially
speaking, this high value tells the neurons: 8.4 Autoassociation and
"Often, it is energetically favorable to hold      traditional application
the same state". The same applies to neg-
ative weights.                                Hopfield networks, like those mentioned
Due to this training we can store a certain    above, are called autoassociators. An
fixed number of patterns p in the weight       autoassociator a exactly shows the afore-
                                               mentioned behavior: Firstly, when a
                                                                                             Ja
matrix. At an input x the network will
converge to the stored pattern that is clos-   known pattern p is entered, exactly this
est to the input p.                            known pattern is returned. Thus,

Unfortunately, the number of the maxi-                         a(p) = p,
mum storable and reconstructible patterns
p is limited to                              with a being the associative mapping. Sec-
                                             ondly, and that is the practical use, this
           |P |MAX ≈ 0.139 · |K|,      (8.3)
                                             also works with inputs that are close to a
which in turn only applies to orthogo- pattern:
nal patterns. This was shown by precise                      a(p + ε) = p.
(and time-consuming) mathematical anal- Afterwards, the autoassociator is, in any
yses, which we do not want to specify case, in a stable state, namely in the state
now. If more patterns are entered, already p.
stored information will be destroyed.
                                             If the set of patterns P consists of, for ex-
Definition 8.5 (Learning rule for Hop- ample, letters or other characters in the             network
                                                                                             restores
field networks). The individual elements form of pixels, the network will be able to         damaged
of the weight matrix W are defined by a correctly recognize deformed or noisy let-           inputs
single processing of the learning rule       ters with high probability (fig. 8.3 on the
              wi,j =                         following page).
                     X
                        pi · pj ,
                    p∈P
                                            The primary fields of application of Hop-
where the diagonal of the matrix is covered field networks are pattern recognition
with zeros. Here, no more than |P |MAX ≈ and pattern completion, such as the zip



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       133
Chapter 8 Hopfield networks                                                dkriesel.com


                                            code recognition on letters in the eighties.
                                            But soon the Hopfield networks were re-
                                            placed by other systems in most of their
                                            fields of application, for example by OCR
                                            systems in the field of letter recognition.
                                            Today Hopfield networks are virtually no
                                            longer used, they have not become estab-
                                            lished in practice.




                                            8.5 Heteroassociation and
                                                analogies to neural data
                                                storage


                                            So far we have been introduced to Hopfield
                                            networks that converge from an arbitrary
                                            input into the closest minimum of a static
                                            energy surface.

                                            Another variant is a dynamic energy sur-
                                            face: Here, the appearance of the energy
                                            surface depends on the current state and
                                            we receive a heteroassociator instead of
                                            an autoassociator. For a heteroassocia-
                                            tor
                                                           a(p + ε) = p
Figure 8.3: Illustration of the convergence of an
exemplary Hopfield network. Each of the pic- is no longer true, but rather
tures has 10 × 12 = 120 binary pixels. In the
Hopfield network each pixel corresponds to one
neuron. The upper illustration shows the train-                h(p + ε) = q,
ing samples, the lower shows the convergence of
a heavily noisy 3 to the corresponding training which means that a pattern is mapped
sample.                                           onto another one. h is the heteroasso-
                                            ciative mapping. Such heteroassociations
                                                                                           Jh
                                            are achieved by means of an asymmetric
                                            weight matrix V .



134                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                   8.5 Heteroassociation and analogies to neural data storage


              Heteroassociations connected in series of Definition 8.6 (Learning rule for the het-
              the form                                  eroassociative matrix). For two training
                                                        samples p being predecessor and q being
                            h(p + ε) = q
                                                        successor of a heteroassociative transition
                            h(q + ε) = r                the weights of the heteroassociative matrix
                            h(r + ε) = s                V result from the learning rule
                                    ..
                                     .
                                                                    vi,j =
                                                                              X
                                                                                      p i qj ,
                            h(z + ε) = p                                   p,q∈P,p6=q

              can provoke a fast cycle of states
                                                            with several heteroassociations being intro-
                   p → q → r → s → . . . → z → p,           duced into the network by a simple addi-
              whereby a single pattern is never com- tion.
              pletely accepted: Before a pattern is en-
              tirely completed, the heteroassociation al-
              ready tries to generate the successor of this
              pattern. Additionally, the network would 8.5.2 Stabilizing the
              never stop, since after having reached the           heteroassociations
              last state z, it would proceed to the first
              state p again.
                                                            We have already mentioned the problem
                                                            that the patterns are not completely gen-
              8.5.1 Generating the
                                                            erated but that the next pattern is already
                      heteroassociative matrix              beginning before the generation of the pre-
                                                            vious pattern is finished.
              We generate the matrix V by means of el-
    VI
              ements v very similar to the autoassocia-
    vI        tive matrix with p being (per transition) This problem can be avoided by not only
              the training sample before the transition influencing the network by means of the
              and q being the training sample to be gen- heteroassociative matrix V but also by
     qI                                                     the already known autoassociative matrix
              erated from p:
                                                            W.
                           vi,j =                     (8.4)
                                   X
                                          p i qj
                                p,q∈P,p6=q
                                                            Additionally, the neuron adaptation rule
              The diagonal of the matrix is again filled    is changed so that competing terms are
              with zeros. The neuron states are, as al-     generated: One term autoassociating an
   netword
is instable   ways, adapted during operation. Several       existing pattern and one term trying to
      while   transitions can be introduced into the ma-    convert the very same pattern into its suc-
              trix by a simple addition, whereby the said   cessor. The associative rule provokes that
 changing
     states
              limitation exists here, too.                  the network stabilizes a pattern, remains



              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      135
                 Chapter 8 Hopfield networks                                                                          dkriesel.com


                 there for a while, goes on to the next pat-                            Which letter in the alphabet follows the
                 tern, and so on.                                                    letter P ?

                 xi (t + 1) =                                            (8.5)
                                                                                 Another example is the phenomenon that
                                                                                 one cannot remember a situation, but the
                                                                         

                                                                                 place at which one memorized it the last
                       X                                                 
                               wi,j xj (t) +           vi,k xk (t − ∆t)
                                                X                        
                 fact 
                      
                       j∈K
                                                                                time is perfectly known. If one returns
                                                 k∈K
                                                                                 to this place, the forgotten situation often
                                                                          
                        |   {z               }   |          {z           }
                                                                                 comes back to mind.
                           autoassociation           heteroassociation



                 Here, the value ∆t causes, descriptively
      ∆tI
                 speaking, the influence of the matrix V
stable change
                 to be delayed, since it only refers to a
     in states
                 network being ∆t versions behind. The
                                                                                 8.6 Continuous Hopfield
                 result is a change in state, during which                           networks
                 the individual states are stable for a short
                 while. If ∆t is set to, for example, twenty
                 steps, then the asymmetric weight matrix                        So far, we only have discussed Hopfield net-
                 will realize any change in the network only                     works with binary activations. But Hop-
                 twenty steps later so that it initially works                   field also described a version of his net-
                 with the autoassociative matrix (since it                       works with continuous activations [Hop84],
                 still perceives the predecessor pattern of                      which we want to cover at least briefly:
                 the current one), and only after that it will                   continuous Hopfield networks. Here,
                 work against it.                                                the activation is no longer calculated by
                                                                                 the binary threshold function but by the
                                                                                 Fermi function with temperature parame-
                 8.5.3 Biological motivation of                                  ters (fig. 8.4 on the next page).
                       heterassociation
                                                                Here, the network is stable for symmetric
                 From a biological point of view the transi- weight matrices with zeros on the diagonal,
                 tion of stable states into other stable states too.
                 is highly motivated: At least in the begin-
                 ning of the nineties it was assumed that Hopfield also stated, that continuous Hop-
                 the Hopfield modell will achieve an ap- field networks can be applied to find ac-
                 proximation of the state dynamics in the ceptable solutions for the NP-hard trav-
                 brain, which realizes much by means of elling salesman problem [HT85]. Accord-
                 state chains: When I would ask you, dear ing to some verification trials [Zel94] this
                 reader, to recite the alphabet, you gener- statement can’t be kept up any more. But
                 ally will manage this better than (please today there are faster algorithms for han-
                 try it immediately) to answer the follow- dling this problem and therefore the Hop-
                 ing question:                                  field network is no longer used here.



                 136                             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                 8.6 Continuous Hopfield networks


                 Fermi Function with Temperature Parameter
         1


        0.8


        0.6
 f(x)




        0.4


        0.2


         0
                −4        −2        0          2        4
                                    x


Figure 8.4: The already known Fermi function
with different temperature parameter variations.




Exercises

Exercise 14. Indicate the storage re-
quirements for a Hopfield network with
|K| = 1000 neurons when the weights wi,j
shall be stored as integers. Is it possible
to limit the value range of the weights in
order to save storage space?

Exercise 15. Compute the weights wi,j
for a Hopfield network using the training
set

              P ={(−1, −1, −1, −1, −1, 1);
                  (−1, 1, 1, −1, −1, −1);
                  (1, −1, −1, 1, −1, 1)}.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          137
Chapter 9
Learning vector quantization
                 Learning Vector Quantization is a learning procedure with the aim to represent
                   the vector training sets divided into predefined classes as well as possible by
                    using a few representative vectors. If this has been managed, vectors which
                        were unkown until then could easily be assigned to one of these classes.




Slowly, part II of this text is nearing its     9.1 About quantization
end – and therefore I want to write a last
chapter for this part that will be a smooth
                                                In order to explore the learning vec-
transition into the next one: A chapter
                                                tor quantization we should at first get
about the learning vector quantization
                                                a clearer picture of what quantization
(abbreviated LVQ) [Koh89] described by
                                                (which can also be referred to as dis-
Teuvo Kohonen, which can be charac-
                                                cretization) is.
terized as being related to the self orga-
nizing feature maps. These SOMs are de-         Everybody knows the sequence of discrete
scribed in the next chapter that already        numbers
belongs to part III of this text, since SOMs
learn unsupervised. Thus, after the explo-                     N = {1, 2, 3, . . .},
ration of LVQ I want to bid farewell to
supervised learning.                            which contains the natural numbers. Dis-
                                                crete means, that this sequence consists of
                                                                                                     discrete
                                                separated elements that are not intercon-            = separated

Previously, I want to announce that there       nected. The elements of our example are
are different variations of LVQ, which will     exactly such numbers, because the natural
be mentioned but not exactly represented.       numbers do not include, for example, num-
The goal of this chapter is rather to ana-      bers between 1 and 2. On the other hand,
lyze the underlying principle.                  the sequence of real numbers R, for in-
                                                stance, is continuous: It does not matter
                                                how close two selected numbers are, there
                                                will always be a number between them.



                                                                                             139
Chapter 9 Learning vector quantization                                        dkriesel.com


Quantization means that a continuous           into classes that reflect the input space
space is divided into discrete sections: By    as well as possible (fig. 9.1 on the facing    input space
deleting, for example, all decimal places      page). Thus, each element of the input         reduced to
of the real number 2.71828, it could be        space should be assigned to a vector as a      vector repre-
                                                                                              sentatives
assigned to the natural number 2. Here         representative, i.e. to a class, where the
it is obvious that any other number hav-       set of these representatives should repre-
ing a 2 in front of the comma would also       sent the entire input space as precisely as
be assigned to the natural number 2, i.e.      possible. Such a vector is called codebook
2 would be some kind of representative         vector. A codebook vector is the represen-
for all real numbers within the interval       tative of exactly those input space vectors
[2; 3).                                        lying closest to it, which divides the input
                                               space into the said discrete areas.
It must be noted that a sequence can be ir-
regularly quantized, too: For instance, the It is to be emphasized that we have to
timeline for a week could be quantized into know in advance how many classes we
working days and weekend.                   have and which training sample belongs
A special case of quantization is digiti- to which class. Furthermore, it is impor-
zation: In case of digitization we always tant that the classes must not be disjoint,
talk about regular quantization of a con- which means they may overlap.
tinuous space into a number system with
                                              Such separation of data into classes is in-
respect to a certain basis. If we enter, for
                                              teresting for many problems for which it
example, some numbers into the computer,
                                              is useful to explore only some characteris-
these numbers will be digitized into the bi-
                                              tic representatives instead of the possibly
nary system (basis 2).
                                              huge set of all vectors – be it because it is
Definition 9.1 (Quantization). Separa- less time-consuming or because it is suffi-
tion of a continuous space into discrete sec- ciently precise.
tions.

Definition 9.2 (Digitization).     Regular
quantization.
                                               9.3 Using codebook vectors:
                                                   the nearest one is the
                                                   winner
9.2 LVQ divides the input
    space into separate areas                  The use of a prepared set of codebook vec-
                                               tors is very simple: For an input vector y
Now it is almost possible to describe by       the class association is easily decided by
                                                                                              closest
means of its name what LVQ should en-          considering which codebook vector is the       vector
able us to do: A set of representatives        closest – so, the codebook vectors build a     wins

should be used to divide an input space        voronoi diagram out of the set. Since



140                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                               9.4 Adjusting codebook vectors




Figure 9.1: BExamples for quantization of a two-dimensional input space. DThe lines represent
the class limit, the × mark the codebook vectors.




each codebook vector can clearly be asso- are used to cause a previously defined num-
ciated to a class, each input vector is asso- ber of randomly initialized codebook vec-
ciated to a class, too.                       tors to reflect the training data as precisely
                                              as possible.

9.4 Adjusting codebook
                                               9.4.1 The procedure of learning
    vectors
                                               Learning works according to a simple
As we have already indicated, the LVQ is       scheme. We have (since learning is su-
a supervised learning procedure. Thus, we      pervised) a set P of |P | training samples.
have a teaching input that tells the learn-    Additionally, we already know that classes
ing procedure whether the classification of    are predefined, too, i.e. we also have a set
the input pattern is right or wrong: In        of classes C. A codebook vector is clearly
other words, we have to know in advance        assigned to each class. Thus, we can say
the number of classes to be represented or     that the set of classes |C| contains many
the number of codebook vectors.                codebook vectors C1 , C2 , . . . , C|C| .
Roughly speaking, it is the aim of the This leads to the structure of the training
learning procedure that training samples samples: They are of the form (p, c) and



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          141
Chapter 9 Learning vector quantization                                          dkriesel.com


therefore contain the training input vector Learning process: The learning process
p and its class affiliation c. For the class    takes place according to the rule
affiliation
                                                       ∆Ci = η(t) · h(p, Ci ) · (p − Ci )
             c ∈ {1, 2, . . . , |C|}                                                 (9.1)
                                                 Ci (t + 1) = Ci (t) + ∆Ci ,         (9.2)
holds, which means that it clearly assigns
the training sample to a class or a code-       which we now want to break down.
book vector.
                                              . We have already seen that the first
Intuitively, we could say about learning:       factor η(t) is a time-dependent learn-
"Why a learning procedure? We calculate         ing rate allowing us to differentiate
the average of all class members and place      between large learning steps and fine
their codebook vectors there – and that’s       tuning.
it." But we will see soon that our learning
                                                  . The last factor (p − Ci ) is obviously
procedure can do a lot more.
                                                    the direction toward which the code-
                                                    book vector is moved.
I only want to briefly discuss the steps
of the fundamental LVQ learning proce-            . But the function h(p, Ci ) is the core of
dure:                                               the rule: It implements a distinction
                                                    of cases.
Initialization: We place our set of code-
                                                    Assignment is correct: The winner
     book vectors on random positions in
                                                         vector is the codebook vector of
     the input space.
                                                         the class that includes p. In this
                                                                                                Important!
                                                         case, the function provides posi-
Training sample: A training sample p of
                                                         tive values and the codebook vec-
    our training set P is selected and pre-
                                                         tor moves towards p.
    sented.
                                                    Assignment is wrong: The winner
Distance measurement: We measure the                     vector does not represent the
    distance ||p − C|| between all code-                 class that includes p. Therefore
    book vectors C1 , C2 , . . . , C|C| and our          it moves away from p.
    input p.
                                    We can see that our definition of the func-
Winner: The closest codebook vector tion h was not precise enough. With good
   wins, i.e. the one with          reason: From here on, the LVQ is divided
                                    into different nuances, dependent of how
                min ||p − Ci ||.    exactly h and the learning rate should
                Ci ∈C               be defined (called LVQ1, LVQ2, LVQ3,



142                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                                           9.5 Connection to neural networks


              OLVQ, etc). The differences are, for in-       Exercises
              stance, in the strength of the codebook vec-
              tor movements. They are not all based on       Exercise 16. Indicate a quantization
              the same principle described here, and as      which equally distributes all vectors H ∈
              announced I don’t want to discuss them         H in the five-dimensional unit cube H into
              any further. Therefore I don’t give any        one of 1024 classes.
              formal definition regarding the aforemen-
              tioned learning rule and LVQ.




              9.5 Connection to neural
                  networks

              Until now, in spite of the learning process,
              the question was what LVQ has to do with
              neural networks. The codebook vectors
              can be understood as neurons with a fixed
              position within the input space, similar to
    vectors
              RBF networks. Additionally, in nature it
= neurons?    often occurs that in a group one neuron
              may fire (a winner neuron, here: a code-
              book vector) and, in return, inhibits all
              other neurons.

              I decided to place this brief chapter about
              learning vector quantization here so that
              this approach can be continued in the fol-
              lowing chapter about self-organizing maps:
              We will classify further inputs by means of
              neurons distributed throughout the input
              space, only that this time, we do not know
              which input belongs to which class.

              Now let us take a look at the unsupervised
              learning networks!




              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      143
           Part III

Unsupervised learning network
         paradigms




                                145
              Chapter 10
              Self-organizing feature maps
                                   A paradigm of unsupervised learning neural networks, which maps an input
                                     space by its fixed topology and thus independently looks for simililarities.
                                                       Function, learning procedure, variations and neural gas.




                                                         Unlike the other network paradigms we
              If you take a look at the concepts of biologi-
              cal neural networks mentioned in the intro-have already got to know, for SOMs it is
              duction, one question will arise: How does unnecessary to ask what the neurons calcu-
              our brain store and recall the impressions late. We only ask which neuron is active at
              it receives every day. Let me point out    the moment. Biologically, this is very mo-                 no output,
              that the brain does not have any training  tivated: If in biology the neurons are con-                but active
   How are
data stored   samples and therefore no "desired output". nected to certain muscles, it will be less                 neuron
     in the   And while already considering this subject interesting to know how strong a certain
              we realize that there is no output in this muscle is contracted but which muscle is
     brain?

              sense at all, too. Our brain responds to   activated. In other words: We are not in-
              external input by changes in state. These  terested in the exact output of the neuron
              are, so to speak, its output.              but in knowing which neuron provides out-
                                                         put. Thus, SOMs are considerably more
                                                         related to biology than, for example, the
                                                         feedforward networks, which are increas-
              Based on this principle and exploring ingly used for calculations.
              the question of how biological neural net-
              works organize themselves, Teuvo Ko-
              honen developed in the Eighties his self- 10.1 Structure of a
              organizing feature maps [Koh82, Koh98],
              shortly referred to as self-organizing
                                                                 self-organizing map
              maps or SOMs. A paradigm of neural
              networks where the output is the state of Typically, SOMs have – like our brain –
              the network, which learns completely un- the task to map a high-dimensional in-
              supervised, i.e. without a teacher.        put (N dimensions) onto areas in a low-



                                                                                                            147
               Chapter 10 Self-organizing feature maps                                          dkriesel.com



                                                                 ()*+
                                                                 /.-,    ()*+
                                                                         /.-,    /.-,
                                                                                 ()*+    ()*+
                                                                                         /.-,       ()*+
                                                                                                    /.-,
               dimensional grid of cells (G dimensions)
               to draw a map of the high-dimensional
   high-dim.
       input   space, so to speak. To generate this map,
           ↓   the SOM simply obtains arbitrary many
                                                                 ()*+
                                                                 /.-,    ()*+
                                                                         /.-,    /.-,
                                                                                 ()*+    ()*+
                                                                                         /.-,       ()*+
                                                                                                    /.-,
               points of the input space. During the in-
    low-dim.
        map
               put of the points the SOM will try to cover
               as good as possible the positions on which
               the points appear by its neurons. This par-       ()*+
                                                                 /.-,    /.-,
                                                                         ()*+    ()*+
                                                                                 /.-,    ()*+
                                                                                         /.-,       ()*+
                                                                                                    /.-,
               ticularly means, that every neuron can be

                                                                 ()*+
                                                                 /.-,    ()*+
                                                                         /.-,    /.-,
                                                                                 ()*+    ()*+
                                                                                         /.-,       ()*+
                                                                                                    /.-,
               assigned to a certain position in the input
               space.

               At first, these facts seem to be a bit con-      ()*+
                                                                /.-,    /.-,
                                                                        ()*+   ()*+
                                                                               /.-,   ()*+
                                                                                      /.-,   ()*+
                                                                                             /.-,
               fusing, and it is recommended to briefly
               reflect about them. There are two spaces
               in which SOMs are working:                  Figure 10.1: Example topologies of a self-
                                                             organizing map. Above we can see a one-
                 . The N -dimensional input space and        dimensional topology, below a two-dimensional
                                                             one.
                  . the G-dimensional grid on which the
 input space
                    neurons are lying and which indi-
and topology        cates the neighborhood relationships
                    between the neurons and therefore Even if N = G is true, the two spaces are
                    the network topology.                    not equal and have to be distinguished. In
                                                             this special case they only have the same
               In a one-dimensional grid, the neurons dimension.
               could be, for instance, like pearls on a
               string. Every neuron would have exactly Initially, we will briefly and formally re-
               two neighbors (except for the two end neu- gard the functionality of a self-organizing
               rons). A two-dimensional grid could be a map and then make it clear by means of
               square array of neurons (fig. 10.1). An- some examples.
               other possible array in two-dimensional Definition 10.1 (SOM neuron). Similar
               space would be some kind of honeycomb to the neurons in an RBF network a SOM
               shape. Irregular topologies are possible, neuron k does not occupy a fixed position
               too, but not very often. Topolgies with c (a center) in the input space.
                                                              k
               more dimensions and considerably more                                                           Jc
               neighborhood relationships would also be Definition 10.2 (Self-organizing map).
               possible, but due to their lack of visualiza- A self-organizing map is a set K of SOM
               tion capability they are not employed very neurons. If an input vector is entered, ex-
               often.                                        actly that neuron k ∈ K is activated which
                                                                                                               JK
 Important!




               148                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
     dkriesel.com                                                                 10.3 Training


     is closest to the input pattern in the input       calculated distance to the input. All
     space. The dimension of the input space            other neurons remain inactive.This
     is referred to as N .                              paradigm of activity is also called
NI                                                                                                  input
                                                        winner-takes-all scheme. The output           ↓
     Definition 10.3 (Topology). The neu-               we expect due to the input of a SOM         winner
     rons are interconnected by neighborhood            shows which neuron becomes active.
     relationships. These neighborhood rela-
     tionships are called topology. The train-      In many literature citations, the descrip-
     ing of a SOM is highly influenced by the       tion of SOMs is more formal: Often an
     topology. It is defined by the topology        input layer is described that is completely
     function h(i, k, t), where i is the winner     linked towards an SOM layer. Then the in-
iI                                                  put layer (N neurons) forwards all inputs
     neuron1 ist, k the neuron to be adapted
kI   (which will be discussed later) and t the      to the SOM layer. The SOM layer is later-
     timestep. The dimension of the topology        ally linked in itself so that a winner neuron
     is referred to as G.                           can be established and inhibit the other
GI                                                  neurons. I think that this explanation of
                                                    a SOM is not very descriptive and there-
     10.2 SOMs always activate                      fore I tried to provide a clearer description
                                                    of the network structure.
          the neuron with the
          least distance to an                      Now the question is which neuron is ac-
                                                    tivated by which input – and the answer
          input pattern                             is given by the network itself during train-
                                                    ing.
     Like many other neural networks, the
     SOM has to be trained before it can be
     used. But let us regard the very simple 10.3 Training
     functionality of a complete self-organizing
     map before training, since there are many
     analogies to the training. Functionality [Training makes the SOM topology cover
     consists of the following steps:            the input space] The training of a SOM
                                                 is nearly as straightforward as the func-
     Input of an arbitrary value p of the input tionality described above. Basically, it is
         space RN .                              structured into five steps, which partially
     Calculation of the distance between ev- correspond to those of functionality.
        ery neuron k and p by means of a Initialization: The network starts with
        norm, i.e. calculation of ||p − ck ||. random neuron centers ck ∈ RN from
     One neuron becomes active, namely         the input space.
         such neuron i with the shortest Creating an input pattern: A stimulus,
     1 We will learn soon what a winner neuron is. i.e. a point p, is selected from the



     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        149
              Chapter 10 Self-organizing feature maps                                               dkriesel.com


  training:
                  input space RN . Now this stimulus is Definition 10.4 (SOM learning rule). A
     input,       entered into the network.                 SOM is trained by presenting an input pat-
→ winner i,                                                 tern and determining the associated win-
              Distance measurement: Then the dis- ner neuron. The winner neuron and its
 change in
   position
      i and       tance ||p − ck || is determined for every neighbor neurons, which are defined by the
 neighbors
                  neuron k in the network.                  topology function, then adapt their cen-
                                                            ters according to the rule
              Winner takes all: The winner neuron i
                  is determined, which has the smallest              ∆ck = η(t) · h(i, k, t) · (p − ck ),
                  distance to p, i.e. which fulfills the                                              (10.1)
                  condition                                     c (t + 1) = c (t) + ∆c (t).           (10.2)
                                                                           k          k        k

                        ||p − ci || ≤ ||p − ck || ∀ k 6= i
                                                                       10.3.1 The topology function
                    . You can see that from several win-                      defines, how a learning
                    ner neurons one can be selected at                        neuron influences its
                    will.                                                     neighbors

              Adapting the centers: The neuron cen- The topology function h is not defined
                 ters are moved within the input space on the input space but on the grid and rep-
                 according to the rule2                   resents the neighborhood relationships be-
                                                          tween the neurons, i.e. the topology of the
                     ∆ck = η(t) · h(i, k, t) · (p − ck ), network. It can be time-dependent (which
                                                          it often is) – which explains the parameter
                                                                                                                   defined on
                                                          t. The parameter k is the index running                  the grid
                 where the values ∆ck are simply through all neurons, and the parameter i
                 added to the existing centers. The is the index of the winner neuron.
                 last factor shows that the change in
                                                          In principle, the function shall take a large
                 position of the neurons k is propor-
                                                          value if k is the neighbor of the winner neu-
                 tional to the distance to the input
                                                          ron or even the winner neuron itself, and
                 pattern p and, as usual, to a time-
                                                          small values if not. SMore precise defini-
                 dependent learning rate η(t). The
                                                          tion: The topology function must be uni-
                 above-mentioned network topology ex-
                                                          modal, i.e. it must have exactly one maxi-
                 erts its influence by means of the func-
                                                          mum. This maximum must be next to the
                 tion h(i, k, t), which will be discussed
                                                          winner neuron i, for which the distance to
                 in the following.
                                                          itself certainly is 0.
                                                                                                                   only 1 maximum
              2 Note: In many sources this rule is written ηh(p −
                                                                       Additionally, the time-dependence enables
                                                                                                                   for the winner
                ck ), which wrongly leads the reader to believe that
                h is a constant. This problem can easily be solved     us, for example, to reduce the neighbor-
                by not omitting the multiplication dots ·.             hood in the course of time.



              150                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
     dkriesel.com                                                                     10.3 Training


     In order to be able to output large values
     for the neighbors of i and small values for
     non-neighbors, the function h needs some
     kind of distance notion on the grid because
     from somewhere it has to know how far i
     and k are apart from each other on the
     grid. There are different methods to cal-
     culate this distance.
                                                      ()*+
                                                      /.-,   89:;
                                                             ?>=<
                                                               i o    1    / ?>=<
                                                                             89:;
                                                                              k      ()*+
                                                                                     /.-,    /.-,
                                                                                             ()*+
     On a two-dimensional grid we could apply,
     for instance, the Euclidean distance (lower

                                                      ()*+
                                                      /.-,    ()*+
                                                              /.-,         /.-,
                                                                           ()*+     ()*+
                                                                                    /.-,     ()*+
                                                                                             /.-,
     part of fig. 10.2) or on a one-dimensional
     grid we could simply use the number of the
     connections between the neurons i and k

                                                      ()*+
                                                      /.-,    ()*+
                                                              /.-,         /.-,
                                                                           ()*+     ?>=<
                                                                                    89:;     ()*+
                                                                                             /.-,
     (upper part of the same figure).
                                                                                 qqq8 kO
     Definition 10.5 (Topology function).                                     qqq
                                                                         2.23
     The topology function h(i, k, t) describes                        qqq
                                                      ()*+
                                                      /.-,   89:;
                                                             ?>=<        /()*+o
                                                                          / .-,o       .-,
                                                                                    //()*+   ()*+
                                                                                             /.-,
                                                                     qq
     the neighborhood relationships in the                       qx q                   
                                                               i o
     topology. It can be any unimodal func-

                                                      ()*+
                                                      /.-,    ()*+
                                                              /.-,         /.-,
                                                                           ()*+     ()*+
                                                                                    /.-,     ()*+
                                                                                             /.-,
     tion that reaches its maximum when i = k
     gilt. Time-dependence is optional, but of-
     ten used.
                                              Figure 10.2: Example distances of a one-
                                              dimensional SOM topology (above) and a two-
                                              dimensional SOM topology (below) between two
     10.3.1.1 Introduction of common          neurons i and k. In the lower case the Euclidean
                                              distance is determined (in two-dimensional space
              distance and topology
                                              equivalent to the Pythagoream theorem). In the
              functions                       upper case we simply count the discrete path
                                              length between i and k. To simplify matters I
                                              required a fixed grid edge length of 1 in both
     A common distance function would be, for cases.
     example, the already known Gaussian
     bell (see fig. 10.3 on page 153). It is uni-
     modal with a maximum close to 0. Addi-
     tionally, its width can be changed by ap-
     plying its parameter σ , which can be used
σI
     to realize the neighborhood being reduced
     in the course of time: We simply relate the
     time-dependence to the σ and the result is



     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                151
Chapter 10 Self-organizing feature maps                                              dkriesel.com


a monotonically decreasing σ(t). Then our Typical sizes of the target value of a learn-
topology function could look like this:   ing rate are two sizes smaller than the ini-
                          2
                                         tial value, e.g
                           ||gi −ck ||
                       −
      h(i, k, t) = e         2·σ(t)2
                                         ,   (10.3)
                                                                    0.01 < η < 0.6
where gi and gk represent the neuron po-
sitions on the grid, not the neuron posi- could be true. But this size must also de-
tions in the input space, which would be pend on the network topology or the size
referred to as ci and ck .                of the neighborhood.

Other functions that can be used in-                  As we have already seen, a decreasing
stead of the Gaussian function are, for               neighborhood size can be realized, for ex-
instance, the cone function, the cylin-               ample, by means of a time-dependent,
der function or the Mexican hat func-                 monotonically decreasing σ with the
tion (fig. 10.3 on the facing page). Here,            Gaussin bell being used in the topology
the Mexican hat function offers a particu-            function.
lar biological motivation: Due to its neg-
ative digits it rejects some neurons close            The advantage of a decreasing neighbor-
to the winner neuron, a behavior that has             hood size is that in the beginning a moving
already been observed in nature. This can             neuron "pulls along" many neurons in its
cause sharply separated map areas – and               vicinity, i.e. the randomly initialized net-
that is exactly why the Mexican hat func-             work can unfold fast and properly in the
tion has been suggested by Teuvo Koho-                beginning. In the end of the learning pro-
nen himself. But this adjustment charac-              cess, only a few neurons are influenced at
teristic is not necessary for the functional-         the same time which stiffens the network
ity of the map, it could even be possible             as a whole but enables a good "fine tuning"
that the map would diverge, i.e. it could             of the individual neurons.
virtually explode.
                                                      It must be noted that
10.3.2 Learning rates and
                                                                       h·η ≤1
       neighborhoods can decrease
       monotonically over time
                                            must always be true, since otherwise the
                                            neurons would constantly miss the current
To avoid that the later training phases training sample.
forcefully pull the entire map towards
a new pattern, the SOMs often work
                                            But enough of theory – let us take a look
with temporally monotonically decreasing
                                            at a SOM in action!
learning rates and neighborhood sizes. At
first, let us talk about the learning rate:



152                         D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                                     10.3 Training




                                     Gaussian in 1D                                                          Cone Function
         1
                                                                                   1
        0.8
                                                                                  0.8

        0.6                                                                       0.6
 h(r)




                                                                          f(x)
        0.4                                                                       0.4

                                                                                  0.2
        0.2
                                                                                   0
         0
              −2   −1.5   −1        −0.5    0    0.5        1   1.5   2                      −4        −2          0             2       4
                                            r                                                                      x

                                    Cylinder Funktion                                                  Mexican Hat Function
                                                                                  3.5
         1                                                                         3
                                                                                  2.5
        0.8
                                                                                   2
        0.6                                                                       1.5
 f(x)




                                                                          f(x)




                                                                                   1
        0.4                                                                       0.5
                                                                                   0
        0.2
                                                                                 −0.5
         0                                                                        −1
                                                                                 −1.5
                   −4          −2           0           2         4                     −3        −2    −1         0         1       2       3
                                           x                                                                       x


Figure 10.3: Gaussian bell, cone function, cylinder function and the Mexican hat function sug-
gested by Kohonen as examples for topology functions of a SOM..




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                              153
Chapter 10 Self-organizing feature maps                                               dkriesel.com




                                                                                             89:;
                                                                                             ?>=<
                                                                                               1


              ?>=<
              89:;
                1                                               ?>=<
                                                                89:;
                                                                  2                          ?>=<
                                                                                             89:;
                                                                                               2


                                     89:;
                                     ?>=<
                                       7                                                     89:;
                                                                                             ?>=<
                                                                                               3


                                                                                             89:;
                                                                                             ?>=<
                                                                                               4
                                                    

                                                                                             89:;
                                                                                             ?>=<
              89:;
              ?>=<                                              89:;
                                                                ?>=<
                                                                                               5
                4>                                                6
                   >>
                                                                                             89:;
                                                                                             ?>=<
                     >>
                       >>
                         >                                                                   6


              89:;
              ?>=<
                3          //         p                         ?>=<
                                                                89:;
                                                                  5                          89:;
                                                                                             ?>=<
                                                                                               7




Figure 10.4: Illustration of the two-dimensional input space (left) and the one-dimensional topolgy
space (right) of a self-organizing map. Neuron 3 is the winner neuron since it is closest to p. In
the topology, the neurons 2 and 4 are the neighbors of 3. The arrows mark the movement of the
winner neuron and its neighbors towards the training sample p.

To illustrate the one-dimensional topology of the network, it is plotted into the input space by the
dotted line. The arrows mark the movement of the winner neuron and its neighbors towards the
pattern.




154                             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                              10.4 Examples


10.4 Examples for the                            Thus, the factor (p − ck ) indicates the
     functionality of SOMs                       vector of the neuron k to the pattern
                                                 p. This is now multiplied by different
                                                 scalars:
Let us begin with a simple, mentally com-
prehensible example.                       Our topology function h indicates that
In this example, we use a two-dimensional      only the winner neuron and its two
input space, i.e. N = 2 is true. Let the       closest neighbors (here: 2 and 4) are
grid structure be one-dimensional (G = 1).     allowed to learn by returning 0 for
Furthermore, our example SOM should            all other neurons. A time-dependence
consist of 7 neurons and the learning rate     is not specified. Thus, our vector
should be η = 0.5.                             (p − ck ) is multiplied by either 1 or
                                               0.
The neighborhood function is also kept
simple so that we will be able to mentally The learning rate indicates, as always,
comprehend the network:                        the strength of learning. As already
                                                 mentioned, η = 0.5, i. e. all in all, the
                1   k direct neighbor of i,
                
                                                 result is that the winner neuron and
                
                
 h(i, k, t) =    1   k = i,                      its neighbors (here: 2, 3 and 4) ap-
                0   otherw.                     proximate the pattern p half the way
                
                

                                        (10.4)   (in the figure marked by arrows).

Now let us take a look at the above-    Although the center of neuron 7 – seen
                                        from the input space – is considerably
mentioned network with random initializa-
                                        closer to the input pattern p than neuron
tion of the centers (fig. 10.4 on the preced-
ing page) and enter a training sample p.2, neuron 2 is learning and neuron 7 is
Obviously, in our example the input pat-not. I want to remind that the network
                                        topology specifies which neuron is allowed
tern is closest to neuron 3, i.e. this is the                                                topology
winning neuron.                         to learn and not its position in the input           specifies,
                                        space. This is exactly the mechanism by              who will learn
We remember the learning rule for which a topology can significantly cover an
SOMs                                    input space without having to be related
                                        to it by any sort.
    ∆ck = η(t) · h(i, k, t) · (p − ck )

and process the three factors from the After the adaptation of the neurons 2, 3
back:                                      and 4 the next pattern is applied, and so
                                           on. Another example of how such a one-
Learning direction: Remember that the dimensional SOM can develop in a two-
    neuron centers ck are vectors in the dimensional input space with uniformly
    input space, as well as the pattern p. distributed input patterns in the course of



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      155
          Chapter 10 Self-organizing feature maps                                      dkriesel.com


          time can be seen in figure 10.5 on the fac-
          ing page.

          End states of one- and two-dimensional
          SOMs with differently shaped input spaces
          can be seen in figure 10.6 on page 158.
          As we can see, not every input space can
          be neatly covered by every network topol-
          ogy. There are so called exposed neurons
          – neurons which are located in an area
          where no input pattern has ever been oc-
          curred. A one-dimensional topology gen-
          erally produces less exposed neurons than
          a two-dimensional one: For instance, dur-
          ing training on circularly arranged input
          patterns it is nearly impossible with a two-
          dimensional squared topology to avoid the
          exposed neurons in the center of the cir-
          cle. These are pulled in every direction Figure 10.7: A topological defect in a two-
          during the training so that they finally dimensional SOM.
          remain in the center. But this does not
          make the one-dimensional topology an op-
          timal topology since it can only find less
          complex neighborhood relationships than
                                                       neighborhood size, because the more com-
          a multi-dimensional one.
                                                       plex the topology is (or the more neigh-
                                                       bors each neuron has, respectively, since a
                                                       three-dimensional or a honeycombed two-
          10.4.1 Topological defects are               dimensional topology could also be gener-
                   failures in SOM unfolding           ated) the more difficult it is for a randomly
                                                       initialized map to unfold.
          During the unfolding of a SOM it
          could happen that a topological defect
          (fig. 10.7) occurs, i.e. the SOM does not     10.5 It is possible to adjust
                                                             the resolution of certain
 "knot"
in map    unfold correctly. A topological defect can
          be described at best by means of the word
          "knotting".
                                                             areas in a SOM

          A remedy for topological defects could We have seen that a SOM is trained by
          be to increase the initial values for the entering input patterns of the input space



          156                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com           10.5 Adjustment of resolution and position-dependent learning rate




Figure 10.5: Behavior of a SOM with one-dimensional topology (G = 1) after the input of 0, 100,
300, 500, 5000, 50000, 70000 and 80000 randomly distributed input patterns p ∈ R2 . During the
training η decreased from 1.0 to 0.1, the σ parameter of the Gauss function decreased from 10.0
to 0.2.

D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                           157
Chapter 10 Self-organizing feature maps                                          dkriesel.com




Figure 10.6: End states of one-dimensional (left column) and two-dimensional (right column)
SOMs on different input spaces. 200 neurons were used for the one-dimensional topology, 10 × 10
neurons for the two-dimensionsal topology and 80.000 input patterns for all maps.


158                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                                            10.6 Application


             RN one after another, again and again so      For example, the different phonemes of
             that the SOM will be aligned with these       the finnish language have successfully been
             patterns and map them. It could happen        mapped onto a SOM with a two dimen-
             that we want a certain subset U of the in-    sional discrete grid topology and therefore
             put space to be mapped more precise than      neighborhoods have been found (a SOM
             the other ones.                               does nothing else than finding neighbor-
                                                           hood relationships). So one tries once
             This problem can easily be solved by          more to break down a high-dimensional
             means of SOMs: During the training dis-       space into a low-dimensional space (the
             proportionally many input patterns of the     topology), looks if some structures have
             area U are presented to the SOM. If the       been developed – et voilà: clearly defined
             number of training patterns of U ⊂ RN         areas for the individual phenomenons are
             presented to the SOM exceeds the number       formed.
             of those patterns of the remaining RN \ U ,
             then more neurons will group there while      Teuvo Kohonen himself made the ef-
             the remaining neurons are sparsely dis-       fort to search many papers mentioning his
             tributed on RN \ U (fig. 10.8 on the next     SOMs in their keywords. In this large in-
     more    page).                                        put space the individual papers now indi-
  patterns                                                 vidual positions, depending on the occur-
         ↓   As you can see in the illustration, the edge  rence of keywords. Then Kohonen created
    higher
resolution
             of the SOM could be deformed. This can        a SOM with G = 2 and used it to map the
             be compensated by assigning to the edge       high-dimensional "paper space" developed
             of the input space a slightly higher proba-   by him.
             bility of being hit by training patterns (an
             often applied approach for reaching every Thus, it is possible to enter any paper
             corner with the SOMs).                        into the completely trained SOM and look
                                                           which neuron in the SOM is activated. It
             Also, a higher learning rate is often used will be likely to discover that the neigh-
             for edge and corner neurons, since they are bored papers in the topology are interest-
             only pulled into the center by the topol- ing, too. This type of brain-like context-
             ogy. This also results in a significantly im- based search also works with many other
             proved corner coverage.                       input spaces.
                                                                                                         SOM finds
                                                                                                         similarities
                                                           It is to be noted that the system itself
             10.6 Application of SOMs                      defines what is neighbored, i.e. similar,
                                                           within the topology – and that’s why it
                                                           is so interesting.
             Regarding the biologically inspired asso-
             ciative data storage, there are many This example shows that the position c of
             fields of application for self-organizing the neurons in the input space is not signif-
             maps and their variations.                icant. It is rather interesting to see which



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      159
Chapter 10 Self-organizing feature maps                                                 dkriesel.com




Figure 10.8: Training of a SOM with G = 2 on a two-dimensional input space. On the left side,
the chance to become a training pattern was equal for each coordinate of the input space. On the
right side, for the central circle in the input space, this chance is more than ten times larger than
for the remaining input space (visible in the larger pattern density in the background). In this circle
the neurons are obviously more crowded and the remaining area is covered less dense but in both
cases the neurons are still evenly distributed. The two SOMS were trained by means of 80.000
training samples and decreasing η (1 → 0.2) as well as decreasing σ (5 → 0.5).




160                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                              10.7 Variations


neuron is activated when an unknown in-        to influence neighboring RBF neurons in
put pattern is entered. Next, we can look      different ways.
at which of the previous inputs this neu-
                                               For this, many neural network simulators
ron was also activated – and will imme-
                                               offer an additional so-called SOM layer
diately discover a group of very similar
                                               in connection with the simulation of RBF
inputs. The more the inputs within the
                                               networks.
topology are diverging, the less things they
have in common. Virtually, the topology
generates a map of the input characteris-
tics – reduced to descriptively few dimen-     10.7 Variations of SOMs
sions in relation to the input dimension.
                                            There are different variations of SOMs
Therefore, the topology of a SOM often for different variations of representation
is two-dimensional so that it can be easily tasks:
visualized, while the input space can be
very high-dimensional.
                                               10.7.1 A neural gas is a SOM
                                                      without a static topology
10.6.1 SOMs can be used to
       determine centers for RBF               The neural gas is a variation of the self-
       neurons                                 organizing maps of Thomas Martinetz
                                               [MBS93], which has been developed from
SOMs arrange themselves exactly towards        the difficulty of mapping complex input
the positions of the outgoing inputs. As a     information that partially only occur in
result they are used, for example, to select   the subspaces of the input space or even
the centers of an RBF network. We have         change the subspaces (fig. 10.9 on the fol-
already been introduced to the paradigm        lowing page).
of the RBF network in chapter 6.             The idea of a neural gas is, roughly speak-
As we have already seen, it is possible ing, to realize a SOM without a grid struc-
to control which areas of the input space ture. Due to the fact that they are de-
should be covered with higher resolution rived from the SOMs the learning steps
- or, in connection with RBF networks, are very similar to the SOM learning steps,
on which areas of our function should the but they include an additional intermedi-
RBF network work with more neurons, i.e. ate step:
work more exactly. As a further useful fea-
                                                 . again, random initialization of ck ∈
ture of the combination of RBF networks
                                                   Rn
with SOMs one can use the topology ob-
tained through the SOM: During the final         . selection and presentation of a pat-
training of a RBF neuron it can be used            tern of the input space p ∈ Rn



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       161
Chapter 10 Self-organizing feature maps                                             dkriesel.com




Figure 10.9: A figure filling different subspaces of the actual input space of different positions
therefore can hardly be filled by a SOM.




  . neuron distance measurement                   of the winner neuron i. The direct re-
                                                  sult is that – similar to the free-floating
  . identification of the winner neuron i         molecules in a gas – the neighborhood rela-
                                                                                                     dynamic
                                                                                                     neighborhood
                                                  tionships between the neurons can change
  . Intermediate step: generation of a list
                                                  anytime, and the number of neighbors is
    L of neurons sorted in ascending order
                                                  almost arbitrary, too. The distance within
    by their distance to the winner neu-
                                                  the neighborhood is now represented by
    ron. Thus, the first neuron in the list
                                                  the distance within the input space.
    L is the neuron that is closest to the
    winner neuron.
                                          The bulk of neurons can become as stiff-
  . changing the centers by means of the ened as a SOM by means of a constantly
    known rule but with the slightly mod- decreasing neighborhood size. It does not
    ified topology function               have a fixed dimension but it can take the
                                          dimension that is locally needed at the mo-
                  hL (i, k, t).           ment, which can be very advantageous.

                                                  A disadvantage could be that there is
The function hL (i, k, t), which is slightly      no fixed grid forcing the input space to
modified compared with the original func-         become regularly covered, and therefore
tion h(i, k, t), now regards the first el-        wholes can occur in the cover or neurons
ements of the list as the neighborhood            can be isolated.



162                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
               dkriesel.com                                                              10.7 Variations


               In spite of all practical hints, it is as al-   problem: What do we do with input pat-
               ways the user’s responsibility not to un-       terns from which we know that they are
               derstand this text as a catalog for easy an-    confined in different (maybe disjoint) ar-
               swers but to explore all advantages and         eas?
                                                                                                            several SOMs
               disadvantages himself.
                                                               Here, the idea is to use not only one
               Unlike a SOM, the neighborhood of a neu-        SOM but several ones: A multi-self-
               ral gas must initially refer to all neurons     organizing map, shortly referred to as
               since otherwise some outliers of the ran-       M-SOM [GKE01b, GKE01a, GS06]. It is
               dom initialization may never reach the re-      unnecessary that the SOMs have the same
               maining group. To forget this is a popular      topology or size, an M-SOM is just a com-
               error during the implementation of a neu-       bination of M SOMs.
               ral gas.                                       This learning process is analog to that of
                                                              the SOMs. However, only the neurons be-
               With a neural gas it is possible to learn a
                                                              longing to the winner SOM of each train-
               kind of complex input such as in fig. 10.9
can classify                                                  ing step are adapted. Thus, it is easy to
               on the preceding page since we are not
   complex
                                                              represent two disjoint clusters of data by
      figure   bound to a fixed-dimensional grid. But
                                                              means of two SOMs, even if one of the
               some computational effort could be neces-
                                                              clusters is not represented in every dimen-
               sary for the permanent sorting of the list
                                                              sion of the input space RN . Actually, the
               (here, it could be effective to store the list
                                                              individual SOMs exactly reflect these clus-
               in an ordered data structure right from the
                                                              ters.
               start).
                                                              Definition 10.7 (Multi-SOM). A multi-
               Definition 10.6 (Neural gas). A neural SOM is nothing more than the simultane-
               gas differs from a SOM by a completely dy- ous use of M SOMs.
               namic neighborhood function. With every
               learning cycle it is decided anew which neu-
               rons are the neigborhood neurons of the 10.7.3 A multi-neural gas consists
               winner neuron. Generally, the criterion                 of several separate neural
               for this decision is the distance between               gases
               the neurosn and the winner neuron in the
               input space.                                   Analogous to the multi-SOM, we also have
                                                              a set of M neural gases: a multi-neural
                                                              gas [GS06, SG06]. This construct be-
                                                                                                            several gases
               10.7.2 A Multi-SOM consists of                 haves analogous to neural gas and M-SOM:
                         several separate SOMs                Again, only the neurons of the winner gas
                                                              are adapted.
               In order to present another variant of the The reader certainly wonders what advan-
               SOMs, I want to formulate an extended tage is there to use a multi-neural gas since



               D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       163
                  Chapter 10 Self-organizing feature maps                                         dkriesel.com


                  an individual neural gas is already capa-      Definition 10.8 (Multi-neural gas). A
                  ble to divide into clusters and to work on     multi-neural gas is nothing more than the
                  complex input patterns with changing di-       simultaneous use of M neural gases.
                  mensions. Basically, this is correct, but
                  a multi-neural gas has two serious advan-
                  tages over a simple neural gas.                10.7.4 Growing neural gases can
                                                                        add neurons to themselves
                   1. With several gases, we can directly
                        tell which neuron belongs to which       A growing neural gas is a variation of
                        gas. This is particularly important      the aforementioned neural gas to which
                        for clustering tasks, for which multi-   more and more neurons are added accord-
                        neural gases have been used recently.    ing to certain rules. Thus, this is an at-
                        Simple neural gases can also find and    tempt to work against the isolation of neu-
                        cover clusters, but now we cannot rec-   rons or the generation of larger wholes in
                        ognize which neuron belongs to which     the cover.
                        cluster.                                      Here, this subject should only be men-
less computa-

                   2. A lot of computational effort is saved tioned but not discussed.
  tional effort

                      when large original gases are divided To build a growing SOM is more difficult
                      into several smaller ones since (as al- because new neurons have to be integrated
                      ready mentioned) the sorting of the in the neighborhood.
                      list L could use a lot of computa-
                      tional effort while the sorting of sev-
                      eral smaller lists L1 , L2 , . . . , LM is less Exercises
                      time-consuming – even if these lists in
                      total contain the same number of neu-
                                                                      Exercise 17. A regular, two-dimensional
                      rons.
                                                                      grid shall cover a two-dimensional surface
                  As a result we will only obtain local in- as "well" as possible.
                  stead of global sortings, but in most cases      1. Which grid structure would suit best
                  these local sortings are sufficient.                for this purpose?
                  Now we can choose between two extreme          2. Which criteria did you use for "well"
                  cases of multi-neural gases: One extreme          and "best"?
                  case is the ordinary neural gas M = 1, i.e.
                                                               The very imprecise formulation of this ex-
                  we only use one single neural gas. Interest-
                                                               ercise is intentional.
                  ing enough, the other extreme case (very
                  large M , a few or only one neuron per gas)
                  behaves analogously to the K-means clus-
                  tering (for more information on clustering
                  procedures see excursus A).



                  164                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
Chapter 11
Adaptive resonance theory
                  An ART network in its original form shall classify binary input vectors, i.e. to
                    assign them to a 1-out-of-n output. Simultaneously, the so far unclassified
                                     patterns shall be recognized and assigned to a new class.




As in the other smaller chapters, we want tionally an ART network shall be capable
to try to figure out the basic idea of to find new classes.
the adaptive resonance theory (abbre-
viated: ART ) without discussing its the-
ory profoundly.
                                                11.1 Task and structure of an
In several sections we have already men-             ART network
tioned that it is difficult to use neural
networks for the learning of new informa-
tion in addition to but without destroying      An ART network comprises exactly two
the already existing information. This cir-     layers: the input layer I and the recog-
cumstance is called stability / plasticity      nition layer O with the input layer be-
dilemma.                                        ing completely linked towards the recog-
                                                nition layer. This complete link induces
In 1987, Stephen Grossberg and Gail             a top-down weight matrix W that con-
Carpenter published the first version of        tains the weight values of the connections
their ART network [Gro76] in order to al-       between each neuron in the input layer
leviate this problem. This was followed         and each neuron in the recognition layer
by a whole family of ART improvements           (fig. 11.1 on the following page).
(which we want to discuss briefly, too).
                                                Simple binary patterns are entered into
It is the idea of unsupervised learning,        the input layer and transferred to the               pattern
whose aim is the (initially binary) pattern     recognition layer while the recognition              recognition
recognition, or more precisely the catego-      layer shall return a 1-out-of-|O| encoding,
rization of patterns into classes. But addi-    i.e. it should follow the winner-takes-all



                                                                                             165
     Chapter 11 Adaptive resonance theory                                                                                                                                                                       dkriesel.com




                                                          @ABC
                                                          GFED                                       GFED
                                                                                                     @ABC                                     @ABC
                                                                                                                                              GFED                                     @ABC
                                                                                                                                                                                       GFED
                                                                                                                                                                                          
                                                                             S                                                                                                       5
                                                           E O 1 4Y 4cFOgSFi4OFOFSOSFOSOSOSOSOSSxSxSx; E O 2 4Y 4cFOg F4OFOFOFOOOoOoOoooxoxox7 ; E O 3 4Y 4cFF4FFkFkkokokokokoxkokxox7 ; E O 4 4Y 44
                                                            i                                           i                                           i                                     i
                                                                     444FFFFxOxxOOSxOSSSSS444oFooFFoFxOxxOOxO kkk4k4k4koFkooFFoFxxxx                                                              444
                                                                               4x44x4xxxFxFFFFOFOOOoOSOoOoSoOoSSooSS4xo44SSx4xxSxFxFkFFFkkOFkkOOkkOoOkOoOokoOooo4xo44x4xxxFxFFFFF                   4444
                                                                                                                                  kS
                                                                   xxxxx44444oooFooFFooFFoFkxOkxxOOxkkxOOkxkO4OkkS44O4kS4okSSooSFSooFSSFooFFSoFSxOxxOOxxOOxO4O44O44 FFFFFF 44444
                                                                 x
                                                            xxxx oo4oo4 kkxkFkxxFkFxkF oOo4oOOo4O xSFxSxFFxSSFSS O4OO4O FFFF 4 4
                                                       xxxxxxoooooookookkokk4kk44k4xk4xkxxxkxoooFooFoFoFoFoFo 44O44x4OxOxOxOxOOxOOOFSFFFSFSFSSS4SS4O4S4S4OOSOOOOOOOFFFFFF 44444
                                                  xxxxooookkkkk xxxx44oooo FFxFFxFxxFxx4444 OOOOOFOOFFOFFF4S444SSSSSSOSSOOSOOFOOFFOFFF4444
                                           xxxxoookokkkkkk xxxxooo4o44                                                       xxFF 4                                         FF 4 SS FF 4
                        @ABC
                        GFED                              @ABC
                                                          GFED                                       GFED
                                                                                                     @ABC                                     GFED
                                                                                                                                              @ABC                                     GFED
                                                                                                                                                                                       @ABC              GFED
                                                                                                                                                                                                         @ABC
                             xo{xkow xxokokxokokookkokkk   xo{xow xxooxooooo 4    x{xxxx FFF4#    OOOOOFOOFF'4#   SSSOSSOOSOSOFSOSOFF) '4# 
                                     ku
                         Ω1                                 Ω2                                          Ω3                                        Ω4                                      Ω5              Ω6


                                                                                                                                                                                                 


     Figure 11.1: Simplified illustration of the ART network structure. Top: the input layer, bottom:
     the recognition layer. In this illustration the lateral inhibition of the recognition layer and the control
     neurons are omitted.




     scheme. For instance, to realize this 1-                                                                        put layer causes an activity within the
                                                                                                                                                                                                                               layers
     out-of-|O| encoding the principle of lateral                                                                    recognition layer while in turn in the recog-                                                             activate
     inhibition can be used – or in the imple-                                                                       nition layer every activity causes an activ-                                                              one

     mentation the most activated neuron can                                                                         ity within the input layer.
                                                                                                                                                                                                                               another

     be searched. For practical reasons an IF
     query would suit this task best.

                                                                                                                     In addition to the two mentioned layers,
                                                                                                                     in an ART network also exist a few neu-
     11.1.1 Resonance takes place by
                                                                                                                     rons that exercise control functions such as
            activities being tossed and
                                                                                                                     signal enhancement. But we do not want
            turned
                                                                                                                     to discuss this theory further since here
                                                                                                                     only the basic principle of the ART net-
     But there also exists a bottom-up weight                                                                        work should become explicit. I have only
     matrix V , which propagates the activi-                                                                         mentioned it to explain that in spite of the
VI
     ties within the recognition layer back into                                                                     recurrences, the ART network will achieve
     the input layer. Now it is obvious that                                                                         a stable state after an input.
     these activities are bounced forth and back
     again and again, a fact that leads us to
     resonance. Every activity within the in-



     166                                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                 dkriesel.com                                                                11.3 Extensions


                 11.2 The learning process of 11.2.3 Adding an output neuron
                      an ART network is
                                              Of course, it could happen that the neu-
                      divided to top-down and rons are nearly equally activated or that
                      bottom-up learning      several neurons are activated, i.e. that the
                                                                  network is indecisive. In this case, the
                 The trick of adaptive resonance theory is        mechanisms of the control neurons acti-
                 not only the configuration of the ART net-       vate a signal that adds a new output neu-
                 work but also the two-piece learning pro-        ron. Then the current pattern is assigned
                 cedure of the theory: On the one hand            to this output neuron and the weight sets
                 we train the top-down matrix W , on the          of the new neuron are trained as usual.
                 other hand we train the bottom-up matrix Thus, the advantage of this system is not
                 V (fig. 11.2 on the next page).            only to divide inputs into classes and to
                                                            find new classes, it can also tell us after
                                                            the activation of an output neuron what a
                 11.2.1 Pattern input and top-down
                                                            typical representative of a class looks like
                          learning
                                                            - which is a significant feature.
                 When a pattern is entered into the net-          Often, however, the system can only mod-
                 work it causes - as already mentioned - an       erately distinguish the patterns. The ques-
                 activation at the output neurons and the         tion is when a new neuron is permitted to
      winner
      neuron     strongest neuron wins. Then the weights          become active and when it should learn.
            is   of the matrix W going towards the output         In an ART network there are different ad-
                                                                  ditional control neurons which answer this
    amplified
                 neuron are changed such that the output
                 of the strongest neuron Ω is still enhanced,     question according to different mathemat-
                 i.e. the class affiliation of the input vector   ical rules and which are responsible for in-
                 to the class of the output neuron Ω be-          tercepting special cases.
                 comes enhanced.
                                                             At the same time, one of the largest ob-
                                                             jections to an ART is the fact that an
                 11.2.2 Resonance and bottom-up              ART   network uses a special distinction of
                          learning                           cases, similar to an IF query, that has been
                                                             forced into the mechanism of a neural net-
                 The training of the backward weights of     work.
      input is
  teach. inp.    the matrix V is a bit tricky: Only the
for backward     weights of the respective winner neuron
                 are trained towards the input layer and 11.3 Extensions
      weights

                 our current input pattern is used as teach-
                 ing input. Thus, the network is trained to As already mentioned above, the ART net-
                 enhance input vectors.                      works have often been extended.



                 D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                         167
Chapter 1111Adaptive
   Kapitel             resonance
             Adaptive Resonance   theory
                                Theory                                                           dkriesel.com     dkriesel.com

                                                               einer IF-Abfrage, die man in den Mecha-
                                                               nismusART-2              is extended
                                                                               [CG87] Netzes
                                                                       eines Neuronalen        gepresstto continuous
                                                               hat. inputs and additionally offers (in an ex-
         @ABC
         GFED            @ABC
                         GFED             @ABC
                                          GFED        GFED
                                                      @ABC
                                                        
          i1 b
             Y
                          i2
                           O Y
                                             i
                                           E O 3      < E i4       tension called ART-2A) enhancements of
                                                                   the learning speed which results in addi-
                                                               11.3 Erweiterungen
                                                                   tional control neurons and layers.
                                                                    ART-3
                                                               Wie schon        [CG90]
                                                                         eingangs             wurden die the learning
                                                                                          3 improves
                                                                                    erwähnt,
                         @ABC
                         GFED             @ABC
                                          GFED
                                        "  
                                                               ART-Netze  vielfach erweitert.
                                |
                          Ω1               Ω2                       ability of ART-2 by adapting additional
                                                               ART-2      [CG87] processes
                                                                      biological                such as the chemical
                                                                                    ist eine Erweiterung
                                                             auf kontinuierliche
                                                                      processes within      the und
                                                                                      Eingaben      bietet 1 .
                                                                                                 synapses
                            0                1                 zusätzlich (in einer ART-2A genannten
                                                               Erweiterung)   Verbesserungen
                                                                     Apart from                der Lernge-
                                                                                      the described   ones there exist
                                                               schwindigkeit, was zusätzliche Kontroll-
                                                                      many other extensions.
                                                               neurone und Schichten zur Folge hat.
         @ABC
         GFED        GFED
                     @ABC              @ABC
                                       GFED           @ABC
                                                      GFED
                                                        
          i1 b FF      i2 4               E i3        < E i4   ART-3 [CG90] verbessert die Lernfähig-
                                                               keit von ART-2, indem zusätzliche biolo-
             Y FF       O Y 4                O
                  FF           44
                                                               gische Vorgänge wie z.B. die chemischen
                    FF           44
                      FF
                           FF 444
                             FF 4
                                 FF 44                         Vorgänge innerhalb der Synapsen adap-
                                    FF 4
                                      FF4                    tiert werden1 .
                     @ABC
                     GFED              @ABC
                                       GFED
                                     "        
                             |
                      Ω1                    Ω2                 Zusätzlich zu den beschriebenen Erweite-
                                                               rungen existieren noch viele mehr.
                                             
                            0                1




         @ABC
         GFED         @ABC
                      GFED               @ABC
                                         GFED         @ABC
                                                      GFED
                                                        
          i1 Fb        i2                    i
                                           E O 3      < E i4
             Y FF       O 4Y 4
                 FF           44
                   FF             44
                     FF             4
                       FF
                           FF 444
                                 FF 4
                                    FF 44
                                      FF 4  
                      @ABC
                      GFED               @ABC
                                         GFED
                                     F"       
                               |
                       Ω1                   Ω2

                                                             1 Durch die häufigen Erweiterungen der Adaptive
                            0                1                   Resonance Theory sprechen böse Zungen bereits
                                                                 von ART-n-Netzen“.
                                                                     ”

Figure   11.2: Simplified
     Abbildung                  illustration
                  11.2: Vereinfachte                 des two-
                                                of the
                                       Darstellung
       training ofTrainings
piecezweigeteilten    an ART   eines           The Die
                                      ART-Netzes:
                                  network:            trained
     168 trainierten D.
     jeweils                          – Ein
                             Krieselsind
                         Gewichte            kleiner Überblick über Neuronale Netze (EPSILON-DE)
                                          durchgezogen
weights   are  represented     by  solid  lines.  Let
     dargestellt. Nehmen wir an, ein Muster wurde in
                                                        us as-
         Netz eingegeben und die Zahlen markieren the
sumedasthat   a  pattern   has    been   entered    into
network   and that
     Ausgaben.   Oben: the numbers
                         Wir             ist Ω2the
                             wir sehen,mark      das outputs.
                                                      Ge-
Top:winnerneuron.
        We can see   Mitte:
                         thatAlsoΩwerden   die Gewichte
                                   2 is the winner neu-
     zum Gewinnerneuron hin trainiert und (unten)
ron. die
      Middle:     So the weights are trained towards 1 Because of the frequent extensions of the adap-
         Gewichte vom Gewinnerneuron zur Eingangs-
the winner    neuron and (below) the weights of                    tive resonance theory wagging tongues already call
     schicht trainiert.
                                                                   them "ART-n networks".
the winner neuron are trained towards the input
layer.


168                                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             Part IV

Excursi, appendices and registers




                                    169
Appendix A
Excursus: Cluster analysis and regional and
online learnable fields
                  In Grimm’s dictionary the extinct German word "Kluster" is described by "was
                     dicht und dick zusammensitzet (a thick and dense group of sth.)". In static
                         cluster analysis, the formation of groups within point clouds is explored.
                           Introduction of some procedures, comparison of their advantages and
                    disadvantages. Discussion of an adaptive clustering method based on neural
                       networks. A regional and online learnable field models from a point cloud,
                         possibly with a lot of points, a comparatively small set of neurons being
                                                                representative for the point cloud.




As already mentioned, many problems can            2. dist(x1 , x2 ) = dist(x2 , x1 ), i.e. sym-
be traced back to problems in cluster                 metry,
analysis. Therefore, it is necessary to re-
search procedures that examine whether             3. dist(x1 , x3 )    ≤       dist(x1 , x2 ) +
groups (so-called clusters) exist within              dist(x2 , x3 ), i.e.        the triangle
point clouds.                                         inequality holds.

Since cluster analysis procedures need a
notion of distance between two points, a Colloquially speaking, a metric is a tool
metric must be defined on the space for determining distances between points
where these points are situated.         in any space. Here, the distances have
                                         to be symmetrical, and the distance be-
We briefly want to specify what a metric tween to points may only be 0 if the two
is.                                      points are equal. Additionally, the trian-
Definition A.1 (Metric). A relation gle inequality must apply.
dist(x1 , x2 ) defined for two objects x1 , x2
                                                 Metrics are provided by, for example, the
is referred to as metric if each of the fol-
                                                 squared distance and the Euclidean
lowing criteria applies:
                                                 distance, which have already been intro-
  1. dist(x1 , x2 ) = 0 if and only if x1 = x2 , duced. Based on such metrics we can de-



                                                                                              171
Appendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com


fine a clustering procedure that uses a met-    7. Continue with 4 until the assignments
ric as distance measure.                           are no longer changed.
                                                                                               number of
Now we want to introduce and briefly dis-      Step 2 already shows one of the great ques-     cluster
                                                                                               must be
cuss different clustering procedures.          tions of the k-means algorithm: The num-        known
                                               ber k of the cluster centers has to be de-      previously
                                               termined in advance. This cannot be done
                                               by the algorithm. The problem is that it
A.1 k-means clustering                         is not necessarily known in advance how k
    allocates data to a                        can be determined best. Another problem
    predefined number of                       is that the procedure can become quite in-
                                               stable if the codebook vectors are badly
    clusters                                   initialized. But since this is random, it
                                               is often useful to restart the procedure.
k-means clustering according to J.             This has the advantage of not requiring
MacQueen [Mac67] is an algorithm that          much computational effort. If you are fully
is often used because of its low computa-      aware of those weaknesses, you will receive
tion and storage complexity and which is       quite good results.
regarded as "inexpensive and good". The
                                            However, complex structures such as "clus-
operation sequence of the k-means cluster-
                                            ters in clusters" cannot be recognized. If k
ing algorithm is the following:
                                            is high, the outer ring of the construction
  1. Provide data to be examined.           in the following illustration will be recog-
                                            nized as many single clusters. If k is low,
  2. Define k, which is the number of clus- the ring with the small inner clusters will
     ter centers.                           be recognized as one cluster.

 3. Select k random vectors for the clus- For an illustration see the upper right part
      ter centers (also referred to as code- of fig. A.1 on page 174.
      book vectors).

 4. Assign each data point to the next         A.2 k-nearest neighboring
      codebook vector1
                                                   looks for the k nearest
 5. Compute cluster centers for all clus-          neighbors of each data
      ters.
                                                   point
 6. Set codebook vectors to new cluster
      centers.                                   The k-nearest neighboring procedure
1 The name codebook vector was created because
                                                 [CH67] connects each data point to the k
  the often used name cluster vector was too un- closest neighbors, which often results in a
  clear.                                         division of the groups. Then such a group



172                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                                                 A.4 The silhouette coefficient


             builds a cluster. The advantage is that        which is the reason for the name epsilon-
             the number of clusters occurs all by it-       nearest neighboring. Points are neig-
             self. The disadvantage is that a large stor-   bors if they are at most ε apart from each
             age and computational effort is required to    other. Here, the storage and computa-
             find the next neighbor (the distances be-      tional effort is obviously very high, which
             tween all data points must be computed         is a disadvantage.
                                                                                                            clustering
             and stored).                                                                                   radii around
clustering                                                  But note that there are some special cases:     points
      next
    points   There are some special cases in which the      Two separate clusters can easily be con-
             procedure combines data points belonging       nected due to the unfavorable situation of
             to different clusters, if k is too high. (see  a single data point. This can also happen
             the two small clusters in the upper right      with k-nearest neighboring, but it would
             of the illustration). Clusters consisting of   be more difficult since in this case the num-
             only one single data point are basically       ber of neighbors per point is limited.
             conncted to another cluster, which is not
                                                            An advantage is the symmetric nature of
             always intentional.
                                                            the neighborhood relationships. Another
             Furthermore, it is not mandatory that the advantage is that the combination of min-
             links between the points are symmetric.        imal clusters due to a fixed number of
                                                            neighbors is avoided.
             But this procedure allows a recognition of
             rings and therefore of "clusters in clusters", On the other hand, it is necessary to skill-
             which is a clear advantage. Another ad- fully initialize ε in order to be successful,
             vantage is that the procedure adaptively i.e. smaller than half the smallest distance
             responds to the distances in and between between two clusters. With variable clus-
             the clusters.                                  ter and point distances within clusters this
                                                            can possibly be a problem.
             For an illustration see the lower left part
             of fig. A.1.                                   For an illustration see the lower right part
                                                            of fig. A.1.


             A.3 ε-nearest neighboring      A.4 The silhouette coefficient
                 looks for neighbors within     determines how accurate
                 the radius ε for each          a given clustering is
                 data point
                                                         As we can see above, there is no easy an-
             Another approach of neighboring: here, swer for clustering problems. Each proce-
             the neighborhood detection does not use a dure described has very specific disadvan-
             fixed number k of neighbors but a radius ε, tages. In this respect it is useful to have



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        173
Appendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com




Figure A.1: Top left: our set of points. We will use this set to explore the different clustering
methods. Top right: k-means clustering. Using this procedure we chose k = 6. As we can
see, the procedure is not capable to recognize "clusters in clusters" (bottom left of the illustration).
Long "lines" of points are a problem, too: They would be recognized as many small clusters (if k
is sufficiently large). Bottom left: k-nearest neighboring. If k is selected too high (higher than
the number of points in the smallest cluster), this will result in cluster combinations shown in the
upper right of the illustration. Bottom right: ε-nearest neighboring. This procedure will cause
difficulties if ε is selected larger than the minimum distance between two clusters (see upper left of
the illustration), which will then be combined.




174                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
                dkriesel.com                                         A.5 Regional and online learnable fields


                a criterion to decide how good our clus-         Apparently, the whole term s(p) can only
                ter division is. This possibility is offered     be within the interval [−1; 1]. A value
                by the silhouette coefficient according          close to -1 indicates a bad classification of
                to [Kau90]. This coefficient measures how        p.
                well the clusters are delimited from each
                other and indicates if points may be as-         The silhouette coefficient S(P ) results
                signed to the wrong clusters.                    from the average of all values s(p):
  clustering

                Let P be a point cloud and p a point in
   quality is
                                                                                    1 X
                                                                              ) =
measureable
                                                                          S(P               s(p).     (A.4)
                P . Let c ⊆ P be a cluster within the                              |P | p∈P
                point cloud and p be part of this cluster,
                i.e. p ∈ c. The set of clusters is called C. As above the total quality of the clus-
                Summary:                                       ter division is expressed by the interval
                                   p∈c⊆P                       [−1; 1].
                applies.
                                                               As different clustering strategies with dif-
                To calculate the silhouette coefficient, we ferent characteristics have been presented
                initially need the average distance between now (lots of further material is presented
                point p and all its cluster neighbors. This in [DHS01]), as well as a measure to in-
                variable is referred to as a(p) and defined dicate the quality of an existing arrange-
                as follows:                                    ment of given data into clusters, I want
                                 1                             to introduce a clustering method based
                     a(p) =                   dist(p, q) (A.1) on an unsupervised learning neural net-
                                       X
                             |c| − 1 q∈c,q6=p
                                                               work [SGE05] which was published in 2005.
                                                               Like all the other methods this one may
                Furthermore, let b(p) be the average dis-
                                                               not be perfect but it eliminates large stan-
                tance between our point p and all points
                                                               dard weaknesses of the known clustering
                of the next cluster (g represents all clusters
                                                               methods
                except for c):

                                     1 X
                    b(p) = min              dist(p, q)   (A.2)
                           g∈C,g6=c |g|
                                        q∈g                      A.5 Regional and online
                The point p is classified well if the distance       learnable fields are a
                to the center of the own cluster is minimal          neural clustering strategy
                and the distance to the centers of the other
                clusters is maximal. In this case, the fol-
                lowing term provides a value close to 1:        The paradigm of neural networks, which I
                                                                want to introduce now, are the regional
                                                          (A.3) and online learnable fields, shortly re-
                                    b(p) − a(p)
                          s(p) =
                                  max{a(p), b(p)}               ferred to as ROLFs.



                D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                           175
              Appendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com


              A.5.1 ROLFs try to cover data with
                    neurons

              Roughly speaking, the regional and online
              learnable fields are a set K of neurons
     KI
              which try to cover a set of points as well
              as possible by means of their distribution
              in the input space. For this, neurons are
              added, moved or changed in their size dur-
   network
     covers   ing training if necessary. The parameters
point cloud   of the individual neurons will be discussed
              later.

              Definition A.2 (Regional and online
              learnable field).     A regional and on-
              line learnable field (abbreviated ROLF or               Figure A.2: Structure of a ROLF neuron.
              ROLF network) is a set K of neurons that
              are trained to cover a certain set in the
              input space as well as possible.

                                                                    ron. This particularly means that the neu-
                                                                    rons are capable to cover surfaces of differ-
              A.5.1.1 ROLF neurons feature a
                                                                    ent sizes.
                      position and a radius in the
                      input space                                   The radius of the perceptive surface is
                                                                    specified by r = ρ · σ (fig. A.2) with
              Here, a ROLF neuron k ∈ K has two                     the multiplier ρ being globally defined and
              parameters: Similar to the RBF networks,              previously specified for all neurons. Intu-
              it has a center ck , i.e. a position in the           itively, the reader will wonder what this
      cI
              input space.                                          multiplicator is used for. Its significance
                                                                    will be discussed later. Furthermore, the
              But it has yet another parameter: The ra-             following has to be observed: It is not nec-
              dius σ, which defines the radius of the per-          essary for the perceptive surface of the dif-
      σI                                                            ferent neurons to be of the same size.
              ceptive surface surrounding the neuron2 .
              A neuron covers the part of the input space
                                                           Definition A.3 (ROLF neuron). The pa-
              that is situated within this radius.
                                                           rameters of a ROLF neuron k are a center
    neuron    ck and σk are locally defined for each neu- ck and a radius σk .
 represents
    surface   2 I write "defines" and not "is" because the actual   Definition A.4 (Perceptive surface).
                radius is specified by σ · ρ.                       The perceptive surface of a ROLF neuron



              176                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
            dkriesel.com                                         A.5 Regional and online learnable fields


            k consists of all points within the radius is an accepting neuron k. Then the radius
            ρ · σ in the input space.                  moves towards ||p − ck || (i.e. towards the
                                                       distance between p and ck ) and the center
                                                       ck towards p. Additionally, let us define
            A.5.2 A ROLF learns unsupervised           the two learning rates ησ and ηc for radii
                     by presenting training            and centers.
                                                                                                                  Jησ , ηc
                    samples online
                                                             ck (t + 1) = ck (t) + ηc (p − ck (t))
            Like many other paradigms of neural net-         σk (t + 1) = σk (t) + ησ (||p − ck (t)|| − σk (t))
            works our ROLF network learns by receiv-
            ing many training samples p of a training        Note that here σk is a scalar while ck is a
            set P . The learning is unsupervised. For        vector in the input space.
            each training sample p entered into the net-
            work two cases can occur:                 Definition A.6 (Adapting a ROLF neu-
                                                      ron). A neuron k accepted by a point p is
             1. There is one accepting neuron k for p
                                                      adapted according to the following rules:
                or
             2. there is no accepting neuron at all.         ck (t + 1) = ck (t) + ηc (p − ck (t))      (A.5)
                                                           σk (t + 1) = σk (t) + ησ (||p − ck (t)|| − σk (t))
            If in the first case several neurons are suit-
                                                                                                      (A.6)
            able, then there will be exactly one ac-
            cepting neuron insofar as the closest neu-
            ron is the accepting one. For the accepting
                                                           A.5.2.2 The radius multiplier allows
            neuron k ck and σk are adapted.
                                                                     neurons to be able not only to
            Definition A.5 (Accepting neuron). The                   shrink
            criterion for a ROLF neuron k to be an
            accepting neuron of a point p is that the        Now we can understand the function of the
            point p must be located within the percep-       multiplier ρ: Due to this multiplier the per-
            tive surface of k. If p is located in the per-   ceptive surface of a neuron includes more
                                                                                                                  Jρ
            ceptive surfaces of several neurons, then        than only all points surrounding the neu-
            the closest neuron will be the accepting         ron in the radius σ. This means that due
            one. If there are several closest neurons,       to the aforementioned learning rule σ can-
            one can be chosen randomly.                      not only decrease but also increase.
                                                                                                                  so the
                                                                                                                  neurons
                                                     Definition A.7 (Radius multiplier). The                      can grow
            A.5.2.1 Both positions and radii are     radius multiplier ρ > 1 is globally defined
                    adapted throughout learning      and expands the perceptive surface of a
Adapting                                             neuron k to a multiple of σk . So it is en-
 existing   Let us assume that we entered a training sured that the radius σk cannot only de-
 neurons
            sample p into the network and that there crease but also increase.



            D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                              177
Appendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com


Generally, the radius multiplier is set to Mean σ: We select the mean σ of all neu-
values in the lower one-digit range, such        rons.
as 2 or 3.
                                            Currently, the mean-σ variant is the fa-
So far we only have discussed the case in vorite one although the learning procedure
the ROLF training that there is an accept- also works with the other ones. In the
ing neuron for the training sample p.       minimum-σ variant the neurons tend to
                                            cover less of the surface, in the maximum-
                                            σ variant they tend to cover more of the
A.5.2.3 As required, new neurons are        surface.
          generated
                                            Definition A.8 (Generating a ROLF neu-
This suggests to discuss the approach for ron). If a new ROLF neuron k is gener-
the case that there is no accepting neu- ated by entering a training sample p, then           initialization
ron.                                        ck is intialized with p and σk according to       of a
                                            one of the aforementioned strategies (init-       neurons
In this case a new accepting neuron k is σ, minimum-σ, maximum-σ, mean-σ).
generated for our training sample. The re-
sult is of course that ck and σk have to be The training is complete when after re-
initialized.                                peated randomly permuted pattern presen-
The initialization of ck can be understood tation no new neuron has been generated
intuitively: The center of the new neuron in an epoch and the positions of the neu-
is simply set on the training sample, i.e. rons barely change.

                  ck = p.
                                              A.5.3 Evaluating a ROLF
We generate a new neuron because there
is no neuron close to p – for logical reasons, The result of the training algorithm is that
we place the neuron exactly on p.              the training set is gradually covered well
                                               and precisely by the ROLF neurons and
But how to set a σ when a new neuron that a high concentration of points on a
is generated? For this purpose there exist spot of the input space does not automati-
different options:                             cally generate more neurons. Thus, a pos-
Init-σ: We always select a predefined sibly very large point cloud is reduced to
     static σ.                                 very few representatives (based on the in-
                                               put set).
Minimum σ: We take a look at the σ of
     each neuron and select the minimum. Then it is very easy to define the num-              cluster =
                                               ber of clusters: Two neurons are (accord-      connected
Maximum σ: We take a look at the σ of ing to the definition of the ROLF) con-                 neurons

     each neuron and select the maximum. nected when their perceptive surfaces over-



178                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                                        A.5 Regional and online learnable fields


              lap (i.e. some kind of nearest neighbor-
              ing is executed with the variable percep-
              tive surfaces). A cluster is a group of
              connected neurons or a group of points of
              the input space covered by these neurons
              (fig. A.3).
              Of course, the complete ROLF network
              can be evaluated by means of other clus-
              tering methods, i.e. the neurons can be
              searched for clusters. Particularly with
              clustering methods whose storage effort
              grows quadratic to |P | the storage effort
              can be reduced dramatically since gener-
              ally there are considerably less ROLF neu-
              rons than original data points, but the
              neurons represent the data points quite
              well.


              A.5.4 Comparison with popular
                    clustering methods

              It is obvious, that storing the neurons
              rather than storing the input points takes
              the biggest part of the storage effort of the
              ROLFs. This is a great advantage for huge
       less
   storage    point clouds with a lot of points.
    effort!
              Since it is unnecessary to store the en-
              tire point cloud, our ROLF, as a neural
              clustering method, has the capability to
              learn online, which is definitely a great ad-
              vantage. Furthermore, it can (similar to
              ε nearest neighboring or k nearest neigh-
              boring) distinguish clusters from enclosed
              clusters – but due to the online presenta-
                                                              Figure A.3: The clustering process. Top: the
 recognize
"cluster in   tion of the data without a quadratically
  clusters"   growing storage effort, which is by far the     input set, middle: the input space covered by
                                                              ROLF neurons, bottom: the input space only
              greatest disadvantage of the two neighbor-
                                                              covered by the neurons (representatives).
              ing methods.



              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          179
Appendix A Excursus: Cluster analysis and regional and online learnable fieldsdkriesel.com


Additionally, the issue of the size of the in-   at least with the mean-σ strategy – they
dividual clusters proportional to their dis-     are relatively robust after some training
tance from each other is addressed by us-        time.
ing variable perceptive surfaces - which is
                                              As a whole, the ROLF is on a par with
also not always the case for the two men-
                                              the other clustering methods and is par-
tioned methods.
                                              ticularly very interesting for systems with
The ROLF compares favorably with k- low storage capacity or huge data sets.
means clustering, as well: Firstly, it is un-
necessary to previously know the number
of clusters and, secondly, k-means cluster-
                                              A.5.6 Application examples
ing recognizes clusters enclosed by other
clusters as separate clusters.                A first application example could be find-
                                              ing color clusters in RGB images. Another
                                              field of application directly described in
A.5.5 Initializing radii, learning            the ROLF publication is the recognition of
        rates and multiplier is not           words  transferred into a 720-dimensional
        trivial                               feature  space. Thus, we can see that
                                              ROLFs are relatively robust against higher
Certainly, the disadvantages of the ROLF dimensions. Further applications can be
shall not be concealed: It is not always found in the field of analysis of attacks on
easy to select the appropriate initial value network systems and their classification.
for σ and ρ. The previous knowledge
about the data set can so to say be in-
cluded in ρ and the initial value of σ of the    Exercises
ROLF: Fine-grained data clusters should
use a small ρ and a small σ initial value. Exercise 18. Determine at least four
But the smaller the ρ the smaller, the     adaptation steps for one single ROLF neu-
chance that the neurons will grow if neces-ron k if the four patterns stated below
sary. Here again, there is no easy answer, are presented one after another in the in-
just like for the learning rates ηc and ησ .
                                           dicated order. Let the initial values for
                                           the ROLF neuron be ck = (0.1, 0.1) and
For ρ the multipliers in the lower single-
                                           σk = 1. Furthermore, let ηc = 0.5 and
digit range such as 2 or 3 are very popu-
                                           η = 0. Let ρ = 3.
lar. ηc and ησ successfully work with val- σ
ues about 0.005 to 0.1, variations during               P = {(0.1, 0.1);
run-time are also imaginable for this type
                                                          = (0.9, 0.1);
of network. Initial values for σ generally
depend on the cluster and data distribu-                  = (0.1, 0.9);
tion (i.e. they often have to be tested).                 = (0.9, 0.9)}.
But compared to wrong initializations –



180                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
Appendix B
Excursus: neural networks used for
prediction
                   Discussion of an application of neural networks: a look ahead into the future
                                                                                  of time series.




After discussing the different paradigms of     B.1 About time series
neural networks it is now useful to take a
look at an application of neural networks
                                                A time series is a series of values dis-
which is brought up often and (as we will
                                                cretized in time. For example, daily mea-
see) is also used for fraud: The applica-
                                                sured temperature values or other meteo-
tion of time series prediction. This ex-
                                                rological data of a specific site could be
cursus is structured into the description of
                                                represented by a time series. Share price
time series and estimations about the re-
                                                values also represent a time series. Often
quirements that are actually needed to pre-
                                                the measurement of time series is timely
dict the values of a time series. Finally, I
                                                equidistant, and in many time series the
will say something about the range of soft-
                                                future development of their values is very
ware which should predict share prices or
                                                interesting, e.g. the daily weather fore-
other economic characteristics by means of
                                                cast.
neural networks or other procedures.                                                                time
                                                                                                    series of
                                              Time series can also be values of an actu-            values
                                              ally continuous function read in a certain
This chapter should not be a detailed distance of time ∆t (fig. B.1 on the next
                                                                                                    J∆t
description but rather indicate some ap- page).
proaches for time series prediction. In this
                                              If we want to predict a time series, we will
respect I will again try to avoid formal def-
                                              look for a neural network that maps the
initions.
                                              previous series values to future develop-
                                              ments of the time series, i.e. if we know
                                              longer sections of the time series, we will



                                                                                             181
Appendix B Excursus: neural networks used for prediction                               dkriesel.com


                                                       have enough training samples. Of course,
                                                       these are not examples for the future to be
                                                       predicted but it is tried to generalize and
                                                       to extrapolate the past by means of the
                                                       said samples.

                                                       But before we begin to predict a time
                                                       series we have to answer some questions
                                                       about this time series we are dealing with
                                                       and ensure that it fulfills some require-
                                                       ments.

                                                        1. Do we have any evidence which sug-
                                                           gests that future values depend in any
                                                           way on the past values of the time se-
                                                           ries? Does the past of a time series
                                                           include information about its future?

                                                        2. Do we have enough past values of the
                                                           time series that can be used as train-
                                                           ing patterns?

                                                        3. In case of a prediction of a continuous
                                                           function: What must a useful ∆t look
                                                           like?

                                                       Now these questions shall be explored in
                                                       detail.
Figure B.1: A function x that depends on the
time is sampled at discrete time steps (time dis-      How much information about the future
cretized), this means that the result is a time        is included in the past values of a time se-
series. The sampled values are entered into a          ries? This is the most important question
neural network (in this example an SLP) which          to be answered for any time series that
shall learn to predict the future values of the time
                                                       should be mapped into the future. If the
series.
                                                       future values of a time series, for instance,
                                                       do not depend on the past values, then a
                                                       time series prediction based on them will
                                                       be impossible.

                                                       In this chapter, we assume systems whose
                                                       future values can be deduced from their
                                                       states – the deterministic systems. This



182                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                   B.2 One-step-ahead prediction


leads us to the question of what a system        B.2 One-step-ahead
state is.                                            prediction
A system state completely describes a sys-
tem for a certain point of time. The future
                                            The first attempt to predict the next fu-
of a deterministic system would be clearly
                                            ture value of a time series out of past val-
defined by means of the complete descrip-
                                            ues is called one-step-ahead prediction
tion of its current state.
                                            (fig. B.2 on the following page).
                                                                                                    predict
                                                                                                    the next
The problem in the real world is that such
                                             Such a predictor system receives the last
                                                                                                    value
a state concept includes all things that in-
                                             n observed state parts of the system as
fluence our system by any means.
                                             input and outputs the prediction for the
                                             next state (or state part). The idea of
In case of our weather forecast for a spe-
                                             a state space with predictable states is
cific site we could definitely determine
                                             called state space forecasting.
the temperature, the atmospheric pres-
sure and the cloud density as the mete-
orological state of the place at a time t. The aim of the predictor is to realize a
But the whole state would include signifi- function
cantly more information. Here, the world-
wide phenomena that control the weather           f (xt−n+1 , . . . , xt−1 , xt ) = x̃t+1 , (B.1)
would be interesting as well as small local
pheonomena such as the cooling system of which receives exactly n past values in or-
the local power plant.                        der to predict the future value. Predicted
                                              values shall be headed by a tilde (e.g. x̃)
So we shall note that the system state is de- to distinguish them from the actual future
                                                                                                    Jx̃
sirable for prediction but not always possi- values.
ble to obtain. Often only fragments of the
current states can be acquired, e.g. for a The most intuitive and simplest approach
weather forecast these fragments are the would be to find a linear combination
said weather data.
                                                x̃i+1 = a0 xi + a1 xi−1 + . . . + aj xi−j
However, we can partially overcome these                                                (B.2)
weaknesses by using not only one single
state (the last one) for the prediction, but that approximately fulfills our condi-
by using several past states. From this tions.
we want to derive our first prediction sys-
tem:
                                             Such a construction is called digital fil-
                                             ter. Here we use the fact that time series



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                             183
Appendix B Excursus: neural networks used for prediction                                   dkriesel.com


           xt−3          xt−2          xt−1           xt                             x̃t+1
                                                                                       K


                                                                .-+ predictor


Figure B.2: Representation of the one-step-ahead prediction. It is tried to calculate the future
value from a series of past values. The predicting element (in this case a neural network) is referred
to as predictor.




usually have a lot of past values so that we means of the delta rule provides results
can set up a series of equations1 :          very close to the analytical solution.

      xt = a0 xt−1 + . . . + aj xt−1−(n−1)
                                               Even if this approach often provides satis-
    xt−1 = a0 xt−2 + . . . + aj xt−2−(n−1)
                                               fying results, we have seen that many prob-
        ..
         .                               (B.3) lems cannot be solved by using a single-
                                               layer perceptron. Additional layers with
    xt−n = a0 xt−n + . . . + aj xt−n−(n−1)
                                               linear activation function are useless, as
Thus, n equations could be found for n un-     well, since a multilayer perceptron with
known coefficients and solve them (if pos- only linear activation functions can be re-
sible). Or another, better approach: we duced to a singlelayer perceptron. Such
could use m > n equations for n unknowns considerations lead to a non-linear ap-
in such a way that the sum of the mean proach.
squared errors of the already known pre-
diction is minimized. This is called mov- The multilayer perceptron and non-linear
ing average procedure.                               activation functions provide a universal
But this linear structure corresponds to a non-linear function approximator, i.e. we
singlelayer perceptron with a linear activa- can use an n-|H|-1-MLP for n n inputs out
tion function which has been trained by of the past. An RBF network could also be
means of data from the past (The experi- used. But remember that here the number
mental setup would comply with fig. B.1 n has to remain low since in RBF networks
on page 182). In fact, the training by high input dimensions are very complex to
                                                     realize. So if we want to include many past
1 Without going into detail, I want to remark that values, a multilayer perceptron will require
  the prediction becomes easier the more past values
  of the time series are available. I would like to
                                                     considerably less computational effort.
  ask the reader to read up on the Nyquist-Shannon
  sampling theorem




184                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
             dkriesel.com                       B.4 Additional optimization approaches for prediction


             B.3 Two-step-ahead                           B.4 Additional optimization
                 prediction                                   approaches for prediction

             What approaches can we use to to see far- The possibility to predict values far away
             ther into the future?                         in the future is not only important because
                                                           we try to look farther ahead into the fu-
                                                           ture. There can also be periodic time se-
             B.3.1 Recursive two-step-ahead                ries where other approaches are hardly pos-
                     prediction                            sible: If a lecture begins at 9 a.m. every
   predict                                                 Thursday, it is not very useful to know how
    future
             In order to extend the prediction to, for in- many people sat in the lecture room on
             stance, two time steps into the future, we Monday to predict the number of lecture
    values

             could perform two one-step-ahead predic- participants. The same applies, for ex-
             tions in a row (fig. B.3 on the following ample, to periodically occurring commuter
             page), i.e. a recursive two-step-ahead jams.
             prediction. Unfortunately, the value de-
             termined by means of a one-step-ahead
                                                          B.4.1 Changing temporal
             prediction is generally imprecise so that
                                                                  parameters
             errors can be built up, and the more pre-
             dictions are performed in a row the more
                                                          Thus, it can be useful to intentionally leave
             imprecise becomes the result.
                                                          gaps in the future values as well as in the
                                                          past values of the time series, i.e. to in-
             B.3.2 Direct two-step-ahead                  troduce the parameter ∆t which indicates
                     prediction                           which past value is used for prediction.
                                                          Technically speaking, we still use a one-       extent
                                                          step-ahead prediction only that we extend       input
             We have already guessed that there exists
                                                          the input space or train the system to pre-     period
             a better approach: Just like the system
                                                          dict values lying farther away.
             can be trained to predict the next value,
             we can certainly train it to predict the It is also possible to combine different ∆t:
    direct
prediction   next but one value. This means we di- In case of the traffic jam prediction for a
 is better   rectly train, for example, a neural network Monday the values of the last few days
             to look two time steps ahead into the fu- could be used as data input in addition to
             ture, which is referred to as direct two- the values of the previous Mondays. Thus,
             step-ahead prediction (fig. B.4 on the we use the last values of several periods,
             next page). Obviously, the direct two-step- in this case the values of a weekly and a
             ahead prediction is technically identical to daily period. We could also include an an-
             the one-step-ahead prediction. The only nual period in the form of the beginning of
             difference is the training.                  the holidays (for sure, everyone of us has



             D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      185
Appendix B Excursus: neural networks used for prediction                              dkriesel.com




                                                                        0 predictor
                                                                              O

                                                                                                  
 xt−3          xt−2          xt−1          xt                               x̃t+1              x̃t+2
                                                                              J


                                                     .-+ predictor



Figure B.3: Representation of the two-step-ahead prediction. Attempt to predict the second future
value out of a past value series by means of a second predictor and the involvement of an already
predicted value.




   xt−3          xt−2          xt−1             xt                          x̃t+1         x̃t+2
                                                                                           E


                                                       .-+ predictor



Figure B.4: Representation of the direct two-step-ahead prediction. Here, the second time step is
predicted directly, the first one is omitted. Technically, it does not differ from a one-step-ahead
prediction.




186                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
               dkriesel.com                                B.5 Remarks on the prediction of share prices


               already spent a lot of time on the highway discrete values – often, for example, in a
               because he forgot the beginning of the hol- daily rhythm (including the maximum and
               idays).                                     minimum values per day, if we are lucky)
                                                           with the daily variations certainly being
                                                           eliminated. But this makes the whole
               B.4.2 Heterogeneous prediction              thing even more difficult.

               Another prediction approach would be to         There are chartists, i.e. people who look
               predict the future values of a single time      at many diagrams and decide by means
               series out of several time series, if it is     of a lot of background knowledge and
               assumed that the additional time series         decade-long experience whether the equi-
         use
information    is related to the future of the first one       ties should be bought or not (and often
  outside of
               (heterogeneous one-step-ahead pre-              they are very successful).
 time series
               diction, fig. B.5 on the following page).
                                                               Apart from the share prices it is very in-
               If we want to predict two outputs of two        teresting to predict the exchange rates of
               related time series, it is certainly possible   currencies: If we exchange 100 Euros into
               to perform two parallel one-step-ahead pre-     Dollars, the Dollars into Pounds and the
               dictions (analytically this is done very of-    Pounds back into Euros it could be pos-
               ten because otherwise the equations would       sible that we will finally receive 110 Eu-
               become very confusing); or in case of           ros. But once found out, we would do this
               the neural networks an additional output        more often and thus we would change the
               neuron is attached and the knowledge of         exchange rates into a state in which such
               both time series is used for both outputs       an increasing circulation would no longer
               (fig. B.6 on the next page).                    be possible (otherwise we could produce
                                                               money by generating, so to speak, a finan-
               You’ll find more and more general material
                                                               cial perpetual motion machine.
               on time series in [WG94].
                                                               At the stock exchange, successful stock
                                                               and currency brokers raise or lower their
               B.5 Remarks on the                              thumbs – and thereby indicate whether in
                   prediction of share prices                  their opinion a share price or an exchange
                                                               rate will increase or decrease. Mathemat-
                                                               ically speaking, they indicate the first bit
               Many people observe the changes of a            (sign) of the first derivative of the ex-
               share price in the past and try to con-         change rate. In that way excellent world-
               clude the future from those values in or-       class brokers obtain success rates of about
               der to benefit from this knowledge. Share       70%.
               prices are discontinuous and therefore they
               are principally difficult functions. Further- In Great Britain, the heterogeneous one-
               more, the functions can only be used for step-ahead prediction was successfully



               D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                        187
Appendix B Excursus: neural networks used for prediction                                 dkriesel.com




          xt−3         xt−2          xt−1         xt                           x̃t+1
                                                                                 K


                                                           .0-1+3 predictor



          yt−3         yt−2          yt−1         yt


Figure B.5: Representation of the heterogeneous one-step-ahead prediction. Prediction of a time
series under consideration of a second one.




          xt−3         xt−2          xt−1         xt                           x̃t+1
                                                                                 K


                                                           .0-1+3 predictor


                                                                                     
          yt−3         yt−2          yt−1         yt                           ỹt+1


   Figure B.6: Heterogeneous one-step-ahead prediction of two time series at the same time.




188                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
              dkriesel.com                               B.5 Remarks on the prediction of share prices


              used to increase the accuracy of such pre-     Again and again some software appears
              dictions to 76%: In addition to the time       which uses scientific key words such as
              series of the values indicators such as the    ”neural networks” to purport that it is ca-
              oil price in Rotterdam or the US national      pable to predict where share prices are go-
              debt were included.                            ing. Do not buy such software! In addi-
                                                             tion to the aforementioned scientific exclu-
              This is just an example to show the mag-       sions there is one simple reason for this:
              nitude of the accuracy of stock-exchange       If these tools work – why should the man-
              evaluations, since we are still talking only   ufacturer sell them? Normally, useful eco-
              about the first bit of the first derivation!   nomic knowledge is kept secret. If we knew
              We still do not know how strong the ex-        a way to definitely gain wealth by means
              pected increase or decrease will be and        of shares, we would earn our millions by
              also whether the effort will pay off: Prob-    using this knowledge instead of selling it
              ably, one wrong prediction could nullify       for 30 euros, wouldn’t we?
              the profit of one hundred correct predic-
              tions.

              How can neural networks be used to pre-
              dict share prices? Intuitively, we assume
              that future share prices are a function of
              the previous share values.

              But this assumption is wrong: Share
              prices are no function of their past val-
              ues, but a function of their assumed fu-
share price
function of   ture value. We do not buy shares be-
  assumed     cause their values have been increased
              during the last days, but because we be-
     future
     value!
              lieve that they will futher increase tomor-
              row. If, as a consequence, many people
              buy a share, they will boost the price.
              Therefore their assumption was right – a
              self-fulfilling prophecy has been gener-
              ated, a phenomenon long known in eco-
              nomics.

              The same applies the other way around:
              We sell shares because we believe that to-
              morrow the prices will decrease. This will
              beat down the prices the next day and gen-
              erally even more the day after the next.



              D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       189
           Appendix C
           Excursus: reinforcement learning
                              What if there were no training samples but it would nevertheless be possible
                              to evaluate how well we have learned to solve a problem? Let us examine a
                                  learning paradigm that is situated between supervised and unsupervised
                                                                                                 learning.




           I now want to introduce a more exotic ap-      While it is generally known that pro-
           proach of learning – just to leave the usual   cedures such as backpropagation cannot
           paths. We know learning procedures in          work in the human brain itself, reinforce-
           which the network is exactly told what to      ment learning is usually considered as be-
           do, i.e. we provide exemplary output val-      ing biologically more motivated.
           ues. We also know learning procedures
                                                          The term reinforcement learning
           like those of the self-organizing maps, into
                                                          comes from cognitive science and
           which only input values are entered.
                                                          psychology and it describes the learning
           Now we want to explore something in-           system of carrot and stick, which occurs
           between: The learning paradigm of rein-        everywhere in nature, i.e. learning by
           forcement learning – reinforcement learn-      means of good or bad experience, reward
           ing according to Sutton and Barto              and punishment. But there is no learning
           [SB98].                                        aid that exactly explains what we have
                                                          to do: We only receive a total result
           Reinforcement learning in itself is no neu-
                                                          for a process (Did we win the game of
           ral network but only one of the three learn-
                                                          chess or not? And how sure was this
           ing paradigms already mentioned in chap-
                                                          victory?), but no results for the individual
           ter 4. In some sources it is counted among
                                                          intermediate steps.
           the supervised learning procedures since a
      no   feedback is given. Due to its very rudimen-    For example, if we ride our bike with worn
 samples   tary feedback it is reasonable to separate     tires and at a speed of exactly 21, 5 kmh
     but
feedback
           it from the supervised learning procedures     through a turn over some sand with a
           – apart from the fact that there are no        grain size of 0.1mm, on the average, then
           training samples at all.                       nobody could tell us exactly which han-



                                                                                                      191
Appendix C Excursus: reinforcement learning                                    dkriesel.com


dlebar angle we have to adjust or, even        interaction between an agent and an envi-
worse, how strong the great number of          ronmental system (fig. C.2).
muscle parts in our arms or legs have to
contract for this. Depending on whether        The agent shall solve some problem. He
we reach the end of the curve unharmed or      could, for instance, be an autonomous
not, we soon have to face the learning expe-   robot that shall avoid obstacles. The
rience, a feedback or a reward, be it good     agent performs some actions within the
or bad. Thus, the reward is very simple        environment and in return receives a feed-
- but on the other hand it is considerably     back from the environment, which in the
easier to obtain. If we now have tested dif-   following is called reward. This cycle of ac-
ferent velocities and turning angles often     tion and reward is characteristic for rein-
enough and received some rewards, we will      forcement learning. The agent influences
get a feel for what works and what does        the system, the system provides a reward
not. The aim of reinforcement learning is      and then changes.
to maintain exactly this feeling.              The reward is a real or discrete scalar
                                               which describes, as mentioned above, how
Another      example     for    the    quasi-
                                               well we achieve our aim, but it does not
impossibility to achieve a sort of cost or
                                               give any guidance how we can achieve it.
utility function is a tennis player who
                                               The aim is always to make the sum of
tries to maximize his athletic success
                                               rewards as high as possible on the long
on the long term by means of complex
                                               term.
movements and ballistic trajectories in
the three-dimensional space including the
wind direction, the importance of the C.1.1 The gridworld
tournament, private factors and many
more.
                                              As a learning example for reinforcement
To get straight to the point: Since we learning I would like to use the so-called
receive only little feedback, reinforcement gridworld. We will see that its struc-
learning often means trial and error – and ture is very simple and easy to figure out
therefore it is very slow.                    and therefore reinforcement is actually not
                                              necessary. However, it is very suitable
                                                                                               simple
                                              for representing the approach of reinforce-      examplary

C.1 System structure                          ment learning. Now let us exemplary de-          world

                                              fine the individual components of the re-
                                              inforcement system by means of the grid-
Now we want to briefly discuss different world. Later, each of these components
sizes and components of the system. We will be examined more exactly.
will define them more precisely in the fol-
lowing sections. Broadly speaking, rein- Environment: The gridworld (fig. C.1 on
forcement learning represents the mutual           the facing page) is a simple, discrete



192                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                 C.1 System structure


    world in two dimensions which in the
    following we want to use as environ-
    mental system.

Agent: As an Agent we use a simple robot                          ×
   being situated in our gridworld.

State space: As we can see, our gridworld
    has 5 × 7 fields with 6 fields being un-
    accessible. Therefore, our agent can
    occupy 29 positions in the grid world.
    These positions are regarded as states
    for the agent.                                                ×
Action space: The actions are still miss-
    ing. We simply define that the robot
    could move one field up or down, to
    the right or to the left (as long as
    there is no obstacle or the edge of our
                                            Figure C.1: A graphical representation of our
    gridworld).                             gridworld. Dark-colored cells are obstacles and
Task: Our agent’s task is to leave the grid-   therefore inaccessible. The exit is located on the
                                               right side of the light-colored field. The symbol
    world. The exit is located on the right
                                               × marks the starting position of our agent. In
    of the light-colored field.                the upper part of our figure the door is open, in
                                               the lower part it is closed.
Non-determinism: The two obstacles can
   be connected by a "door". When the
   door is closed (lower part of the illus-
   tration), the corresponding field is in-
   accessible. The position of the door
   cannot change during a cycle but only                                       ? Agent
   between the cycles.

We now have created a small world that
                                                    reward / new situation                   action


will accompany us through the following
                                                                                         _
learning strategies and illustrate them.                                     environment


C.1.2 Agent und environment                    Figure C.2: The agent performs some actions
                                               within the environment and in return receives a
                                               reward.
Our aim is that the agent learns what hap-
pens by means of the reward. Thus, it



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                       193
                Appendix C Excursus: reinforcement learning                                   dkriesel.com


                is trained over, of and by means of a dy-     described as a mapping of the situation
                namic system, the environment, in order       space S into the action space A(st ). The
                to reach an aim. But what does learning       meaning of situations st will be defined
                mean in this context?                         later and should only indicate that the ac-
                                                              tion space depends on the current situa-
                The agent shall learn a mapping of sit-
       agent                                                  tion.
      acts in   uations to actions (called policy), i.e. it
environment     shall learn what to do in which situation                  Agent: S → A(st )           (C.1)
                to achieve a certain (given) aim. The aim
                is simply shown to the agent by giving an Definition C.2 (Environment). The en-
                award for the achievement.                    vironment represents a stochastic map-
                                                              ping of an action A in the current situa-
                Such an award must not be mistaken for
                                                              tion st to a reward rt and a new situation
                the reward – on the agent’s way to the
                                                              s .
                solution it may sometimes be useful to t+1
                receive a smaller award or a punishment
                                                                Environment: S × A → P (S × rt ) (C.2)
                when in return the longterm result is max-
                imum (similar to the situation when an
                investor just sits out the downturn of the C.1.3 States, situations and actions
                share price or to a pawn sacrifice in a chess
                game). So, if the agent is heading into
                                                              As already mentioned, an agent can be in
                the right direction towards the target, it
                                                              different states: In case of the gridworld,
                receives a positive reward, and if not it re-
                                                              for example, it can be in different positions
                ceives no reward at all or even a negative
                                                              (here we get a two-dimensional state vec-
                reward (punishment). The award is, so to
                                                              tor).
                speak, the final sum of all rewards – which
                is also called return.                        For an agent is ist not always possible to
                After having colloquially named all the ba- realize all information about its current
                sic components, we want to discuss more state so that we have to introduce the term
                precisely which components can be used to situation. A situation is a state from the
                make up our abstract reinforcement learn- agent’s point of view, i.e. only a more or
                ing system.                                   less precise approximation of a state.

                In the gridworld: In the gridworld, the Therefore, situations generally do not al-
                                                        low to clearly "predict" successor situa-
                agent is a simple robot that should find the
                exit of the gridworld. The environment  tions – even with a completely determin-
                                                        istic system this may not be applicable.
                is the gridworld itself, which is a discrete
                gridworld.                              If we knew all states and the transitions
                                                        between them exactly (thus, the complete
                Definition C.1 (Agent). In reinforce- system), it would be possible to plan op-
                ment learning the agent can be formally timally and also easy to find an optimal



                194                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                             C.1 System structure


policy (methods are provided, for example, Definition C.4 (Situation). Situations
by dynamic programming).                   st (here at time t) of a situation space
                                           S are the agent’s limited, approximate
                                                                                                    Jst
Now we know that reinforcement learning                                                             JS
                                           knowledge about its state. This approx-
is an interaction between the agent and
                                           imation (about which the agent cannot
the system including actions at and sit-
                                           even know how good it is) makes clear pre-
uations st . The agent cannot determine
                                           dictions impossible.
by itself whether the current situation is
good or bad: This is exactly the reason Definition C.5 (Action). Actions at can
why it receives the said reward from the be performed by the agent (whereupon it                    Jat
environment.                               could be possible that depending on the
                                           situation another action space A(S) ex-
In the gridworld: States are positions                                                              JA(S)
                                           ists). They cause state transitions and
where the agent can be situated. Sim-
                                           therefore a new situation from the agent’s
ply said, the situations equal the states
                                           point of view.
in the gridworld. Possible actions would
be to move towards north, south, east or
west.                                      C.1.4 Reward and return
Situation and action can be vectorial, the
reward is always a scalar (in an extreme         As in real life it is our aim to receive
case even only a binary value) since the         an award that is as high as possible, i.e.
aim of reinforcement learning is to get          to maximize the sum of the expected re-
along with little feedback. A complex vec-       wards r, called return R, on the long
torial reward would equal a real teaching        term. For finitely many time steps1 the
input.                                           rewards can simply be added:

By the way, the cost function should be                   Rt = rt+1 + rt+2 + . . .          (C.3)
minimized, which would not be possible,                           ∞
                                                             =                              (C.4)
                                                                 X
however, with a vectorial reward since we                             rt+x
do not have any intuitive order relations                       x=1

in multi-dimensional space, i.e. we do not Certainly, the return is only estimated
directly know what is better or worse.       here (if we knew all rewards and therefore
                                             the return completely, it would no longer
Definition C.3 (State). Within its en-
                                             be necessary to learn).
vironment the agent is in a state. States
contain any information about the agent Definition C.6 (Reward). A reward rt is
within the environmental system. Thus, a scalar, real or discrete (even sometimes                   Jrt
it is theoretically possible to clearly pre- only binary) reward or punishment which
dict a successor state to a performed ac-
                                             1 In practice, only finitely many time steps will be
tion within a deterministic system out of      possible, even though the formulas are stated with
this godlike state knowledge.                  an infinite sum in the first place




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                              195
       Appendix C Excursus: reinforcement learning                                        dkriesel.com


       the environmental system returns to the Thus, we divide the timeline into
       agent as reaction to an action.                episodes. Usually, one of the two meth-
                                                      ods is used to limit the sum, if not both
       Definition C.7 (Return). The return Rt methods together.
       is the accumulation of all received rewards
Rt I                                                  As in daily living we try to approximate
       until time t.
                                                      our current situation to a desired state.
                                                      Since it is not mandatory that only the
       C.1.4.1 Dealing with long periods of           next expected reward but the expected to-
                 time                                 tal sum decides what the agent will do, it
                                                      is also possible to perform actions that, on
                                                      short notice, result in a negative reward
       However, not every problem has an ex-
                                                      (e.g. the pawn sacrifice in a chess game)
       plicit target and therefore a finite sum (e.g.
                                                      but will pay off later.
       our agent can be a robot having the task
       to drive around again and again and to
       avoid obstacles). In order not to receive a
                                                      C.1.5 The policy
       diverging sum in case of an infinite series
       of reward estimations a weakening factor
       0 < γ < 1 is used, which weakens the in- After having considered and formalized
γI
       fluence of future rewards. This is not only some system components of reinforcement
       useful if there exists no target but also if learning the actual aim is still to be dis-
       the target is very far away:                   cussed:

         Rt = rt+1 + γ 1 rt+2 + γ 2 rt+3 + . . . (C.5) During reinforcement learning the agent
                                                       learns a policy
               ∞                                                                                         JΠ
             =                                      (C.6)
                 X
                       γ x−1 rt+x
                 x=1                                                     Π : S → P (A),

       The farther the reward is away, the smaller Thus, it continuously adjusts a mapping
       is the influence it has in the agent’s deci- of the situations to the probabilities P (A),
       sions.                                        with which any action A is performed in
                                                     any situation S. A policy can be defined
       Another possibility to handle the return as a strategy to select actions that would
       sum would be a limited time horizon maximize the reward in the long term.
       τ so that only τ many following rewards
 τI
       rt+1 , . . . , rt+τ are regarded:             In the gridworld: In the gridworld the pol-
                                                     icy is the strategy according to which the
               Rt = rt+1 + . . . + γ τ −1 rt+τ (C.7) agent tries to exit the gridworld.
                       τ
                 =                                  (C.8)   Definition C.8 (Policy). The policy Π
                       X
                             γ x−1 rt+x
                       x=1                                  s a mapping of situations to probabilities



       196                           D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                               C.1 System structure


to perform every action out of the action in a manner of speaking. Here, the envi-
space. So it can be formalized as          ronment influences our action or the agent
                                           responds to the input of the environment,
              Π : S → P (A).         (C.9) respectively, as already illustrated in fig.
                                           C.2. A closed-loop policy, so to speak, is
Basically, we distinguish between two pol- a reactive plan to map current situations
icy paradigms: An open loop policy rep- to actions to be performed.
resents an open control chain and creates             In the gridworld: A closed-loop policy
out of an initial situation s0 a sequence of          would be responsive to the current posi-
actions a0 , a1 , . . . with ai 6= ai (si ); i > 0.   tion and choose the direction according to
Thus, in the beginning the agent develops             the action. In particular, when an obsta-
a plan and consecutively executes it to the           cle appears dynamically, such a policy is
end without considering the intermediate              the better choice.
situations (therefore ai 6= ai (si ), actions af-
ter a0 do not depend on the situations).     When selecting the actions to be per-
                                             formed, again two basic strategies can be
In the gridworld: In the gridworld, an examined.
open-loop policy would provide a precise
direction towards the exit, such as the way
from the given starting position to (in ab- C.1.5.1 Exploitation vs. exploration
breviations of the directions) EEEEN.
                                             As in real life, during reinforcement learn-
So an open-loop policy is a sequence of ing often the question arises whether the
actions without interim feedback. A se- exisiting knowledge is only willfully ex-
quence of actions is generated out of a ploited or new ways are also explored.
starting situation. If the system is known Initially, we want to discuss the two ex-
well and truly, such an open-loop policy tremes:
can be used successfully and lead to use-                                                          research

ful results. But, for example, to know the A greedy policy always chooses the way
                                                                                                   or safety?

chess game well and truly it would be nec- of the highest reward that can be deter-
essary to try every possible move, which mined in advance, i.e. the way of the high-
would be very time-consuming. Thus, for est known reward. This policy represents
such problems we have to find an alterna- the exploitation approach and is very
tive to the open-loop policy, which incorpo- promising when the used system is already
rates the current situations into the action known.
plan:
                                             In contrast to the exploitation approach it
A closed loop policy is a closed loop, a is the aim of the exploration approach
function                                     to explore a system as detailed as possible
                                             so that also such paths leading to the tar-
        Π : si → ai with ai = ai (si ),      get can be found which may be not very



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                             197
Appendix C Excursus: reinforcement learning                                dkriesel.com


promising at first glance but are in fact he leaves of such a tree are the end situ-
very successful.                           ations of the system. The exploration ap-
                                           proach would search the tree as thoroughly
Let us assume that we are looking for the as possible and become acquainted with all
way to a restaurant, a safe policy would leaves. The exploitation approach would
be to always take the way we already unerringly go to the best known leave.
know, not matter how unoptimal and long
it may be, and not to try to explore bet- Analogous to the situation tree, we also
ter ways. Another approach would be to can create an action tree. Here, the re-
explore shorter ways every now and then, wards for the actions are within the nodes.
even at the risk of taking a long time and Now we have to adapt from daily life how
being unsuccessful, and therefore finally we learn exactly.
having to take the original way and arrive
too late at the restaurant.

In reality, often a combination of both        C.2.1 Rewarding strategies
methods is applied: In the beginning of
the learning process it is researched withInteresting and very important is the ques-
a higher probability while at the end moretion for what a reward and what kind of
existing knowledge is exploited. Here, a  reward is awarded since the design of the
static probability distribution is also pos-
                                          reward significantly controls system behav-
sible and often applied.                  ior. As we have seen above, there gener-
                                          ally are (again as in daily life) various ac-
In the gridworld: For finding the way in
                                          tions that can be performed in any situa-
the gridworld, the restaurant example ap-
                                          tion. There are different strategies to eval-
plies equally.
                                          uate the selected situations and to learn
                                          which series of actions would lead to the
                                          target. First of all, this principle should
C.2 Learning process                      be explained in the following.

                                             We now want to indicate some extreme
Let us again take a look at daily life. Ac- cases as design examples for the reward:
tions can lead us from one situation into
different subsituations, from each subsit- A rewarding similar to the rewarding in a
uation into further sub-subsituations. In chess game is referred to as pure delayed
a sense, we get a situation tree where reward: We only receive the reward at
links between the nodes must be consid- the end of and not during the game. This
ered (often there are several ways to reach method is always advantageous when we
a situation – so the tree could more accu- finally can say whether we were succesful
rately be referred to as a situation graph). or not, but the interim steps do not allow



198                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                        C.2 Learning process


an estimation of our situation. If we win, robot but unfortunately was not intended
then                                       to do so.

                rt = 0 ∀t < τ        (C.10)  Furthermore, we can show that especially
                                             small tasks can be solved better by means
as well as rτ = 1. If we lose, then rτ = −1. of negative rewards while positive, more
With this rewarding strategy a reward is differentiated rewards are useful for large,
only returned by the leaves of the situation complex tasks.
tree.                                        For our gridworld we want to apply the
Pure negative reward: Here,                   pure negative reward strategy: The robot
                                              shall find the exit as fast as possible.
               rt = −1 ∀t < τ.       (C.11)

This system finds the most rapid way to       C.2.2 The state-value function
reach the target because this way is auto-
matically the most favorable one in respect   Unlike our agent we have a godlike view        state
of the reward. The agent receives punish-     of our gridworld so that we can swiftly de-    evaluation
ment for anything it does – even if it does   termine which robot starting position can
nothing. As a result it is the most inex-     provide which optimal return.
pensive method for the agent to reach the     In figure C.3 on the next page these opti-
target fast.                                  mal returns are applied per field.
Another strategy is the avoidance strat- In the gridworld: The state-value function
egy: Harmful situations are avoided. for our gridworld exactly represents such
Here,                                     a function per situation (= position) with
                                          the difference being that here the function
              rt ∈ {0, −1},        (C.12)
                                          is unknown and has to be learned.
Most situations do not receive any reward,    Thus, we can see that it would be more
only a few of them receive a negative re-     practical for the robot to be capable to
ward. The agent agent will avoid getting      evaluate the current and future situations.
too close to such negative situations         So let us take a look at another system
                                              component of reinforcement learning: the
Warning: Rewarding strategies can have
                                              state-value function V (s), which with
unexpected consequences. A robot that is
                                              regard to a policy Π is often called VΠ (s).
told "have it your own way but if you touch
                                              Because whether a situation is bad often
an obstacle you will be punished" will sim-
                                              depends on the general behavior Π of the
ply stand still. If standing still is also pun-
                                              agent.
ished, it will drive in small circles. Recon-
sidering this, we will understand that this A situation being bad under a policy that
behavior optimally fulfills the return of the is searching risks and checking out limits



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                      199
          Appendix C Excursus: reinforcement learning                                      dkriesel.com


                      -6    -5   -4    -3    -2         EΠ denotes the set of the expected returns
                      -7                     -1         under Π and the current situation st .
                      -6    -5   -4    -3    -2
                      -7    -6   -5          -3                   VΠ (s) = EΠ {Rt |s = st }
                      -8    -7   -6          -4
                      -9    -8   -7          -5         Definition C.9 (State-value function).
                                                        The state-value function VΠ (s) has the
                     -10    -9   -8    -7    -6
                                                        task of determining the value of situations
                    -6     -5    -4     -3    -2        under a policy, i.e. to answer the agent’s
                                                        question of whether a situation s is good
                    -7                        -1
                                                        or bad or how good or bad it is. For this
                    -8      -9   -10          -2
                                                        purpose it returns the expectation of the
                    -9     -10   -11          -3
                                                        return under the situation:
                   -10     -11   -10          -4
                   -11     -10    -9          -5               VΠ (s) = EΠ {Rt |s = st }         (C.13)
                   -10      -9    -8    -7    -6
                                                        The optimal state-value function is called
          Figure C.3: Representation of each optimal re- V ∗ (s).
                                                            Π
          turn per field in our gridworld by means of pure                                                JVΠ∗ (s)
          negative reward awarding, at the top with an
          open and at the bottom with a closed door.       Unfortunaely, unlike us our robot does not
                                                       have a godlike view of its environment. It
                                                       does not have a table with optimal returns
                                                       like the one shown above to orient itself.
                                                       The aim of reinforcement learning is that
                                                       the robot generates its state-value func-
          would be, for instance, if an agent on a bi- tion bit by bit on the basis of the returns of
          cycle turns a corner and the front wheel many trials and approximates the optimal
          begins to slide out. And due to its dare- state-value function V ∗ (if there is one).
          devil policy the agent would not brake in
                                                       In this context I want to introduce two
          this situation. With a risk-aware policy
                                                       terms closely related to the cycle between
          the same situations would look much bet-
                                                       state-value function and policy:
          ter, thus it would be evaluated higher by
          a good state-value function
                                                        C.2.2.1 Policy evaluation
          VΠ (s) simply returns the value the current
VΠ (s)I   situation s has for the agent under policy    Policy evaluation is the approach to try
          Π. Abstractly speaking, according to the      a policy a few times, to provide many re-
          above definitions, the value of the state-    wards that way and to gradually accumu-
          value function corresponds to the return      late a state-value function by means of
          Rt (the expected value) of a situation st .   these rewards.



          200                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                              C.2 Learning process


                                                  C.2.3 Monte Carlo method
                          )
                 V i          Π
                                                The easiest approach to accumulate a
                                              state-value function is mere trial and er-
                  V∗        Π∗                  ror. Thus, we select a randomly behaving
                                                policy which does not consider the accumu-
Figure C.4: The cycle of reinforcement learning lated state-value function for its random
which ideally leads to optimal Π∗ and V ∗ .     decisions. It can be proved that at some
                                                point we will find the exit of our gridworld
                                                by chance.
                                                  Inspired by random-based games of chance
C.2.2.2 Policy improvement                        this approach is called Monte Carlo
                                                  method.
Policy improvement means to improve               If we additionally assume a pure negative
a policy itself, i.e. to turn it into a new and   reward, it is obvious that we can receive
better one. In order to improve the policy        an optimum value of −6 for our starting
we have to aim at the return finally having       field in the state-value function. Depend-
a larger value than before, i.e. until we         ing on the random way the random policy
have found a shorter way to the restaurant        takes values other (smaller) than −6 can
and have walked it successfully                   occur for the starting field. Intuitively, we
                                                  want to memorize only the better value for
The principle of reinforcement learning is        one state (i.e. one field). But here caution
to realize an interaction. It is tried to eval-   is advised: In this way, the learning proce-
uate how good a policy is in individual           dure would work only with deterministic
situations. The changed state-value func-         systems. Our door, which can be open or
tion provides information about the sys-          closed during a cycle, would produce oscil-
tem with which we again improve our pol-          lations for all fields and such oscillations
icy. These two values lift each other, which      would influence their shortest way to the
can mathematically be proved, so that the         target.
final result is an optimal policy Π∗ and an
optimal state-value function V ∗ (fig. C.4). With the Monte Carlo method we prefer
                                                                          2
This cycle sounds simple but is very time- to use the learning rule
consuming.
                                                V (st )new = V (st )alt + α(Rt − V (st )alt ),
At first, let us regard a simple, random pol- in which the update of the state-value func-
icy by which our robot could slowly fulfill tion is obviously influenced by both the
and improve its state-value function with-
out any previous knowledge.                    2 The learning rule is, among others, derived by
                                                    means of the Bellman equation, but this deriva-
                                                    tion is not discussed in this chapter.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                               201
     Appendix C Excursus: reinforcement learning                                              dkriesel.com


     old state value and the received return (α
     is the learning rate). Thus, the agent gets
αI
     some kind of memory, new findings always
     change the situation value just a little bit.
     An exemplary learning step is shown in                                                  -1
     fig. C.5.                                                       -6    -5    -4    -3    -2

     In this example, the computation of the
     state value was applied for only one single
     state (our initial state). It should be ob-
     vious that it is possible (and often done)
     to train the values for the states visited in-
     between (in case of the gridworld our ways                                               -1
     to the target) at the same time. The result                   -14    -13    -12          -2
     of such a calculation related to our exam-
                                                                                 -11          -3
     ple is illustrated in fig. C.6 on the facing
                                                                                 -10          -4
     page.
                                                                                  -9          -5
     The Monte Carlo method seems to be                                           -8    -7    -6
     suboptimal and usually it is significantly
     slower than the following methods of re-
     inforcement learning. But this method is
     the only one for which it can be mathemat-
     ically proved that it works and therefore                            -10
     it is very useful for theoretical considera-
     tions.

     Definition C.10 (Monte Carlo learning).
      Actions are randomly performed regard-
     less of the state-value function and in the        Figure C.5: Application of the Monte Carlo
     long term an expressive state-value func-          learning rule with a learning rate of α = 0.5.
                                                        Top: two exemplary ways the agent randomly
     tion is accumulated by means of the fol-
                                                        selects are applied (one with an open and one
     lowing learning rule.                              with a closed door). Bottom: The result of the
                                                        learning rule for the value of the initial state con-
       V (st )new = V (st )alt + α(Rt − V (st )alt ),   sidering both ways. Due to the fact that in the
                                                        course of time many different ways are walked
                                                        given a random policy, a very expressive state-
     C.2.4 Temporal difference learning                 value function is obtained.

     Most of the learning is the result of ex-
     periences; e.g. walking or riding a bicycle



     202                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                     C.2 Learning process


                                              the temporal difference learning (abbre-
                                          -1  viated: TD learning), does the same by
          -10 -9 -8 -3                    -2  training VΠ (s) (i.e. the agent learns to esti-
                     -11                  -3  mate which situations are worth a lot and
                     -10                  -4  which are not). Again the current situa-
                      -9                  -5  tion is identified with st , the following sit-
                      -8 -7               -6  uations with st+1 and so on. Thus, the
                                              learning formula for the state-value func-
Figure C.6: Extension of the learning example tion VΠ (st ) is
in fig. C.5 in which the returns for intermedi-
ate states are also used to accumulate the state-       V (st )new =V (st )
value function. Here, the low value on the door
field can be seen very well: If this state is possi-                 + α(rt+1 + γV (st+1 ) − V (st ))
ble, it must be very positive. If the door is closed,
                                                                       |                {z                }
                                                                              change of previous value

this state is impossible.
                                                        We can see that the change in value of the
                                                        current situation st , which is proportional
                     Evaluation                         to the learning rate α, is influenced by
                                  !
                Πa                    Q                   . the received reward rt+1 ,

                 policy improvement
                                                          . the previous return weighted with a
                                                            factor γ of the following situation
                                                            V (st+1 ),
Figure C.7: We try different actions within the
environment and as a result we learn and improve          . the previous value of the situation
the policy.                                                 V (st ).

                                             Definition C.11 (Temporal difference
                                             learning).     Unlike the Monte Carlo
                                             method, TD learning looks ahead by re-
without getting injured (or not), even men- garding the following situation st+1 . Thus,
tal skills like mathematical problem solv- the learning rule is given by
ing benefit a lot from experience and sim-
ple trial and error. Thus, we initialize our V (st )new =V (st )                          (C.14)
policy with arbitrary values – we try, learn             + α(rt+1 + γV (st+1 ) − V (st )) .
and improve the policy due to experience
                                                            |               {z                }
                                                                 change of previous value

(fig. C.7). In contrast to the Monte Carlo
method we want to do this in a more di-
rected manner.                               C.2.5 The action-value function

Just as we learn from experience to re- Analogous to the state-value function
act on different situations in different ways VΠ (s), the action-value function                                action
                                                                                                               evaluation



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                          203
              Appendix C Excursus: reinforcement learning                                            dkriesel.com


                                                                 C.2.6 Q learning
                             0
                             ×    +1
                                                                 This implies QΠ (s, a) as learning fomula
                             -1                                  for the action-value function, and – analo-
                                                                 gously to TD learning – its application is
                                                                 called Q learning:


              Figure C.8: Exemplary values of an action-
              value function for the position ×. Moving right, Q(st , a)new =Q(st , a)
              one remains on the fastest way towards the tar-                + α(rt+1 + γ max Q(st+1 , a) −Q(st , a)) .
              get, moving up is still a quite fast way, moving
                                                                                            a
                                                                                          |       {z        }
              down is not a good way at all (provided that the                              greedy strategy

              door is open for all cases).
                                                                               |                  {z                }
                                                                                       change of previous value


                                                           Again we break down the change of the
                                                           current action value (proportional to the
                                                           learning rate α) under the current situa-
              QΠ (s, a) is another system component of tion. It is influenced by
QΠ (s, a)I    reinforcement learning, which evaluates a
              certain action a under a certain situation     . the received reward rt+1 ,
              s and the policy Π.
                                                             . the maximum action over the follow-
              In the gridworld: In the gridworld, the           ing actions weighted with γ (Here, a
              action-value function tells us how good it        greedy strategy is applied since it can
              is to move from a certain field into a cer-       be assumed that the best known ac-
              tain direction (fig. C.8).                        tion is selected. With TD learning,
                                                                on the other hand, we do not mind to
              Definition C.12 (Action-value function).          always get into the best known next
               Like the state-value function, the action-       situation.),
              value function QΠ (st , a) evaluates certain
              actions on the basis of certain situations     . the previous value of the action under
              under a policy. The optimal action-value          our situation st known as Q(st , a) (re-
              function is called Q∗Π (st , a).                  member that this is also weighted by
Q∗Π (s, a)I                                                     means of α).
              As shown in fig. C.9, the actions are per-
                                                                 Usually, the action-value function learns
              formed until a target situation (here re-
                                                                 considerably faster than the state-value
              ferred to as sτ ) is achieved (if there exists a
                                                                 function. But we must not disregard that
              target situation, otherwise the actions are
                                                                 reinforcement learning is generally quite
              simply performed again and again).
                                                                 slow: The system has to find out itself
                                                                 what is good. But the advantage of Q



              204                      D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                           C.3 Example applications


                                         direction of actions


                      @ABC
                       s0 hk 0 / GFED
                      GFED       @ABC
                                  s1 k 1 / GFED
                                           @ABC           HIJK
                                                          sτ −1 l τ −1 / GFED
                                                        / ONML           @ABC
                             a         a          aτ −2         a      (
                                            ··· k                         sτ
                             r1          r2           rτ −1       rτ
                                         direction of reward




Figure C.9: Actions are performed until the desired target situation is achieved. Attention should
be paid to numbering: Rewards are numbered beginning with 1, actions and situations beginning
with 0 (This has simply been adopted as a convention).




learning is: Π can be initialized arbitrar- played backgammon knows that the situ-
ily, and by means of Q learning the result ation space is huge (approx. 1020 situa-
is always Q∗ .                                        tions). As a result, the state-value func-
                                                      tions cannot be computed explicitly (par-
Definition C.13 (Q learning). Q learn- ticularly in the late eighties when TD gam-
ing trains the action-value function by mon was introduced). The selected re-
means of the learning rule                           warding strategy was the pure delayed re-
                                                     ward, i.e. the system receives the reward
                                                      not before the end of the game and at the
Q(st , a)new =Q(st , a)                     (C.15) same time the reward is the return. Then
                                                      the system was allowed to practice itself
              + α(rt+1 + γ max Q(st+1 , a) − Q(st , a)).
                            a
                                                     (initially against a backgammon program,
                                                      then against an entity of itself). The result
and thus finds Q in any case.
                      ∗                              was  that it achieved the highest ranking in
                                                      a computer-backgammon league and strik-
                                                      ingly disproved the theory that a computer
                                                      programm is not capable to master a task
C.3 Example applications                              better than its programmer.


C.3.1 TD gammon                                     C.3.2 The car in the pit

TD gammon is a very successful                      Let us take a look at a car parking on a
backgammon game based on TD learn-                  one-dimensional road at the bottom of a
ing invented by Gerald Tesauro. The                 deep pit without being able to get over
situation here is the current configura-            the slope on both sides straight away by
tion of the board. Anyone who has ever              means of its engine power in order to leave



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                               205
Appendix C Excursus: reinforcement learning                                   dkriesel.com


the pit. Trivially, the executable actions     The angle of the pole relative to the verti-
here are the possibilities to drive forwards   cal line is referred to as α. Furthermore,
and backwards. The intuitive solution we       the vehicle always has a fixed position x an
think of immediately is to move backwards,     our one-dimensional world and a velocity
to gain momentum at the opposite slope         of ẋ. Our one-dimensional world is lim-
and oscillate in this way several times to     ited, i.e. there are maximum values and
dash out of the pit.                           minimum values x can adopt.
The actions of a reinforcement learning
system would be "full throttle forward", The aim of our system is to learn to steer
"full reverse" and "doing nothing".         the car in such a way that it can balance
                                            the pole, to prevent the pole from tipping
Here, "everything costs" would be a good over. This is achieved best by an avoid-
choice for awarding the reward so that the ance strategy: As long as the pole is bal-
system learns fast how to leave the pit and anced the reward is 0. If the pole tips over,
realizes that our problem cannot be solved the reward is -1.
by means of mere forward directed engine
power. So the system will slowly build up
the movement.                               Interestingly, the system is soon capable
                                            to keep the pole balanced by tilting it suf-
The policy can no longer be stored as a ficiently fast and with small movements.
table since the state space is hard to dis- At this the system mostly is in the cen-
cretize. As policy a function has to be ter of the space since this is farthest from
generated.                                  the walls which it understands as negative
                                            (if it touches the wall, the pole will tip
                                            over).
C.3.3 The pole balancer

The pole balancer was developed by
Barto, Sutton and Anderson.
                                               C.3.3.1 Swinging up an inverted
Let be given a situation including a vehicle           pendulum
that is capable to move either to the right
at full throttle or to the left at full throt-
tle (bang bang control). Only these two More difficult for the system is the fol-
actions can be performed, standing still lowing initial situation: the pole initially
is impossible. On the top of this car is hangs down, has to be swung up over the
hinged an upright pole that could tip over vehicle and finally has to be stabilized. In
to both sides. The pole is built in such a the literature this task is called swing up
way that it always tips over to one side so an inverted pendulum.
it never stands still (let us assume that the
pole is rounded at the lower end).



206                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com             C.4 Reinforcement learning in connection with neural networks


C.4 Reinforcement learning in                 ment learning to find a strategy in order
    connection with neural                    to exit a maze as fast as possible.

    networks                                    . What could an appropriate state-
                                                  value function look like?
Finally, the reader would like to ask why a     . How would you generate an appropri-
text on "neural networks" includes a chap-        ate reward?
ter about reinforcement learning.
                                              Assume that the robot is capable to avoid
The answer is very simple. We have al- obstacles and at any time knows its posi-
ready been introduced to supervised and tion (x, y) and orientation φ.
unsupervised learning procedures.         Al-
though we do not always have an om- Exercise 20. Describe the function of
niscient teacher who makes unsupervised the two components ASE and ACE as
learning possible, this does not mean that they have been proposed by Barto, Sut-
we do not receive any feedback at all. ton and Anderson to control the pole
There is often something in between, some balancer.
kind of criticism or school mark. Problems Bibliography: [BSA83].
like this can be solved by means of rein-
forcement learning.                           Exercise 21. Indicate several "classical"
                                              problems of informatics which could be
But not every problem is that easily solved solved efficiently by means of reinforce-
like our gridworld: In our backgammon ex- ment learning. Please give reasons for
ample we have approx. 1020 situations and your answers.
the situation tree has a large branching fac-
tor, let alone other games. Here, the tables
used in the gridworld can no longer be re-
alized as state- and action-value functions.
Thus, we have to find approximators for
these functions.
And which learning approximators for
these reinforcement learning components
come immediately into our mind? Exactly:
neural networks.


Exercises

Exercise 19. A robot control system
shall be persuaded by means of reinforce-



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                    207
Bibliography

[And72]   James A. Anderson. A simple neural network generating an interactive
          memory. Mathematical Biosciences, 14:197–220, 1972.
[APZ93]   D. Anguita, G. Parodi, and R. Zunino. Speed improvement of the back-
          propagation on current-generation workstations. In WCNN’93, Portland:
          World Congress on Neural Networks, July 11-15, 1993, Oregon Convention
          Center, Portland, Oregon, volume 1. Lawrence Erlbaum, 1993.
[BSA83]   A. Barto, R. Sutton, and C. Anderson. Neuron-like adaptive elements
          that can solve difficult learning control problems. IEEE Transactions on
          Systems, Man, and Cybernetics, 13(5):834–846, September 1983.
[CG87]    G. A. Carpenter and S. Grossberg. ART2: Self-organization of stable cate-
          gory recognition codes for analog input patterns. Applied Optics, 26:4919–
          4930, 1987.
[CG88]    M.A. Cohen and S. Grossberg. Absolute stability of global pattern forma-
          tion and parallel memory storage by competitive neural networks. Com-
          puter Society Press Technology Series Neural Networks, pages 70–81, 1988.
[CG90]    G. A. Carpenter and S. Grossberg. ART 3: Hierarchical search using
          chemical transmitters in self-organising pattern recognition architectures.
          Neural Networks, 3(2):129–152, 1990.
[CH67]    T. Cover and P. Hart. Nearest neighbor pattern classification. IEEE
          Transactions on Information Theory, 13(1):21–27, 1967.
[CR00]    N.A. Campbell and JB Reece. Biologie. Spektrum. Akademischer Verlag,
          2000.
[Cyb89]   G. Cybenko. Approximation by superpositions of a sigmoidal function.
          Mathematics of Control, Signals, and Systems (MCSS), 2(4):303–314,
          1989.
[DHS01]   R.O. Duda, P.E. Hart, and D.G. Stork. Pattern classification. Wiley New
          York, 2001.



                                                                                 209
Bibliography                                                                dkriesel.com


[Elm90]        Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–
               211, April 1990.
[Fah88]        S. E. Fahlman. An empirical sudy of learning speed in back-propagation
               networks. Technical Report CMU-CS-88-162, CMU, 1988.
[FMI83]        K. Fukushima, S. Miyake, and T. Ito. Neocognitron: A neural network
               model for a mechanism of visual pattern recognition. IEEE Transactions
               on Systems, Man, and Cybernetics, 13(5):826–834, September/October
               1983.
[Fri94]        B. Fritzke. Fast learning with incremental RBF networks. Neural Process-
               ing Letters, 1(1):2–5, 1994.
[GKE01a]       N. Goerke, F. Kintzler, and R. Eckmiller. Self organized classification of
               chaotic domains from a nonlinearattractor. In Neural Networks, 2001. Pro-
               ceedings. IJCNN’01. International Joint Conference on, volume 3, 2001.
[GKE01b]       N. Goerke, F. Kintzler, and R. Eckmiller. Self organized partitioning of
               chaotic attractors for control. Lecture notes in computer science, pages
               851–856, 2001.
[Gro76]        S. Grossberg. Adaptive pattern classification and universal recoding, I:
               Parallel development and coding of neural feature detectors. Biological
               Cybernetics, 23:121–134, 1976.
[GS06]         Nils Goerke and Alexandra Scherbart. Classification using multi-soms and
               multi-neural gas. In IJCNN, pages 3895–3902, 2006.
[Heb49]        Donald O. Hebb. The Organization of Behavior: A Neuropsychological
               Theory. Wiley, New York, 1949.
[Hop82]        John J. Hopfield. Neural networks and physical systems with emergent col-
               lective computational abilities. Proc. of the National Academy of Science,
               USA, 79:2554–2558, 1982.
[Hop84]        JJ Hopfield. Neurons with graded response have collective computational
               properties like those of two-state neurons. Proceedings of the National
               Academy of Sciences, 81(10):3088–3092, 1984.
[HT85]         JJ Hopfield and DW Tank. Neural computation of decisions in optimiza-
               tion problems. Biological cybernetics, 52(3):141–152, 1985.
[Jor86]        M. I. Jordan. Attractor dynamics and parallelism in a connectionist se-
               quential machine. In Proceedings of the Eighth Conference of the Cognitive
               Science Society, pages 531–546. Erlbaum, 1986.



210                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                 Bibliography


[Kau90]        L. Kaufman. Finding groups in data: an introduction to cluster analysis.
               In Finding Groups in Data: An Introduction to Cluster Analysis. Wiley,
               New York, 1990.

[Koh72]        T. Kohonen. Correlation matrix memories. IEEEtC, C-21:353–359, 1972.

[Koh82]        Teuvo Kohonen. Self-organized formation of topologically correct feature
               maps. Biological Cybernetics, 43:59–69, 1982.

[Koh89]        Teuvo Kohonen. Self-Organization and Associative Memory. Springer-
               Verlag, Berlin, third edition, 1989.

[Koh98]        T. Kohonen. The self-organizing map. Neurocomputing, 21(1-3):1–6, 1998.

[KSJ00]        E.R. Kandel, J.H. Schwartz, and T.M. Jessell. Principles of neural science.
               Appleton & Lange, 2000.

[lCDS90]       Y. le Cun, J. S. Denker, and S. A. Solla. Optimal brain damage. In
               D. Touretzky, editor, Advances in Neural Information Processing Systems
               2, pages 598–605. Morgan Kaufmann, 1990.

[Mac67]        J. MacQueen. Some methods for classification and analysis of multivariate
               observations. In Proceedings of the Fifth Berkeley Symposium on Mathe-
               matics, Statistics and Probability, Vol. 1, pages 281–296, 1967.

[MBS93]        Thomas M. Martinetz, Stanislav G. Berkovich, and Klaus J. Schulten.
               ’Neural-gas’ network for vector quantization and its application to time-
               series prediction. IEEE Trans. on Neural Networks, 4(4):558–569, 1993.

[MBW+ 10] K.D. Micheva, B. Busse, N.C. Weiler, N. O’Rourke, and S.J. Smith. Single-
          synapse analysis of a diverse synapse population: proteomic imaging meth-
          ods and markers. Neuron, 68(4):639–653, 2010.

[MP43]         W.S. McCulloch and W. Pitts. A logical calculus of the ideas immanent
               in nervous activity. Bulletin of Mathematical Biology, 5(4):115–133, 1943.

[MP69]         M. Minsky and S. Papert. Perceptrons. MIT Press, Cambridge, Mass,
               1969.

[MR86]         J. L. McClelland and D. E. Rumelhart. Parallel Distributed Processing:
               Explorations in the Microstructure of Cognition, volume 2. MIT Press,
               Cambridge, 1986.



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                       211
Bibliography                                                                dkriesel.com


[Par87]        David R. Parker. Optimal algorithms for adaptive networks: Second or-
               der back propagation, second order direct propagation, and second order
               hebbian learning. In Maureen Caudill and Charles Butler, editors, IEEE
               First International Conference on Neural Networks (ICNN’87), volume II,
               pages II–593–II–600, San Diego, CA, June 1987. IEEE.
[PG89]         T. Poggio and F. Girosi. A theory of networks for approximation and
               learning. MIT Press, Cambridge Mass., 1989.
[Pin87]        F. J. Pineda. Generalization of back-propagation to recurrent neural net-
               works. Physical Review Letters, 59:2229–2232, 1987.
[PM47]         W. Pitts and W.S. McCulloch. How we know universals the perception of
               auditory and visual forms. Bulletin of Mathematical Biology, 9(3):127–147,
               1947.
[Pre94]        L. Prechelt. Proben1: A set of neural network benchmark problems and
               benchmarking rules. Technical Report, 21:94, 1994.
[RB93]         M. Riedmiller and H. Braun. A direct adaptive method for faster back-
               propagation learning: The rprop algorithm. In Neural Networks, 1993.,
               IEEE International Conference on, pages 586–591. IEEE, 1993.
[RD05]         G. Roth and U. Dicke. Evolution of the brain and intelligence. Trends in
               Cognitive Sciences, 9(5):250–257, 2005.
[RHW86a] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by
         back-propagating errors. Nature, 323:533–536, October 1986.
[RHW86b] David E. Rumelhart, Geoffrey E. Hinton, and R. J. Williams. Learning
         internal representations by error propagation. In D. E. Rumelhart, J. L.
         McClelland, and the PDP research group., editors, Parallel distributed pro-
         cessing: Explorations in the microstructure of cognition, Volume 1: Foun-
         dations. MIT Press, 1986.
[Rie94]        M. Riedmiller. Rprop - description and implementation details. Technical
               report, University of Karlsruhe, 1994.
[Ros58]        F. Rosenblatt. The perceptron: a probabilistic model for information
               storage and organization in the brain. Psychological Review, 65:386–408,
               1958.
[Ros62]        F. Rosenblatt. Principles of Neurodynamics. Spartan, New York, 1962.
[SB98]         R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction.
               MIT Press, Cambridge, MA, 1998.



212                     D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                    Bibliography


[SG06]         A. Scherbart and N. Goerke. Unsupervised system for discovering patterns
               in time-series, 2006.
[SGE05]        Rolf Schatten, Nils Goerke, and Rolf Eckmiller. Regional and online learn-
               able fields. In Sameer Singh, Maneesha Singh, Chidanand Apté, and Petra
               Perner, editors, ICAPR (2), volume 3687 of Lecture Notes in Computer
               Science, pages 74–83. Springer, 2005.
[Ste61]        K. Steinbuch. Die lernmatrix. Kybernetik (Biological Cybernetics), 1:36–45,
               1961.
[vdM73]        C. von der Malsburg. Self-organizing of orientation sensitive cells in striate
               cortex. Kybernetik, 14:85–100, 1973.
[Was89]        P. D. Wasserman. Neural Computing Theory and Practice. New York :
               Van Nostrand Reinhold, 1989.
[Wer74]        P. J. Werbos. Beyond Regression: New Tools for Prediction and Analysis
               in the Behavioral Sciences. PhD thesis, Harvard University, 1974.
[Wer88]        P. J. Werbos. Backpropagation: Past and future. In Proceedings ICNN-88,
               San Diego, pages 343–353, 1988.
[WG94]         A.S. Weigend and N.A. Gershenfeld. Time series prediction. Addison-
               Wesley, 1994.
[WH60]         B. Widrow and M. E. Hoff. Adaptive switching circuits. In Proceedings
               WESCON, pages 96–104, 1960.
[Wid89]        R. Widner. Single-stage logic. AIEE Fall General Meeting, 1960. Wasser-
               man, P. Neural Computing, Theory and Practice, Van Nostrand Reinhold,
               1989.
[Zel94]        Andreas Zell. Simulation Neuronaler Netze. Addison-Wesley, 1994. Ger-
               man.




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                          213
List of Figures

 1.1    Robot with 8 sensors and 2 motors . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    6
 1.3    Black box with eight inputs and two outputs                                 .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    7
 1.2    Learning samples for the example robot . . .                                .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    8
 1.4    Institutions of the field of neural networks . .                            .   .   .   .   .   .   .   .   .   .   .   .   .   .   .    9

 2.1    Central nervous system      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   14
 2.2    Brain . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   15
 2.3    Biological neuron . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   17
 2.4    Action potential . . . .    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   22
 2.5    Compound eye . . . . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   27

 3.1    Data processing of a neuron . . . . . . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   .   .   35
 3.2    Various popular activation functions . . . . . .                                .   .   .   .   .   .   .   .   .   .   .   .   .   .   38
 3.3    Feedforward network . . . . . . . . . . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   .   .   40
 3.4    Feedforward network with shortcuts . . . . . .                                  .   .   .   .   .   .   .   .   .   .   .   .   .   .   41
 3.5    Directly recurrent network . . . . . . . . . . . .                              .   .   .   .   .   .   .   .   .   .   .   .   .   .   41
 3.6    Indirectly recurrent network . . . . . . . . . . .                              .   .   .   .   .   .   .   .   .   .   .   .   .   .   42
 3.7    Laterally recurrent network . . . . . . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   .   .   43
 3.8    Completely linked network . . . . . . . . . . . .                               .   .   .   .   .   .   .   .   .   .   .   .   .   .   44
 3.10   Examples for different types of neurons . . . .                                 .   .   .   .   .   .   .   .   .   .   .   .   .   .   45
 3.9    Example network with and without bias neuron                                    .   .   .   .   .   .   .   .   .   .   .   .   .   .   46

 4.1    Training samples and network capacities                         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   56
 4.2    Learning curve with different scalings .                        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   60
 4.3    Gradient descent, 2D visualization . . .                        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   62
 4.4    Possible errors during a gradient descent                       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   63
 4.5    The 2-spiral problem . . . . . . . . . . .                      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   65
 4.6    Checkerboard problem . . . . . . . . . .                        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   65

 5.1    The perceptron in three different views . .                         . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   72
 5.2    Singlelayer perceptron . . . . . . . . . . .                        . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   74
 5.3    Singlelayer perceptron with several output                          neurons             .   .   .   .   .   .   .   .   .   .   .   .   74
 5.4    AND and OR singlelayer perceptron . . .                             . . . . .           .   .   .   .   .   .   .   .   .   .   .   .   75



                                                                                                                                                215
List of Figures                                                                                                                         dkriesel.com


      5.5    Error surface of a network with 2 connections . . . . . . . . .                                                        .   .   .   .   .   .    78
      5.6    Sketch of a XOR-SLP . . . . . . . . . . . . . . . . . . . . . .                                                        .   .   .   .   .   .    82
      5.7    Two-dimensional linear separation . . . . . . . . . . . . . . .                                                        .   .   .   .   .   .    82
      5.8    Three-dimensional linear separation . . . . . . . . . . . . . .                                                        .   .   .   .   .   .    83
      5.9    The XOR network . . . . . . . . . . . . . . . . . . . . . . . .                                                        .   .   .   .   .   .    84
      5.10   Multilayer perceptrons and output sets . . . . . . . . . . . . .                                                       .   .   .   .   .   .    85
      5.11   Position of an inner neuron for derivation of backpropagation                                                          .   .   .   .   .   .    87
      5.12   Illustration of the backpropagation derivation . . . . . . . . .                                                       .   .   .   .   .   .    89
      5.13   Momentum term . . . . . . . . . . . . . . . . . . . . . . . . .                                                        .   .   .   .   .   .    97
      5.14   Fermi function and hyperbolic tangent . . . . . . . . . . . . .                                                        .   .   .   .   .   .   102
      5.15   Functionality of 8-2-8 encoding . . . . . . . . . . . . . . . . .                                                      .   .   .   .   .   .   103

      6.1    RBF network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                .   107
      6.2    Distance function in the RBF network . . . . . . . . . . . . . . . . . .                                                                   .   108
      6.3    Individual Gaussian bells in one- and two-dimensional space . . . . . .                                                                    .   109
      6.4    Accumulating Gaussian bells in one-dimensional space . . . . . . . . .                                                                     .   109
      6.5    Accumulating Gaussian bells in two-dimensional space . . . . . . . . .                                                                     .   110
      6.6    Even coverage of an input space with radial basis functions . . . . . .                                                                    .   116
      6.7    Uneven coverage of an input space with radial basis functions . . . . .                                                                    .   117
      6.8    Random, uneven coverage of an input space with radial basis functions                                                                      .   117

      7.1    Roessler attractor .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   122
      7.2    Jordan network . .     .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   123
      7.3    Elman network . .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   124
      7.4    Unfolding in time .    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   126

      8.1    Hopfield network . . . . . . . . . .                       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   130
      8.2    Binary threshold function . . . . .                        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   132
      8.3    Convergence of a Hopfield network                          .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   134
      8.4    Fermi function . . . . . . . . . . .                       .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   137

      9.1    Examples for quantization . . . . . . . . . . . . . . . . . . . . . . . . . . 141

      10.1   Example topologies of a SOM . . . . . . . . . . . . . . . . . . . . . . . . 148
      10.2   Example distances of SOM topologies . . . . . . . . . . . . . . . . . . . 151
      10.3   SOM topology functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
      10.4   First example of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
      10.7   Topological defect of a SOM . . . . . . . . . . . . . . . . . . . . . . . . . 156
      10.5   Training a SOM with one-dimensional topology . . . . . . . . . . . . . . 157
      10.6   SOMs with one- and two-dimensional topologies and different input spaces158
      10.8   Resolution optimization of a SOM to certain areas . . . . . . . . . . . . 160



216                       D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                     List of Figures


   10.9 Shape to be classified by neural gas . . . . . . . . . . . . . . . . . . . . . 162

   11.1 Structure of an ART network . . . . . . . . . . . . . . . . . . . . . . . . 166
   11.2 Learning process of an ART network . . . . . . . . . . . . . . . . . . . . 168

   A.1 Comparing cluster analysis methods . . . . . . . . . . . . . . . . . . . . 174
   A.2 ROLF neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
   A.3 Clustering by means of a ROLF . . . . . . . . . . . . . . . . . . . . . . . 179

   B.1   Neural network reading time series . . .                . . .       . . . . . . . .                 .   .   .   .   .   .   .   182
   B.2   One-step-ahead prediction . . . . . . . .               . . .       . . . . . . . .                 .   .   .   .   .   .   .   184
   B.3   Two-step-ahead prediction . . . . . . . .               . . .       . . . . . . . .                 .   .   .   .   .   .   .   186
   B.4   Direct two-step-ahead prediction . . . .                . . .       . . . . . . . .                 .   .   .   .   .   .   .   186
   B.5   Heterogeneous one-step-ahead prediction                 . . .       . . . . . . . .                 .   .   .   .   .   .   .   188
   B.6   Heterogeneous one-step-ahead prediction                 with        two outputs .                   .   .   .   .   .   .   .   188

   C.1   Gridworld . . . . . . . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   193
   C.2   Reinforcement learning . . . . .    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   193
   C.3   Gridworld with optimal returns      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   200
   C.4   Reinforcement learning cycle .      .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   201
   C.5   The Monte Carlo method . . .        .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   202
   C.6   Extended Monte Carlo method         .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   203
   C.7   Improving the policy . . . . . .    .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   203
   C.8   Action-value function . . . . . .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   204
   C.9   Reinforcement learning timeline     .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   .   205




D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                          217
Index

                                                                        ATP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
*                                                                       attractor . . . . . . . . . . . . . . . . . . . . . . . . . . 119
                                                                        autoassociator . . . . . . . . . . . . . . . . . . . . . 131
100-step rule . . . . . . . . . . . . . . . . . . . . . . . . 5         axon . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18, 23




A                                                                       B
Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
                                                                        backpropagation . . . . . . . . . . . . . . . . . . . . 88
action potential . . . . . . . . . . . . . . . . . . . . 21
                                                                             second order . . . . . . . . . . . . . . . . . . . 95
action space . . . . . . . . . . . . . . . . . . . . . . . 195
                                                                        backpropagation of error. . . . . . . . . . . .84
action-value function . . . . . . . . . . . . . . 203
                                                                             recurrent . . . . . . . . . . . . . . . . . . . . . . 125
activation . . . . . . . . . . . . . . . . . . . . . . . . . . 36
                                                                        bar . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
activation function . . . . . . . . . . . . . . . . . 36
                                                                        basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
     selection of . . . . . . . . . . . . . . . . . . . . . 98
                                                                        bias neuron . . . . . . . . . . . . . . . . . . . . . . . . . 44
ADALINE . . see adaptive linear neuron
                                                                        binary threshold function . . . . . . . . . . 37
adaptive linear element . . . see adaptive
                                                                        bipolar cell . . . . . . . . . . . . . . . . . . . . . . . . . 27
           linear neuron
                                                                        black box . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
adaptive linear neuron . . . . . . . . . . . . . . 10
                                                                        brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
adaptive resonance theory . . . . . 11, 165
                                                                        brainstem . . . . . . . . . . . . . . . . . . . . . . . . . . 16
agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
algorithm. . . . . . . . . . . . . . . . . . . . . . . . . . .50
amacrine cell . . . . . . . . . . . . . . . . . . . . . . . 28
approximation. . . . . . . . . . . . . . . . . . . . .110
ART . . . . see adaptive resonance theory                               C
ART-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
ART-2A . . . . . . . . . . . . . . . . . . . . . . . . . . . 168        capability to learn . . . . . . . . . . . . . . . . . . . 4
ART-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168       center
artificial intelligence . . . . . . . . . . . . . . . . 10                  of a ROLF neuron . . . . . . . . . . . . 176
associative data storage . . . . . . . . . . . 157                          of a SOM neuron . . . . . . . . . . . . . . 146



                                                                                                                                         219
Index                                                                                                                   dkriesel.com


     of an RBF neuron . . . . . . . . . . . . . 104                      digitization . . . . . . . . . . . . . . . . . . . . . . . . 138
       distance to the . . . . . . . . . . . . . . 107                   discrete . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
central nervous system . . . . . . . . . . . . . 14                      discretization . . . . . . . . . see quantization
cerebellum . . . . . . . . . . . . . . . . . . . . . . . . . 15          distance
cerebral cortex . . . . . . . . . . . . . . . . . . . . . 14                  Euclidean . . . . . . . . . . . . . . . . . 56, 171
cerebrum . . . . . . . . . . . . . . . . . . . . . . . . . . . 14             squared. . . . . . . . . . . . . . . . . . . .76, 171
change in weight. . . . . . . . . . . . . . . . . . . .64                dynamical system . . . . . . . . . . . . . . . . . 119
cluster analysis . . . . . . . . . . . . . . . . . . . . 171
clusters . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
CNS . . . . . . . see central nervous system
codebook vector . . . . . . . . . . . . . . 138, 172
complete linkage. . . . . . . . . . . . . . . . . . . .39                E
compound eye . . . . . . . . . . . . . . . . . . . . . . 26
                                                                         early stopping . . . . . . . . . . . . . . . . . . . . . . 59
concentration gradient . . . . . . . . . . . . . . 19
                                                                         electronic brain . . . . . . . . . . . . . . . . . . . . . . 9
cone function . . . . . . . . . . . . . . . . . . . . . . 150
                                                                         Elman network . . . . . . . . . . . . . . . . . . . . 121
connection. . . . . . . . . . . . . . . . . . . . . . . . . .34          environment . . . . . . . . . . . . . . . . . . . . . . . 193
context-based search . . . . . . . . . . . . . . 157                     episode . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
continuous . . . . . . . . . . . . . . . . . . . . . . . . 137           epoch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
cortex . . . . . . . . . . . . . . see cerebral cortex                   epsilon-nearest neighboring . . . . . . . . 173
     visual . . . . . . . . . . . . . . . . . . . . . . . . . . 15       error
cortical field . . . . . . . . . . . . . . . . . . . . . . . . 14             specific . . . . . . . . . . . . . . . . . . . . . . . . . 56
     association . . . . . . . . . . . . . . . . . . . . . 15                 total . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
     primary . . . . . . . . . . . . . . . . . . . . . . . . 15          error function . . . . . . . . . . . . . . . . . . . . . . 75
cylinder function . . . . . . . . . . . . . . . . . . 150                     specific . . . . . . . . . . . . . . . . . . . . . . . . . 75
                                                                         error vector . . . . . . . . . . . . . . . . . . . . . . . . 53
                                                                         evolutionary algorithms . . . . . . . . . . . 125
                                                                         exploitation approach . . . . . . . . . . . . . 197
                                                                         exploration approach . . . . . . . . . . . . . . 197
D                                                                        exteroceptor . . . . . . . . . . . . . . . . . . . . . . . . 24
Dartmouth Summer Research Project9
deep networks . . . . . . . . . . . . . . . . . . 93, 97
Delta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
delta rule . . . . . . . . . . . . . . . . . . . . . . . . . . . 79      F
dendrite . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
     tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18     fastprop . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
depolarization . . . . . . . . . . . . . . . . . . . . . . 21            fault tolerance . . . . . . . . . . . . . . . . . . . . . . . 4
diencephalon . . . . . . . . . . . . see interbrain                      feedforward. . . . . . . . . . . . . . . . . . . . . . . . .39
difference vector . . . . . . . see error vector                         Fermi function . . . . . . . . . . . . . . . . . . . . . 37
digital filter . . . . . . . . . . . . . . . . . . . . . . . 183         flat spot elimination . . . . . . . . . . . . . . . . 95



220                                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                                            Index


fudging . . . . . . . see flat spot elimination
function approximation . . . . . . . . . . . . . 98                      I
function approximator
    universal . . . . . . . . . . . . . . . . . . . . . . . 82           individual eye . . . . . . . . see ommatidium
                                                                         input dimension . . . . . . . . . . . . . . . . . . . . 48
                                                                         input patterns . . . . . . . . . . . . . . . . . . . . . . 50
                                                                         input vector . . . . . . . . . . . . . . . . . . . . . . . . 48
                                                                         interbrain . . . . . . . . . . . . . . . . . . . . . . . . . . 15
G                                                                        internodes . . . . . . . . . . . . . . . . . . . . . . . . . . 23
                                                                         interoceptor . . . . . . . . . . . . . . . . . . . . . . . . 24
                                                                         interpolation
ganglion cell . . . . . . . . . . . . . . . . . . . . . . . . 27
                                                                               precise . . . . . . . . . . . . . . . . . . . . . . . . 110
Gauss-Markov model . . . . . . . . . . . . . . 111
                                                                         ion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Gaussian bell . . . . . . . . . . . . . . . . . . . . . . 149
                                                                         iris . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
generalization . . . . . . . . . . . . . . . . . . . . 4, 49
glial cell . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
gradient descent . . . . . . . . . . . . . . . . . . . . 59
     problems . . . . . . . . . . . . . . . . . . . . . . . 60           J
grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
gridworld. . . . . . . . . . . . . . . . . . . . . . . . . .192          Jordan network. . . . . . . . . . . . . . . . . . . .120




H                                                                        K
                                                                         k-means clustering . . . . . . . . . . . . . . . . 172
Heaviside function see binary threshold
                                                                         k-nearest neighboring. . . . . . . . . . . . . .172
         function
Hebbian rule . . . . . . . . . . . . . . . . . . . . . . . 64
    generalized form . . . . . . . . . . . . . . . . 65
heteroassociator . . . . . . . . . . . . . . . . . . . 132
Hinton diagram . . . . . . . . . . . . . . . . . . . . 34                L
history of development. . . . . . . . . . . . . . .8
Hopfield networks . . . . . . . . . . . . . . . . . 127                  layer
    continuous . . . . . . . . . . . . . . . . . . . . 134                   hidden . . . . . . . . . . . . . . . . . . . . . . . . . 39
horizontal cell . . . . . . . . . . . . . . . . . . . . . . 28               input . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
hyperbolic tangent . . . . . . . . . . . . . . . . . 37                      output . . . . . . . . . . . . . . . . . . . . . . . . . 39
hyperpolarization . . . . . . . . . . . . . . . . . . . 21               learnability . . . . . . . . . . . . . . . . . . . . . . . . . 97
hypothalamus . . . . . . . . . . . . . . . . . . . . . . 15              learning



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                            221
Index                                                                                                           dkriesel.com


     batch . . . . . . . . . . see learning, offline                      S . . . . . . . . . . . . . . see situation space
     offline . . . . . . . . . . . . . . . . . . . . . . . . . . 52       T . . . . . . see temperature parameter
     online . . . . . . . . . . . . . . . . . . . . . . . . . . 52        VΠ∗ (s) . . . . . see state-value function,
     reinforcement . . . . . . . . . . . . . . . . . . 51                        optimal
     supervised. . . . . . . . . . . . . . . . . . . . . .51              VΠ (s) . . . . . see state-value function
     unsupervised . . . . . . . . . . . . . . . . . . . 50                W . . . . . . . . . . . . . . see weight matrix
learning rate . . . . . . . . . . . . . . . . . . . . . . . 89            ∆wi,j . . . . . . . . see change in weight
     variable . . . . . . . . . . . . . . . . . . . . . . . . 90          Π . . . . . . . . . . . . . . . . . . . . . . . see policy
learning strategy . . . . . . . . . . . . . . . . . . . 39                Θ . . . . . . . . . . . . . . see threshold value
learning vector quantization . . . . . . . 137                            α . . . . . . . . . . . . . . . . . . see momentum
lens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27   β . . . . . . . . . . . . . . . . see weight decay
linear separability . . . . . . . . . . . . . . . . . . 81                δ . . . . . . . . . . . . . . . . . . . . . . . . see Delta
linearer associator . . . . . . . . . . . . . . . . . . 11                η . . . . . . . . . . . . . . . . . see learning rate
locked-in syndrome . . . . . . . . . . . . . . . . . 16                   η ↑ . . . . . . . . . . . . . . . . . . . . . . see Rprop
logistic function . . . . see Fermi function                              η ↓ . . . . . . . . . . . . . . . . . . . . . . see Rprop
     temperature parameter . . . . . . . . . 37                           ηmax . . . . . . . . . . . . . . . . . . . . see Rprop
LVQ . . see learning vector quantization                                  ηmin . . . . . . . . . . . . . . . . . . . . see Rprop
LVQ1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140        ηi,j . . . . . . . . . . . . . . . . . . . . . see Rprop
LVQ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140        ∇ . . . . . . . . . . . . . . see nabla operator
LVQ3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140        ρ . . . . . . . . . . . . . see radius multiplier
                                                                          Err . . . . . . . . . . . . . . . . see error, total
                                                                          Err(W ) . . . . . . . . . see error function
                                                                          Errp . . . . . . . . . . . . . see error, specific
                                                                          Errp (W )see error function, specific
M                                                                         ErrWD . . . . . . . . . . . see weight decay
                                                                          at . . . . . . . . . . . . . . . . . . . . . . see action
M-SOM . see self-organizing map, multi                                    c. . . . . . . . . . . . . . . . . . . . . . . .see center
Mark I perceptron . . . . . . . . . . . . . . . . . . 10                         of an RBF neuron, see neuron,
Mathematical Symbols                                                             self-organizing map, center
   (t) . . . . . . . . . . . . . . . see time concept                     m . . . . . . . . . . . see output dimension
   A(S) . . . . . . . . . . . . . see action space                        n . . . . . . . . . . . . . see input dimension
   Ep . . . . . . . . . . . . . . . . see error vector                    p . . . . . . . . . . . . . see training pattern
   G . . . . . . . . . . . . . . . . . . . . see topology                 rh . . . see center of an RBF neuron,
   N . . see self-organizing map, input                                          distance to the
         dimension                                                        rt . . . . . . . . . . . . . . . . . . . . . . see reward
   P . . . . . . . . . . . . . . . . . see training set                   st . . . . . . . . . . . . . . . . . . . . see situation
   Q∗Π (s, a) . see action-value function,                                t . . . . . . . . . . . . . . . see teaching input
         optimal                                                          wi,j . . . . . . . . . . . . . . . . . . . . see weight
   QΠ (s, a) . see action-value function                                  x . . . . . . . . . . . . . . . . . see input vector
   Rt . . . . . . . . . . . . . . . . . . . . . . see return              y . . . . . . . . . . . . . . . . see output vector



222                                    D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                                    Index


   fact . . . . . . . . see activation function                            self-organizing map. . . . . . . . . . . .146
   fout . . . . . . . . . . . see output function                          tanh . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
membrane . . . . . . . . . . . . . . . . . . . . . . . . . . 19            winner . . . . . . . . . . . . . . . . . . . . . . . . 148
   -potential . . . . . . . . . . . . . . . . . . . . . . 19           neuron layers . . . . . . . . . . . . . . . . . see layer
memorized . . . . . . . . . . . . . . . . . . . . . . . . . 54         neurotransmitters . . . . . . . . . . . . . . . . . . 17
metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171   nodes of Ranvier . . . . . . . . . . . . . . . . . . . 23
Mexican hat function . . . . . . . . . . . . . . 150
MLP. . . . . . . . see perceptron, multilayer
momentum . . . . . . . . . . . . . . . . . . . . . . . . . 94
momentum term. . . . . . . . . . . . . . . . . . . .94
Monte Carlo method . . . . . . . . . . . . . . 201                     O
Moore-Penrose pseudo inverse . . . . . 110
                                                                       oligodendrocytes . . . . . . . . . . . . . . . . . . . 23
moving average procedure . . . . . . . . . 184
                                                                       OLVQ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
myelin sheath . . . . . . . . . . . . . . . . . . . . . . 23
                                                                       on-neuron . . . . . . . . . . . . . see bias neuron
                                                                       one-step-ahead prediction . . . . . . . . . 183
                                                                            heterogeneous . . . . . . . . . . . . . . . . . 187
                                                                       open loop learning. . . . . . . . . . . . . . . . .125
N                                                                      optimal brain damage . . . . . . . . . . . . . . 96
                                                                       order of activation . . . . . . . . . . . . . . . . . . 45
nabla operator. . . . . . . . . . . . . . . . . . . . . .59                 asynchronous
Neocognitron . . . . . . . . . . . . . . . . . . . . . . . 12                 fixed order . . . . . . . . . . . . . . . . . . . 47
nervous system . . . . . . . . . . . . . . . . . . . . . 13                   random order . . . . . . . . . . . . . . . . 46
network input . . . . . . . . . . . . . . . . . . . . . . 35                  randomly permuted order . . . . 46
                                                                              topological order . . . . . . . . . . . . . 47
neural gas . . . . . . . . . . . . . . . . . . . . . . . . . 159
                                                                            synchronous . . . . . . . . . . . . . . . . . . . . 46
    growing . . . . . . . . . . . . . . . . . . . . . . . 162
                                                                       output dimension . . . . . . . . . . . . . . . . . . . 48
    multi- . . . . . . . . . . . . . . . . . . . . . . . . . 161
                                                                       output function. . . . . . . . . . . . . . . . . . . . .38
neural network . . . . . . . . . . . . . . . . . . . . . 34
                                                                       output vector . . . . . . . . . . . . . . . . . . . . . . . 48
    recurrent . . . . . . . . . . . . . . . . . . . . . . 119
neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
    accepting . . . . . . . . . . . . . . . . . . . . . 177
    binary. . . . . . . . . . . . . . . . . . . . . . . . . .71
    context. . . . . . . . . . . . . . . . . . . . . . . .120          P
    Fermi . . . . . . . . . . . . . . . . . . . . . . . . . . 71
    identity . . . . . . . . . . . . . . . . . . . . . . . . 71        parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . 5
    information processing . . . . . . . . . 71                        pattern . . . . . . . . . . . see training pattern
    input . . . . . . . . . . . . . . . . . . . . . . . . . . . 71     pattern recognition . . . . . . . . . . . . 98, 131
    RBF . . . . . . . . . . . . . . . . . . . . . . . . . . 104        perceptron . . . . . . . . . . . . . . . . . . . . . . . . . 71
      output . . . . . . . . . . . . . . . . . . . . . . 104               multilayer . . . . . . . . . . . . . . . . . . . . . . 82
    ROLF. . . . . . . . . . . . . . . . . . . . . . . . .176                  recurrent . . . . . . . . . . . . . . . . . . . . 119



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                    223
Index                                                                                                                   dkriesel.com


    singlelayer . . . . . . . . . . . . . . . . . . . . . . 72             von der Malsburg, Christoph . . . 11
perceptron convergence theorem . . . . 73                                  Werbos, Paul . . . . . . . . . . . 11, 84, 96
perceptron learning algorithm . . . . . . 73                               Widrow, Bernard . . . . . . . . . . . . . . . 10
period . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119       Wightman, Charles . . . . . . . . . . . . . 10
peripheral nervous system . . . . . . . . . . 13                           Williams . . . . . . . . . . . . . . . . . . . . . . . 12
Persons                                                                    Zuse, Konrad . . . . . . . . . . . . . . . . . . . . 9
    Anderson . . . . . . . . . . . . . . . . . . . . 206 f.            pinhole eye . . . . . . . . . . . . . . . . . . . . . . . . . 26
    Anderson, James A. . . . . . . . . . . . . 11                      PNS . . . . see peripheral nervous system
    Anguita . . . . . . . . . . . . . . . . . . . . . . . . 37         pole balancer . . . . . . . . . . . . . . . . . . . . . . 206
    Barto . . . . . . . . . . . . . . . . . . . 191, 206 f.            policy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
    Carpenter, Gail . . . . . . . . . . . . 11, 165                        closed loop . . . . . . . . . . . . . . . . . . . . 197
    Elman . . . . . . . . . . . . . . . . . . . . . . . . 120              evaluation . . . . . . . . . . . . . . . . . . . . . 200
    Fukushima . . . . . . . . . . . . . . . . . . . . . 12                 greedy . . . . . . . . . . . . . . . . . . . . . . . . 197
    Girosi . . . . . . . . . . . . . . . . . . . . . . . . . 103           improvement . . . . . . . . . . . . . . . . . . 200
    Grossberg, Stephen . . . . . . . . 11, 165                             open loop . . . . . . . . . . . . . . . . . . . . . 197
    Hebb, Donald O. . . . . . . . . . . . . 9, 64                      pons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
    Hinton . . . . . . . . . . . . . . . . . . . . . . . . . 12        propagation function . . . . . . . . . . . . . . . 35
    Hoff, Marcian E. . . . . . . . . . . . . . . . 10                  pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
    Hopfield, John . . . . . . . . . . . 11 f., 127                    pupil . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
    Ito . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
    Jordan . . . . . . . . . . . . . . . . . . . . . . . . 120
    Kohonen, Teuvo . 11, 137, 145, 157
    Lashley, Karl . . . . . . . . . . . . . . . . . . . . 9
    MacQueen, J. . . . . . . . . . . . . . . . . . 172                 Q
    Martinetz, Thomas . . . . . . . . . . . . 159
                                                                       Q learning . . . . . . . . . . . . . . . . . . . . . . . . 204
    McCulloch, Warren . . . . . . . . . . . . 8 f.
                                                                       quantization . . . . . . . . . . . . . . . . . . . . . . . 137
    Minsky, Marvin . . . . . . . . . . . . . . . . 9 f.
                                                                       quickpropagation . . . . . . . . . . . . . . . . . . . 95
    Miyake . . . . . . . . . . . . . . . . . . . . . . . . . 12
    Nilsson, Nils. . . . . . . . . . . . . . . . . . . .10
    Papert, Seymour . . . . . . . . . . . . . . . 10
    Parker, David . . . . . . . . . . . . . . . . . . 95
    Pitts, Walter . . . . . . . . . . . . . . . . . . . 8 f.           R
    Poggio . . . . . . . . . . . . . . . . . . . . . . . . 103
    Pythagoras . . . . . . . . . . . . . . . . . . . . . 56            RBF network. . . . . . . . . . . . . . . . . . . . . .104
    Riedmiller, Martin . . . . . . . . . . . . . 90                        growing . . . . . . . . . . . . . . . . . . . . . . . 115
    Rosenblatt, Frank . . . . . . . . . . 10, 69                       receptive field . . . . . . . . . . . . . . . . . . . . . . 27
    Rumelhart . . . . . . . . . . . . . . . . . . . . . 12             receptor cell . . . . . . . . . . . . . . . . . . . . . . . . 24
    Steinbuch, Karl . . . . . . . . . . . . . . . . 10                     photo-. . . . . . . . . . . . . . . . . . . . . . . . . .27
    Sutton . . . . . . . . . . . . . . . . . . 191, 206 f.                 primary . . . . . . . . . . . . . . . . . . . . . . . . 24
    Tesauro, Gerald . . . . . . . . . . . . . . . 205                      secondary . . . . . . . . . . . . . . . . . . . . . . 24



224                                  D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
dkriesel.com                                                                                                                       Index


recurrence . . . . . . . . . . . . . . . . . . . . . 40, 119           situation . . . . . . . . . . . . . . . . . . . . . . . . . . 194
     direct . . . . . . . . . . . . . . . . . . . . . . . . . . 40     situation space . . . . . . . . . . . . . . . . . . . . 195
     indirect . . . . . . . . . . . . . . . . . . . . . . . . 41       situation tree . . . . . . . . . . . . . . . . . . . . . . 198
     lateral . . . . . . . . . . . . . . . . . . . . . . . . . . 42    SLP . . . . . . . . see perceptron, singlelayer
refractory period . . . . . . . . . . . . . . . . . . . 23             Snark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
regional and online learnable fields 175                               SNIPE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi
reinforcement learning . . . . . . . . . . . . . 191                   sodium-potassium pump . . . . . . . . . . . . 20
repolarization . . . . . . . . . . . . . . . . . . . . . . 21          SOM . . . . . . . . . . see self-organizing map
representability . . . . . . . . . . . . . . . . . . . . . 97          soma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
resilient backpropagation . . . . . . . . . . . 90                     spin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
resonance . . . . . . . . . . . . . . . . . . . . . . . . . 166        spinal cord . . . . . . . . . . . . . . . . . . . . . . . . . 14
retina . . . . . . . . . . . . . . . . . . . . . . . . . . . 27, 71    stability / plasticity dilemma . . . . . . 165
return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195   state . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195     state space forecasting . . . . . . . . . . . . . 183
     avoidance strategy . . . . . . . . . . . . 199                    state-value function . . . . . . . . . . . . . . . 200
     pure delayed . . . . . . . . . . . . . . . . . . 198              stimulus . . . . . . . . . . . . . . . . . . . . . . . 21, 147
     pure negative . . . . . . . . . . . . . . . . . 198               stimulus-conducting apparatus. . . . . .24
RMS . . . . . . . . . . . . see root mean square                       surface, perceptive. . . . . . . . . . . . . . . . .176
ROLFs . . . . . . . . . see regional and online                        swing up an inverted pendulum. . . .206
           learnable fields                                            symmetry breaking . . . . . . . . . . . . . . . . . 98
root mean square . . . . . . . . . . . . . . . . . . . 56              synapse
Rprop . . . see resilient backpropagation                                  chemical . . . . . . . . . . . . . . . . . . . . . . . 17
                                                                           electrical . . . . . . . . . . . . . . . . . . . . . . . 17
                                                                       synapses. . . . . . . . . . . . . . . . . . . . . . . . . . . .17
                                                                       synaptic cleft . . . . . . . . . . . . . . . . . . . . . . . 17

S
saltatory conductor . . . . . . . . . . . . . . . . . 23
Schwann cell . . . . . . . . . . . . . . . . . . . . . . . 23          T
self-fulfilling prophecy . . . . . . . . . . . . . 189
self-organizing feature maps . . . . . . . . 11                        target . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
self-organizing map . . . . . . . . . . . . . . . . 145                TD gammon . . . . . . . . . . . . . . . . . . . . . . 205
     multi- . . . . . . . . . . . . . . . . . . . . . . . . . 161      TD learning. . . .see temporal difference
sensory adaptation . . . . . . . . . . . . . . . . . 25                           learning
sensory transduction. . . . . . . . . . . . . . . .24                  teacher forcing . . . . . . . . . . . . . . . . . . . . 125
shortcut connections . . . . . . . . . . . . . . . . 39                teaching input . . . . . . . . . . . . . . . . . . . . . . 53
silhouette coefficient . . . . . . . . . . . . . . . 175               telencephalon . . . . . . . . . . . . see cerebrum
single lense eye . . . . . . . . . . . . . . . . . . . . . 27          temporal difference learning . . . . . . . 202
Single Shot Learning . . . . . . . . . . . . . . 130                   thalamus . . . . . . . . . . . . . . . . . . . . . . . . . . . 15



D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)                                                                       225
Index                                                                                                             dkriesel.com


threshold potential . . . . . . . . . . . . . . . . . 21                weighted sum . . . . . . . . . . . . . . . . . . . . . . . 35
threshold value . . . . . . . . . . . . . . . . . . . . . 36            Widrow-Hoff rule . . . . . . . . see delta rule
time concept . . . . . . . . . . . . . . . . . . . . . . . 33           winner-takes-all scheme . . . . . . . . . . . . . 42
time horizon . . . . . . . . . . . . . . . . . . . . . . 196
time series . . . . . . . . . . . . . . . . . . . . . . . . 181
time series prediction . . . . . . . . . . . . . . 181
topological defect. . . . . . . . . . . . . . . . . .154
topology . . . . . . . . . . . . . . . . . . . . . . . . . . 147
topology function . . . . . . . . . . . . . . . . . 148
training pattern . . . . . . . . . . . . . . . . . . . . 53
    set of . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
training set . . . . . . . . . . . . . . . . . . . . . . . . . 50
transfer functionsee activation function
truncus cerebri . . . . . . . . . . see brainstem
two-step-ahead prediction . . . . . . . . . 185
    direct . . . . . . . . . . . . . . . . . . . . . . . . . 185




U
unfolding in time . . . . . . . . . . . . . . . . . . 123




V
voronoi diagram . . . . . . . . . . . . . . . . . . . 138




W
weight . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
weight matrix . . . . . . . . . . . . . . . . . . . . . . 34
    bottom-up . . . . . . . . . . . . . . . . . . . . 166
    top-down. . . . . . . . . . . . . . . . . . . . . .165
weight vector . . . . . . . . . . . . . . . . . . . . . . . 34



226                                   D. Kriesel – A Brief Introduction to Neural Networks (ZETA2-EN)
